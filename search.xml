<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SparkSession</title>
      <link href="/2022/10/28/sparksession/"/>
      <url>/2022/10/28/sparksession/</url>
      
        <content type="html"><![CDATA[<h1 id="A-tale-of-Spark-Session-and-Spark-Context-https-medium-com-achilleus-spark-session-10d0d66d1d24"><a href="#A-tale-of-Spark-Session-and-Spark-Context-https-medium-com-achilleus-spark-session-10d0d66d1d24" class="headerlink" title="A tale of Spark Session and Spark Context(https://medium.com/@achilleus/spark-session-10d0d66d1d24)"></a>A tale of Spark Session and Spark Context(<a href="https://medium.com/@achilleus/spark-session-10d0d66d1d24">https://medium.com/@achilleus/spark-session-10d0d66d1d24</a>)</h1><p>I have Spark Context, SQL context, Hive context already!</p><p><img src="https://miro.medium.com/max/1280/1*aJSwrFLDlDbf9axjJ7gXHw.jpeg" class="lazyload placeholder" data-srcset="https://miro.medium.com/max/1280/1*aJSwrFLDlDbf9axjJ7gXHw.jpeg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img"></p><p>Spark session is a unified entry point of a spark application from Spark 2.0. It provides a way to interact with various spark’s functionality with a lesser number of constructs. Instead of having a spark context, hive context, SQL context, now all of it is encapsulated in a Spark session.</p><h2 id="Some-History…"><a href="#Some-History…" class="headerlink" title="Some History…."></a>Some History….</h2><p>Prior Spark 2.0, Spark Context was the entry point of any spark application and used to access all spark features and needed a sparkConf which had all the cluster configs and parameters to create a Spark Context object. We could primarily create just RDDs using Spark Context and we had to create specific spark contexts for any other spark interactions. For SQL SQLContext, hive HiveContext, streaming Streaming Application. In a nutshell, Spark session is a combination of all these different contexts. Internally, Spark session creates a new SparkContext for all the operations and also all the above-mentioned contexts can be accessed using the SparkSession object.</p><h2 id="How-do-I-create-a-Spark-session"><a href="#How-do-I-create-a-Spark-session" class="headerlink" title="How do I create a Spark session?"></a>How do I create a Spark session?</h2><p>A Spark Session can be created using a builder pattern.</p><pre class="line-numbers language-none"><code class="language-none">import org.apache.spark.sql.SparkSessionval spark &#x3D; SparkSession.builder.appName(&quot;SparkSessionExample&quot;) .master(&quot;local[4]&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;target&#x2F;spark-warehouse&quot;).enableHiveSupport().getOrCreate<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The spark session builder will try to get a spark session if there is one already created or create a new one and assigns the newly created SparkSession as the global default. Note that <code>enableHiveSupport</code> here is similar to creating a HiveContext and all it does is enables access to Hive metastore, Hive serdes, and Hive udfs.</p><p>Note that, we don’t have to create a spark session object when using spark-shell. It is already created for us with the variable <code>**spark**</code>.</p><p><img src="https://miro.medium.com/max/60/1*iWGQ3PvMvNy12nmH_eH_8A.png?q=20" class="lazyload placeholder" data-srcset="https://miro.medium.com/max/60/1*iWGQ3PvMvNy12nmH_eH_8A.png?q=20" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img"></p><p><img src="https://miro.medium.com/max/875/1*iWGQ3PvMvNy12nmH_eH_8A.png" class="lazyload placeholder" data-srcset="https://miro.medium.com/max/875/1*iWGQ3PvMvNy12nmH_eH_8A.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img"></p><pre class="line-numbers language-none"><code class="language-none">scala&gt; sparkres1: org.apache.spark.sql.SparkSession &#x3D; org.apache.spark.sql.SparkSession@2bd158ea<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>We can access spark context and other contexts using the spark session object.</p><pre class="line-numbers language-none"><code class="language-none">scala&gt; spark.sparkContextres2: org.apache.spark.SparkContext &#x3D; org.apache.spark.SparkContext@6803b02dscala&gt; spark.sqlContextres3: org.apache.spark.sql.SQLContext &#x3D; org.apache.spark.sql.SQLContext@74037f9b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>Accessing Spark’s configuration:</p><p>We can still access the spark’s configurations using spark session the same way as we used using <code>spark conf</code> .</p><h2 id="Why-do-I-need-Spark-session-when-I-already-have-Spark-context"><a href="#Why-do-I-need-Spark-session-when-I-already-have-Spark-context" class="headerlink" title="Why do I need Spark session when I already have Spark context?"></a>Why do I need Spark session when I already have Spark context?</h2><p>A part of the answer would be that it <em>unifies</em> all the <em>different contexts</em> in spark and avoids the developer to worry about creating difference contexts. But apart from this big advantage, the developers of spark have tried to solve the problem when there are multiple users using the same spark context.</p><p>Let’s say we have multiple users accessing the same notebook environment which had shared spark context and the requirement was to have an isolated environment sharing the same spark context. Prior to 2.0, the solution to this was to create multiple spark contexts ie spark context per isolated environment or users and is an expensive operation(generally 1 per JVM). But with the introduction of the spark session, this issue has been addressed.</p><blockquote><p>Note: we can have multiple spark contexts by setting <code>*spark.driver.allowMultipleContexts*</code> to <code>*true*</code> . But having multiple spark contexts in the same jvm is not encouraged and is not considered as a good practice as it makes it more unstable and crashing of 1 spark context can affect the other.</p></blockquote><h2 id="How-do-I-create-multiple-sessions"><a href="#How-do-I-create-multiple-sessions" class="headerlink" title="How do I create multiple sessions?"></a>How do I create multiple sessions?</h2><p>Spark gives a straight forward API to create a new session which shares the same spark context.<code>spark.newSession()</code> creates a new spark session object. If we look closely at the hash of the <code>spark</code> and <code>session2</code> , they both are different. In contrast, the underneath spark context is the same.</p><pre class="line-numbers language-none"><code class="language-none">scala&gt; val session2 &#x3D; spark.newSession()session2: org.apache.spark.sql.SparkSession &#x3D; org.apache.spark.sql.SparkSession@691fffb9scala&gt; sparkres22: org.apache.spark.sql.SparkSession &#x3D; org.apache.spark.sql.SparkSession@506bc254scala&gt; spark.sparkContextres26: org.apache.spark.SparkContext &#x3D; org.apache.spark.SparkContext@715fceafscala&gt; session2.sparkContextres27: org.apache.spark.SparkContext &#x3D; org.apache.spark.SparkContext@715fceaf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Also, we can verify that the spark session gives a unified view of all the contexts and isolation of configuration and environment. We can directly query without creating a SQL Context like we used and run the queries similarly. Let’s say we have a table called <code>people_session1</code> .This table will be only visible in the session <code>spark</code> . Let’s say we created a new session <code>session2</code> .These tables won’t be visible for when we try to access them and also we can create another table with the same name without affecting the table in <code>spark</code> session.</p><pre class="line-numbers language-none"><code class="language-none">scala&gt; people.createOrReplaceTempView(&quot;people_session1&quot;)scala&gt; spark.sql(&quot;show tables&quot;).show()+--------+---------------+-----------+|database|      tableName|isTemporary|+--------+---------------+-----------+|        |people_session1|       true|+--------+---------------+-----------+scala&gt; spark.catalog.listTables.show()+---------------+--------+-----------+---------+-----------+|           name|database|description|tableType|isTemporary|+---------------+--------+-----------+---------+-----------+|people_session1|    null|       null|TEMPORARY|       true|+---------------+--------+-----------+---------+-----------+scala&gt; session2.sql(&quot;show tables&quot;).show()+--------+---------+-----------+|database|tableName|isTemporary|+--------+---------+-----------++--------+---------+-----------+scala&gt;   session2.catalog.listTables.show()+----+--------+-----------+---------+-----------+|name|database|description|tableType|isTemporary|+----+--------+-----------+---------+-----------++----+--------+-----------+---------+-----------+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This isolation is for the configurations as well. Both sessions can have their own configs.</p><pre class="line-numbers language-none"><code class="language-none">scala&gt; spark.conf.get(&quot;spark.sql.crossJoin.enabled&quot;)res21: String &#x3D; truescala&gt;   session2.conf.get(&quot;spark.sql.crossJoin.enabled&quot;)res25: String &#x3D; false<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Get-the-existing-configurations"><a href="#Get-the-existing-configurations" class="headerlink" title="Get the existing configurations:"></a>Get the existing configurations:</h2><pre class="line-numbers language-none"><code class="language-none">scala&gt; spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, 100)scala&gt; spark.conf.getAllres55: Map[String,String] &#x3D; Map(spark.driver.host -&gt; 19e0778ea843, spark.driver.port -&gt; 38121, spark.repl.class.uri -&gt; spark:&#x2F;&#x2F;19e0778ea843:38121&#x2F;classes, spark.jars -&gt; &quot;&quot;, spark.repl.class.outputDir -&gt; &#x2F;tmp&#x2F;spark-cfe820cd-b2f1-4d23-9c9a-3ee42bc78e01&#x2F;repl-fae1a516-761a-4f31-b957-f5860882478f, spark.sql.crossJoin.enabled -&gt; true, spark.app.name -&gt; Spark shell, spark.ui.showConsoleProgress -&gt; true, spark.executor.id -&gt; driver, spark.submit.deployMode -&gt; client, spark.master -&gt; local[*], spark.home -&gt; &#x2F;opt&#x2F;spark, spark.notebook.name -&gt; SparkSessionSimpleZipExample, spark.sql.catalogImplementation -&gt; hive, spark.app.id -&gt; local-1553489583142, spark.sql.shuffle.partitions -&gt; 100)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="Set-configurations"><a href="#Set-configurations" class="headerlink" title="Set configurations:"></a>Set configurations:</h2><p>To set any configurations, we can either set the configs when we create our spark session using the <code>.config</code> option or use the set method.</p><pre class="line-numbers language-none"><code class="language-none">scala&gt; spark.conf.get(&quot;spark.sql.crossJoin.enabled&quot;)res4: String &#x3D; falsescala&gt; spark.conf.set(&quot;spark.sql.crossJoin.enabled&quot;, &quot;true&quot;)scala&gt; spark.conf.get(&quot;spark.sql.crossJoin.enabled&quot;)res6: String &#x3D; true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="Some-common-operations"><a href="#Some-common-operations" class="headerlink" title="Some common operations:"></a>Some common operations:</h2><ol><li>Create a dataframe from a Seq.</li></ol><pre class="line-numbers language-none"><code class="language-none">val emp &#x3D; Seq((101, &quot;Amy&quot;, Some(2)))val employee &#x3D; spark.createDataFrame(emp).toDF(&quot;employeeId&quot;,&quot;employeeName&quot;,&quot;managerId&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="2"><li>Create a Dataset</li></ol><pre class="line-numbers language-none"><code class="language-none">scala&gt; val employeeDs &#x3D; spark.createDataset(emp).as[Employee]employeeDs: org.apache.spark.sql.Dataset[Employee] &#x3D; [employeeId: int, employeeName: string ... 1 more field]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>3)Read data from a source: Read basically gives access to the dataframe reader which can be used to read from multiple sources such as csv, json, avro, JDBC, and many other 3rd party data source API implementations.</p><pre class="line-numbers language-none"><code class="language-none">scala&gt; val df &#x3D; spark.read.option(&quot;header&quot;,&quot;true&quot;).csv(&quot;src&#x2F;main&#x2F;resources&#x2F;Characters.csv&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="4"><li>Accessing the catalog and listenerManager.</li></ol><p>The catalog provides information about the underlying databases, tables, functions and lot more for the current session.</p><pre class="line-numbers language-none"><code class="language-none">scala&gt; spark.catalog.listColumns(&quot;people_session1&quot;).show+------+-----------+--------+--------+-----------+--------+|  name|description|dataType|nullable|isPartition|isBucket|+------+-----------+--------+--------+-----------+--------+|    id|       null|     int|   false|      false|   false||gender|       null|  string|    true|      false|   false|| alive|       null| boolean|   false|      false|   false|+------+-----------+--------+--------+-----------+--------+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The listenerManager can be used to register custom QueryExecutionListener for execution metrics. It is still marked as experimental though.</p><ol start="5"><li>Spark implicits</li></ol><p>Spark session provides with *<strong>spark.implicits._*</strong> which is 1 of the most useful imports in all of the spark packages which comes in handy with a lot of implicit methods for converting Scala objects to Datasets and some other handy utils.</p><pre class="line-numbers language-none"><code class="language-none">import spark.implicits.StringToColumngot.select($&quot;id&quot;)import spark.implicits.symbolToColumngot.select(&#39;id)import spark.implicits.newProductEncoderimport spark.implicits.localSeqToDatasetHolderval people &#x3D; Seq((1,&quot;male&quot;,false),(2,&quot;female&quot;,true)).toDF(&quot;id&quot;,&quot;gender&quot;,&quot;alive&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="6"><li>Create UDF’s</li></ol><pre class="line-numbers language-none"><code class="language-none">val capitalizeAndRemoveSpaces &#x3D; spark.udf.register(&quot;capitalizeAndRemoveSpaces&quot;, capAndRemoveSpaces(_: String): String)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="7"><li>SQL handler to run SQL queries and a SQL context.</li></ol><pre class="line-numbers language-none"><code class="language-none">spark.sql(&quot;SHOW TABLES&quot;).showspark.sqlContext.cacheTable(&quot;people_session1&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="8"><li>Utility method to calculate the time taken to run some function.</li></ol><pre class="line-numbers language-none"><code class="language-none">scala&gt;  spark.time(spark.catalog.cacheTable(&quot;people_session1&quot;))Time taken: 9 ms<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="9"><li>The table method gives a way of converting a hive table into a dataframe.</li></ol><pre class="line-numbers language-none"><code class="language-none">val peopleFromTable &#x3D; spark.table(&quot;people_session1&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="10"><li>The <code>**close**</code> and <code>**stop**</code> methods are provided to stop the underlying spark context.</li></ol><pre class="line-numbers language-none"><code class="language-none">scala&gt; session2.closescala&gt; session2.sparkContext.isStoppedres5: Boolean &#x3D; true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>Also, note that closing/stopping <code>session2</code> will kill <code>spark</code> spark session as well because the underlying spark context is the same.</p><pre class="line-numbers language-none"><code class="language-none">scala&gt; spark.sparkContext.isStoppedres6: Boolean &#x3D; true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>As always, Thanks for reading! Please do share the article, if you liked it. Any comments or suggestions are welcome! Check out my other articles <a href="https://medium.com/@achilleus">*<strong>here*</strong></a>.</p><p><a href="https://medium.com/@achilleus?source=post_sidebar--------------------------post_sidebar--------------">achilleus</a></p><p>Follow</p><p>ACHILLEUS FOLLOWS</p><ul><li><p><a href="https://medium.com/@Pinterest_Engineering?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------"><img src="https://miro.medium.com/fit/c/25/25/1*FNWs-r9nA_QNtBbYvUkAtg.png" class="lazyload placeholder" data-srcset="https://miro.medium.com/fit/c/25/25/1*FNWs-r9nA_QNtBbYvUkAtg.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="Pinterest Engineering"></a></p><p><a href="https://medium.com/@Pinterest_Engineering?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------">Pinterest Engineering</a></p></li><li><p><a href="https://alibabatech.medium.com/?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------"><img src="https://miro.medium.com/fit/c/25/25/1*doanyR6AmLHe1ZcnauMrXw.jpeg" class="lazyload placeholder" data-srcset="https://miro.medium.com/fit/c/25/25/1*doanyR6AmLHe1ZcnauMrXw.jpeg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="Alibaba Tech"></a></p><p><a href="https://alibabatech.medium.com/?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------">Alibaba Tech</a></p></li><li><p><a href="https://medium.com/@SeattleDataGuy?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------"><img src="https://miro.medium.com/fit/c/25/25/0*yQUyY8JtXeDsGnPR.jpg" class="lazyload placeholder" data-srcset="https://miro.medium.com/fit/c/25/25/0*yQUyY8JtXeDsGnPR.jpg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="SeattleDataGuy"></a></p><p><a href="https://medium.com/@SeattleDataGuy?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------">SeattleDataGuy</a></p></li><li><p><a href="https://netflixtechblog.medium.com/?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------"><img src="https://miro.medium.com/fit/c/25/25/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" class="lazyload placeholder" data-srcset="https://miro.medium.com/fit/c/25/25/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="Netflix Technology Blog"></a></p><p><a href="https://netflixtechblog.medium.com/?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------">Netflix Technology Blog</a></p></li><li><p><a href="https://medium.com/@anicolaspp?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------"><img src="https://miro.medium.com/fit/c/25/25/1*8gSpvHtBZmLTIzZW8dus9g@2x.jpeg" class="lazyload placeholder" data-srcset="https://miro.medium.com/fit/c/25/25/1*8gSpvHtBZmLTIzZW8dus9g@2x.jpeg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="Nicolas A Perez"></a></p><p><a href="https://medium.com/@anicolaspp?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------">Nicolas A Perez</a></p></li></ul><p><a href="https://medium.com/@achilleus/following?source=blogrolls_sidebar-----10d0d66d1d24-----------------------------------">See all (52)</a></p><p>847</p><p>10</p><p><strong>Related</strong></p><p><a href="https://medium.com/@chuck.connell.3/databricks-a-history-d8dd12fe9695?source=read_next_recirc---------0-------------------------------"><img src="https://miro.medium.com/focal/73/73/50/50/1*ykuOQuztWYCCANJBc31Uag.jpeg" class="lazyload placeholder" data-srcset="https://miro.medium.com/focal/73/73/50/50/1*ykuOQuztWYCCANJBc31Uag.jpeg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img">Databricks — A HistoryWhy was Databricks invented? What came before? What needs does it fill?</a></p><p><a href="https://medium.com/@robin.loche/how-to-work-with-multiple-languages-on-databricks-e22dea9f8c7?source=read_next_recirc---------1-------------------------------"><img src="https://miro.medium.com/focal/73/73/50/50/1*2zpUhCtoN3BTaujGifAxzg.png" class="lazyload placeholder" data-srcset="https://miro.medium.com/focal/73/73/50/50/1*2zpUhCtoN3BTaujGifAxzg.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img">How to work with multiple languages on Databricks</a></p><p><a href="https://fred-gu.medium.com/glue-spark-does-not-like-unsigned-int-from-mysql-2a73249e9c49?source=read_next_recirc---------2-------------------------------"><img src="https://miro.medium.com/focal/73/73/50/50/1*ve2-Te3Rs83Ij6OMrCLnMQ.png" class="lazyload placeholder" data-srcset="https://miro.medium.com/focal/73/73/50/50/1*ve2-Te3Rs83Ij6OMrCLnMQ.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img">Glue spark does not like Unsigned Int from MySQL</a></p><p><a href="https://fnuson.medium.com/calling-synapse-spark-jobs-using-service-principal-3d16c98574db?source=read_next_recirc---------3-------------------------------"><img src="https://miro.medium.com/focal/73/73/50/50/1*zmRmH90BbIGvudggiBZngw.jpeg" class="lazyload placeholder" data-srcset="https://miro.medium.com/focal/73/73/50/50/1*zmRmH90BbIGvudggiBZngw.jpeg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img">Calling Synapse Spark Jobs using Service Principal</a></p><ul><li><a href="https://medium.com/tag/spark">Spark</a></li><li><a href="https://medium.com/tag/big-data">Big Data</a></li><li><a href="https://medium.com/tag/analytics">Analytics</a></li><li><a href="https://medium.com/tag/programming">Programming</a></li><li><a href="https://medium.com/tag/technology">Technology</a></li></ul><p>847</p><p>10</p><h2 id="More-from-achilleus"><a href="#More-from-achilleus" class="headerlink" title="More from achilleus"></a><a href="https://medium.com/@achilleus?source=follow_footer-----10d0d66d1d24-----------------------------------">More from achilleus</a></h2><p>Follow</p><p><a href="https://medium.com/@achilleus/a-practical-introduction-to-sparks-column-part-2-1e52f1d29eb1?source=follow_footer-----10d0d66d1d24----0-------------------------------">Mar 23, 2019</a><a href="https://medium.com/@achilleus/a-practical-introduction-to-sparks-column-part-2-1e52f1d29eb1?source=follow_footer-----10d0d66d1d24----0-------------------------------">A practical introduction to Spark’s Column- part 2</a>This is a continuation of the <a href="https://medium.com/@achilleus/https-medium-com-achilleus-a-practical-introduction-to-sparks-column-3f5fe83125c9"><em>last article</em></a> wherein I covered some basic and commonly used Column functions. In this post, we will discuss some other common functions available.<a href="https://medium.com/@achilleus/a-practical-introduction-to-sparks-column-part-2-1e52f1d29eb1?source=follow_footer-----10d0d66d1d24----0-------------------------------"><img src="https://miro.medium.com/max/721/1*4E-Ggod3IRmewV9eTmzlIA.jpeg" class="lazyload placeholder" data-srcset="https://miro.medium.com/max/721/1*4E-Ggod3IRmewV9eTmzlIA.jpeg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img"></a>7) castLet’s say you have the below dataset and you want to validate the status based on the start and end date.</p>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
          <category> About Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWSEMRFilesystem</title>
      <link href="/2022/10/28/web-pa-chong-de-yi-chang-chu-li/"/>
      <url>/2022/10/28/web-pa-chong-de-yi-chang-chu-li/</url>
      
        <content type="html"><![CDATA[<h2 id="获取Web数据，撰写爬虫需要处理大量异常"><a href="#获取Web数据，撰写爬虫需要处理大量异常" class="headerlink" title="获取Web数据，撰写爬虫需要处理大量异常"></a>获取Web数据，撰写爬虫需要处理大量异常</h2><h2 id="不要使用except"><a href="#不要使用except" class="headerlink" title="不要使用except:"></a>不要使用except:</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">try</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span><span class="token keyword">except</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>使用except：是不好的行为。</p><h2 id="不要采取一直检测linux后台进程状态，根据进程状态去重启爬虫脚本的方法"><a href="#不要采取一直检测linux后台进程状态，根据进程状态去重启爬虫脚本的方法" class="headerlink" title="不要采取一直检测linux后台进程状态，根据进程状态去重启爬虫脚本的方法"></a>不要采取一直检测linux后台进程状态，根据进程状态去重启爬虫脚本的方法</h2><ol><li>尽量在脚本内部进行异常处理。因为脚本内部代码的执行，总会出现你意想不到的状况，粗暴的在系统层面强制重启进程一是会导致获取数据出现错误，二是可能会永远卡在某个错误上。</li></ol><h2 id="网页数据的获取包含大量的异常情况"><a href="#网页数据的获取包含大量的异常情况" class="headerlink" title="网页数据的获取包含大量的异常情况"></a>网页数据的获取包含大量的异常情况</h2><ol><li>网页服务器返回错误，异常需要处理，处理这一类异常的时候，需要确保确实是服务器返回的异常，比如服务器返回异常，某个用户不存在等等。<strong>注意在处理数据的时候，比如，某个用户的数据不存在，服务器返回异常，代表这个用户的数据为空，但是有些用户的数据确实为空，若两个都标记为””，那么后期不好区分究竟是这个用户的数据真的不存在还是只是一次获取失败。</strong></li><li>网页服务器返回为空（可能本地原因，也可能服务器端原因），也需要及时处理这种情况。</li><li>存储为json格式的时候，为方便后去处理，尽量少使用<code>[]</code>，多使用<code>&#123;&#125;</code>(字典格式)，可以考虑multiple json。</li></ol><h2 id="Headless-Chrome"><a href="#Headless-Chrome" class="headerlink" title="Headless Chrome"></a>Headless Chrome</h2><ol><li>谨慎处理本地<code>Chrome WebdriverException</code>异常。<ol><li>发送请求之后，一定使用<code>implicit_wait(time_out)</code></li></ol></li><li>Chrome需要消耗大量的内存，尽量使用Request。</li></ol><h2 id="想到再更…"><a href="#想到再更…" class="headerlink" title="想到再更…"></a>想到再更…</h2>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
          <category> About Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About Web </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws emr notebook attached successfully but still &quot;Workspace is not attached to cluster. Click Ok to continue.&quot;</title>
      <link href="/2022/10/28/aws-emr-notebook-attached-successfully-but-still-workspace-is-not-attached-to-cluster-click-ok-to-continue/"/>
      <url>/2022/10/28/aws-emr-notebook-attached-successfully-but-still-workspace-is-not-attached-to-cluster-click-ok-to-continue/</url>
      
        <content type="html"><![CDATA[<h2 id="aws-emr-notebook-attached-successfully-but-still-“Workspace-is-not-attached-to-cluster-Click-Ok-to-continue-”"><a href="#aws-emr-notebook-attached-successfully-but-still-“Workspace-is-not-attached-to-cluster-Click-Ok-to-continue-”" class="headerlink" title="aws emr notebook attached successfully but still “Workspace is not attached to cluster. Click Ok to continue.”"></a>aws emr notebook attached successfully but still “Workspace is not attached to cluster. Click Ok to continue.”</h2><ol><li>需要使用经过验证的的aws emr版本（youtube上按照别人的版本配置来）。</li><li>不要自己启动cluster，在创建notebook的时候启动cluster更好（使用默认配置就行）。</li><li>最好使用IAM USER而不是root account去创建和使用集群，stackoverflow上有人提到了这一点。</li></ol><h2 id="a-little-emo-for-wasting-two-full-days-of-my-life"><a href="#a-little-emo-for-wasting-two-full-days-of-my-life" class="headerlink" title="a little emo for wasting two full days of my life"></a>a little emo for wasting two full days of my life</h2><ol><li>hope bug fixed soon.</li></ol>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
          <category> About AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About AWS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nohup失效时</title>
      <link href="/2022/10/28/nohup-shi-xiao-shi/"/>
      <url>/2022/10/28/nohup-shi-xiao-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="使用screen命令可一定程度上模拟-Linux-screen命令-菜鸟教程-runoob-com"><a href="#使用screen命令可一定程度上模拟-Linux-screen命令-菜鸟教程-runoob-com" class="headerlink" title="[使用screen命令可一定程度上模拟](Linux screen命令 | 菜鸟教程 (runoob.com))"></a>[使用screen命令可一定程度上模拟](<a href="https://www.runoob.com/linux/linux-comm-screen.html">Linux screen命令 | 菜鸟教程 (runoob.com)</a>)</h2><p>screen为多重视窗管理程序。此处所谓的视窗，是指一个全屏幕的文字模式画面。通常只有在使用telnet登入主机或是使用老式的终端机时，才有可能用到screen程序。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">screen [-AmRvx -ls -wipe][-d &lt;作业名称&gt;][-h &lt;行数&gt;][-r &lt;作业名称&gt;][-s &lt;shell&gt;][-S &lt;作业名称&gt;]-A 　将所有的视窗都调整为目前终端机的大小。-d&lt;作业名称&gt; 　将指定的screen作业离线。-h&lt;行数&gt; 　指定视窗的缓冲区行数。-m 　即使目前已在作业中的screen作业，仍强制建立新的screen作业。-r&lt;作业名称&gt; 　恢复离线的screen作业。-R 　先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业。-s&lt;shell&gt; 　指定建立新视窗时，所要执行的shell。-S&lt;作业名称&gt; 　指定screen作业的名称。-v 　显示版本信息。-x 　恢复之前离线的screen作业。-ls或--list 　显示目前所有的screen作业。-wipe 　检查目前所有的screen作业，并删除已经无法使用的screen作业。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">screen -S xyz(起一个好记的名字)python xxx.py（启动你的程序）ctrl+a+d然后尽管退出。下次登进来之后，再执行：screen -r xyz就可以回到上次退出的界面<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
          <category> About Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMRNotebookCannotWriteFileThroughS3</title>
      <link href="/2022/10/28/emrnotebookcannotwritefilethroughs3/"/>
      <url>/2022/10/28/emrnotebookcannotwritefilethroughs3/</url>
      
        <content type="html"><![CDATA[<h1 id="自AWS-EMR5-x和6-x的某个版本后-AWS-EMR-Notebook在EMR创建的EC2机器上运行"><a href="#自AWS-EMR5-x和6-x的某个版本后-AWS-EMR-Notebook在EMR创建的EC2机器上运行" class="headerlink" title="自AWS EMR5.x和6.x的某个版本后, AWS EMR Notebook在EMR创建的EC2机器上运行"></a>自AWS EMR5.x和6.x的某个版本后, AWS EMR Notebook在EMR创建的EC2机器上运行</h1><h2 id="Notebook没有在EC2上写文件的权力"><a href="#Notebook没有在EC2上写文件的权力" class="headerlink" title="Notebook没有在EC2上写文件的权力"></a>Notebook没有在EC2上写文件的权力</h2><ul><li><p>利用AWS s3n, 写文件的时候(目前hadoop已经不再推荐s3n的使用, 推荐使用s3a), 需要在机器上创建tmp临时文件夹, 然而notebook并没有写文件的权限(<strong>原因待验证</strong>…), 因此, notebook仅仅方便于简单的代码测试. </p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">---------------------------------------------------------------------------Py4JJavaError                             Traceback (most recent call last)&lt;ipython-input-72-18e7cdbec7d6&gt; in &lt;module&gt;      5 #     )      6 all_df.write.format(&quot;csv&quot;).option(&quot;header&quot;, &quot;true&quot;).save(----&gt; 7         path &#x3D; &quot;s3n:&#x2F;&#x2F;process-roblox-data-bucket&#x2F;0_60.json&quot;, mode &#x3D; &quot;overwrite&quot;      8     )~&#x2F;.local&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;pyspark&#x2F;sql&#x2F;readwriter.py in save(self, path, format, mode, partitionBy, **options)    738             self._jwrite.save()    739         else:--&gt; 740             self._jwrite.save(path)    741     742     @since(1.4)~&#x2F;.local&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;py4j&#x2F;java_gateway.py in __call__(self, *args)   1308         answer &#x3D; self.gateway_client.send_command(command)   1309         return_value &#x3D; get_return_value(-&gt; 1310             answer, self.gateway_client, self.target_id, self.name)   1311    1312         for temp_arg in temp_args:~&#x2F;.local&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;pyspark&#x2F;sql&#x2F;utils.py in deco(*a, **kw)    109     def deco(*a, **kw):    110         try:--&gt; 111             return f(*a, **kw)    112         except py4j.protocol.Py4JJavaError as e:    113             converted &#x3D; convert_exception(e.java_exception)~&#x2F;.local&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;py4j&#x2F;protocol.py in get_return_value(answer, gateway_client, target_id, name)    326                 raise Py4JJavaError(    327                     &quot;An error occurred while calling &#123;0&#125;&#123;1&#125;&#123;2&#125;.\n&quot;.--&gt; 328                     format(target_id, &quot;.&quot;, name), value)    329             else:    330                 raise Py4JError(Py4JJavaError: An error occurred while calling o880.save.: org.apache.spark.SparkException: Job aborted.    at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)    at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)    at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)    at sun.reflect.GeneratedMethodAccessor116.invoke(Unknown Source)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)    at py4j.Gateway.invoke(Gateway.java:282)    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)    at py4j.commands.CallCommand.execute(CallCommand.java:79)    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)    at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 82.0 failed 1 times, most recent failure: Lost task 0.0 in stage 82.0 (TID 131) (ip-172-31-12-72.ec2.internal executor driver): java.io.IOException: fs.s3.buffer.dir not configured    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:311)    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:476)    at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:213)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.newBackupFile(NativeS3FileSystem.java:254)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.&lt;init&gt;(NativeS3FileSystem.java:236)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.create(NativeS3FileSystem.java:404)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)    at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.&lt;init&gt;(CsvOutputWriter.scala:38)    at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:85)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:146)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)    at org.apache.spark.scheduler.Task.run(Task.scala:131)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)Driver stacktrace:    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)    at scala.Option.foreach(Option.scala:407)    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)    ... 40 moreCaused by: java.io.IOException: fs.s3.buffer.dir not configured    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:311)    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:476)    at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:213)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.newBackupFile(NativeS3FileSystem.java:254)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.&lt;init&gt;(NativeS3FileSystem.java:236)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.create(NativeS3FileSystem.java:404)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)    at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.&lt;init&gt;(CsvOutputWriter.scala:38)    at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:85)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:146)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)    at org.apache.spark.scheduler.Task.run(Task.scala:131)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    ... 1 more<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>更新,利用AWS s3a, 写文件到s3可以成功, notebook进行代码测试的时候可以用s3a, 用s3n会报错(spark 3.x之后)</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">---------------------------------------------------------------------------Py4JJavaError                             Traceback (most recent call last)&lt;ipython-input-72-18e7cdbec7d6&gt; in &lt;module&gt;      5 #     )      6 all_df.write.format(&quot;csv&quot;).option(&quot;header&quot;, &quot;true&quot;).save(----&gt; 7         path &#x3D; &quot;s3n:&#x2F;&#x2F;process-roblox-data-bucket&#x2F;0_60.json&quot;, mode &#x3D; &quot;overwrite&quot;      8     )~&#x2F;.local&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;pyspark&#x2F;sql&#x2F;readwriter.py in save(self, path, format, mode, partitionBy, **options)    738             self._jwrite.save()    739         else:--&gt; 740             self._jwrite.save(path)    741     742     @since(1.4)~&#x2F;.local&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;py4j&#x2F;java_gateway.py in __call__(self, *args)   1308         answer &#x3D; self.gateway_client.send_command(command)   1309         return_value &#x3D; get_return_value(-&gt; 1310             answer, self.gateway_client, self.target_id, self.name)   1311    1312         for temp_arg in temp_args:~&#x2F;.local&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;pyspark&#x2F;sql&#x2F;utils.py in deco(*a, **kw)    109     def deco(*a, **kw):    110         try:--&gt; 111             return f(*a, **kw)    112         except py4j.protocol.Py4JJavaError as e:    113             converted &#x3D; convert_exception(e.java_exception)~&#x2F;.local&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;py4j&#x2F;protocol.py in get_return_value(answer, gateway_client, target_id, name)    326                 raise Py4JJavaError(    327                     &quot;An error occurred while calling &#123;0&#125;&#123;1&#125;&#123;2&#125;.\n&quot;.--&gt; 328                     format(target_id, &quot;.&quot;, name), value)    329             else:    330                 raise Py4JError(Py4JJavaError: An error occurred while calling o880.save.: org.apache.spark.SparkException: Job aborted.    at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)    at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)    at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)    at sun.reflect.GeneratedMethodAccessor116.invoke(Unknown Source)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)    at py4j.Gateway.invoke(Gateway.java:282)    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)    at py4j.commands.CallCommand.execute(CallCommand.java:79)    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)    at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 82.0 failed 1 times, most recent failure: Lost task 0.0 in stage 82.0 (TID 131) (ip-172-31-12-72.ec2.internal executor driver): java.io.IOException: fs.s3.buffer.dir not configured    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:311)    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:476)    at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:213)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.newBackupFile(NativeS3FileSystem.java:254)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.&lt;init&gt;(NativeS3FileSystem.java:236)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.create(NativeS3FileSystem.java:404)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)    at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.&lt;init&gt;(CsvOutputWriter.scala:38)    at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:85)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:146)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)    at org.apache.spark.scheduler.Task.run(Task.scala:131)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)Driver stacktrace:    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)    at scala.Option.foreach(Option.scala:407)    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)    ... 40 moreCaused by: java.io.IOException: fs.s3.buffer.dir not configured    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:311)    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:476)    at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:213)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.newBackupFile(NativeS3FileSystem.java:254)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.&lt;init&gt;(NativeS3FileSystem.java:236)    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.create(NativeS3FileSystem.java:404)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)    at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.&lt;init&gt;(CsvOutputWriter.scala:38)    at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:85)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:146)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)    at org.apache.spark.scheduler.Task.run(Task.scala:131)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    ... 1 more<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>真正进行大规模数据处理的时候, 可以SSH连接至master node, 将notebook上测试好的代码挂到机器上跑. 或者直接使用AWS的创建step的功能, 在这几个地方,可以直接使用s3://访问s3 bucket上的数据.</p></li></ul><h2 id="EMR-Notebook启动kernel失败的问题-好像在EMR6-5-0中已经被修复"><a href="#EMR-Notebook启动kernel失败的问题-好像在EMR6-5-0中已经被修复" class="headerlink" title="EMR Notebook启动kernel失败的问题, 好像在EMR6.5.0中已经被修复."></a>EMR Notebook启动kernel失败的问题, 好像在EMR6.5.0中已经被修复.</h2><h2 id="EMR-Notebook使用PySpark核时遇到下面的问题"><a href="#EMR-Notebook使用PySpark核时遇到下面的问题" class="headerlink" title="EMR Notebook使用PySpark核时遇到下面的问题"></a>EMR Notebook使用PySpark核时遇到下面的问题</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">Starting Spark applicationThe code failed because of a fatal error:    Session 0 did not start up in 60 seconds..Some things to try:a) Make sure Spark has enough available resources for Jupyter to create a Spark context.b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.c) Restart the kernel.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解决办法: 未知…但是可以换成python核手动安装spark去处理…知道的欢迎留言告诉我.</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">%pip install spark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
          <category> About AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About AWS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWSEMRFilesystem</title>
      <link href="/2022/10/28/awsemrfilesystem/"/>
      <url>/2022/10/28/awsemrfilesystem/</url>
      
        <content type="html"><![CDATA[<h1 id="Latest-AWS-EMR-Filesystem-recommended"><a href="#Latest-AWS-EMR-Filesystem-recommended" class="headerlink" title="Latest AWS EMR Filesystem(recommended)"></a>Latest AWS EMR Filesystem(recommended)</h1><p><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html">Work with storage and file systems - Amazon EMR</a></p><h1 id="About-old-s3n-s3a-etc"><a href="#About-old-s3n-s3a-etc" class="headerlink" title="About old s3n, s3a, etc."></a>About old s3n, s3a, etc.</h1><h2 id="s3-s3n-s3a-from-stackoverflow"><a href="#s3-s3n-s3a-from-stackoverflow" class="headerlink" title="s3,s3n,s3a from stackoverflow"></a>s3,s3n,s3a from <a href="https://stackoverflow.com/questions/33356041/technically-what-is-the-difference-between-s3n-s3a-and-s3">stackoverflow</a></h2><p>in Apache Hadoop, “s3://“ refers to the original S3 client, which used a non-standard structure for scalability. That library is deprecated and soon to be deleted,</p><p>s3n is its successor, which used direct path names to objects, so you can read and write data with other applications. Like s3://, it uses jets3t.jar to talk to S3.</p><p>On Amazon’s EMR service, s3:// refers to Amazon’s own S3 client, which is different. A path in s3:// on EMR refers directly to an object in the object store.</p><p>In Apache Hadoop, S3N and S3A are both connectors to S3, with S3A the successor built using Amazon’s own AWS SDK. Why the new name? so we could ship it side-by-side with the one which was stable. S3A is where all ongoing work on scalability, performance, security, etc, goes. S3N is left alone so we don’t break it. S3A shipped in Hadoop 2.6, but was still stabilising until 2.7, primarily with some minor scale problems surfacing.</p><p>If you are using Hadoop 2.7 or later, use s3a. If you are using Hadoop 2.5 or earlier. s3n, If you are using Hadoop 2.6, it’s a tougher choice. -I’d try s3a and switch back to s3n if there were problems-</p><p>For more of the history, see <a href="http://hortonworks.com/blog/history-apache-hadoops-support-amazon-s3/">http://hortonworks.com/blog/history-apache-hadoops-support-amazon-s3/</a></p><p><strong>2017-03-14 Update</strong> actually, partitioning is broken on S3a in Hadoop 2.6, as the block size returned in a <code>listFiles()</code> call is 0: things like Spark &amp; pig partition the work into one task/byte. You cannot use S3a for analytics work in Hadoop 2.6, even if core filesystem operations &amp; data generation is happy. Hadoop 2.7 fixes that.</p><p><strong>2018-01-10 Update</strong> Hadoop 3.0 has cut its s3: and s3n implementations: s3a is all you get. It is now significantly better than its predecessor and performs as least as good as the Amazon implementation. Amazon’s “s3:” is still offered by EMR, which is their closed source client. Consult the <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html">EMR docs</a> for more info.</p><h2 id="Which-S3-file-system-should-I-use-with-Amazon-Elastic-MapReduce-Amazon-EMR-S3N-S3A-or-S3"><a href="#Which-S3-file-system-should-I-use-with-Amazon-Elastic-MapReduce-Amazon-EMR-S3N-S3A-or-S3" class="headerlink" title="Which S3 file system should I use with Amazon Elastic MapReduce (Amazon EMR): S3N, S3A, or S3?"></a><a href="https://web.archive.org/web/20170718025436/https://aws.amazon.com/premiumsupport/knowledge-center/emr-file-system-s3/">Which S3 file system should I use with Amazon Elastic MapReduce (Amazon EMR): S3N, S3A, or S3?</a></h2><h3 id="Issue"><a href="#Issue" class="headerlink" title="Issue"></a>Issue</h3><p>Apache Hadoop provides the following filesystem clients for reading from and writing to Amazon S3:</p><ul><li>S3N (URI scheme: <strong>s3n</strong>) - A native filesystem for reading and writing regular files on S3. S3N allows Hadoop to access files on S3 that were written with other tools, and conversely, other tools can access files written to S3N using Hadoop. S3N is stable and widely used, but it is not being updated with any new features. S3N requires a suitable version of the jets3t JAR on the classpath.</li><li>S3A (URI scheme: <strong>s3a</strong>) - Hadoop’s successor to the S3N filesystem. S3A uses Amazon’s libraries to interact with S3. S3A supports accessing files larger than 5 GB, and it provides performance enhancements and other improvements. For Apache Hadoop, S3A is the successor to S3N and is backward compatible with S3N. Using Apache Hadoop, all objects accessible from s3n:// URLs should also be accessible from S3A by replacing the URL scheme.<br><strong>Note</strong><br>Amazon EMR does not currently support use of the Apache Hadoop S3A file system.</li><li>S3 (URI scheme: <strong>s3</strong>) - Apache Hadoop implementation of a block-based filesystem backed by S3. Apache Hadoop has deprecated use of this filesystem as of May 2016.</li></ul><p>Amazon EMR uses the s3 URI scheme in the <a href="https://web.archive.org/web/20170718025436/https://aws.amazon.com/documentation/elastic-mapreduce/">EMR documentation</a>. Which of these three URI schemes should I use with EMR?</p><h3 id="Short-Description"><a href="#Short-Description" class="headerlink" title="Short Description"></a>Short Description</h3><p>Because of the differences between the Apache Hadoop S3 file systems and Amazon EMR S3 file systems, it is not always clear which URI scheme and filesystem to use with Amazon EMR.</p><h3 id="Resolution"><a href="#Resolution" class="headerlink" title="Resolution"></a>Resolution</h3><p>For Amazon EMR, both the s3:// and s3n:// URIs are associated with the EMR filesystem and are functionally interchangeable in the context of Amazon EMR. For consistency sake, however, it is recommended to use the s3:// URI in the context of Amazon EMR.</p><p>The s3a:// URI is not compatible with Amazon EMR. For more information, see <a href="https://web.archive.org/web/20170718025436/http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-plan-file-systems.html">File Systems Compatible with Amazon EMR</a> and <a href="https://web.archive.org/web/20170718025436/https://wiki.apache.org/hadoop/AmazonS3">S3 Support in Apache Hadoop</a>.</p><h3 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h3><p>S3, S3N, S3A, Hadoop file system, HDFS, EMRFS</p>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
          <category> About AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About AWS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Automatic Reliability Testing for Cluster Management Controllers</title>
      <link href="/2022/10/28/automatic-reliability-testing-for-cluster-management-controllers/"/>
      <url>/2022/10/28/automatic-reliability-testing-for-cluster-management-controllers/</url>
      
        <content type="html"><![CDATA[<h1 id="Automatic-Reliability-Testing-for-Cluster-Management-Controllers"><a href="#Automatic-Reliability-Testing-for-Cluster-Management-Controllers" class="headerlink" title="Automatic Reliability Testing for Cluster Management Controllers"></a>Automatic Reliability Testing for Cluster Management Controllers</h1><p><strong>作者根据insight给的一些可能的错误类型，对k8s的调度器调用API的接口进行了一个封装，通过可能的错误类型生成了一些测试计划，找到了一些bug。</strong></p><p>Borg, Omega and Kubernetes 依赖state-reconciliation principle去变得高弹性和可拓展性，所有的<strong>集群管理</strong>的<strong>逻辑</strong>即<strong>控制器</strong>被嵌入在松散耦合的微服务中（这些<strong>集群管理的逻辑就是微服务</strong>）。每一个控制器独立的观测当前的集群的状态，采取特定的措施使得当前的集群满足特定的状态。复杂的分布式的本质是建立稳定性强的正确的控制器很难，控制器面临着<strong>无数的可靠性问题</strong>，导致<strong>数据损失，安全和资源泄漏</strong>。</p><p><strong>Sieve</strong>，测试集群控制器的工具，通过不断的干扰控制器看到的当前集群的状态，然后去比较集群受到干扰和没受到干扰时候的状态去检测安全和liveness的问题。Sieve的设计基于基本<strong>状态恢复系统的基本机会</strong>，这些系统基于<strong>state-centric interfaces between controllers and cluster state</strong>，Sieve找到了46个严重的安全和liveness的bug，35个确认，22个修复，false-positive比例是$3.5%$</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>控制器遵循<strong>state-reconciliation principle</strong>，reconciles the current state of the cluster to match a desired state. 集群的状态放在逻辑上中心化的，高可用的数据中心，K8S里面的<strong>pods、nodes、volumns和应用实例都被表示为集群中状态的对象</strong>。</p><p><img src="/.io//123.png" class="lazyload placeholder" data-srcset="/.io//123.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>Kubernetes controllers管理Cassandra的时候，删掉pod和finalizing中间如果受到干扰，vol没删掉，就会导致storage的泄漏。</p><p><strong>优点</strong>：高可用，1) 不需要正式的对控制器和集群管理的配置说明文件 2) 不需要假设代码可能在哪里出现bug 3) 高度专业化的测试输入。<strong>只需要创建控制器镜像和基本测试工作流的表示，自动化测试</strong>，即只需要提供manifest说明如何在测试下创建和部署控制器，以及一系列的测试工作流（成熟的控制器一般都有）。</p><p><strong>insight</strong>：认为state-centric interfaces非常适合去观察和干扰控制器的view。<strong>控制器的动作是当前它看到的集群状态的严格函数</strong>。</p><p>1） 自动的修改了控制器声称支持的观测到的集群状态。</p><p>2）自动的使用生成的测试计划，标记安全和liveness的问题。（可以这么做是由于state-centric interfaces的一些特点，这样的系统往往<strong>有简单的高度内聚的</strong>state centric的接口，这种接口基本上只做读写和接收状态改变的操作，并且所有的对象共享策略（例如k8s里面就用同样的域去表示metadata））。</p><p><strong>测试方法：</strong>干扰控制器的view，插入1）中间状态 2）已经失效的状态 3）未观测的状态，根据控制器的状态和行为去生成<strong>测试计划</strong>，避免冗余和无效的测试计划。</p><h2 id="Bg"><a href="#Bg" class="headerlink" title="Bg"></a>Bg</h2><p><strong>k8s的架构</strong>：</p><p><img src="/.io//123-1667563760827-2.png" class="lazyload placeholder" data-srcset="/.io//123-1667563760827-2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>k8s的核心由API服务器的集成和高可用、高一致性的数据存储（etcd）组成，etcd里面存储了<strong>对象状态（pods、volumns、nodes、groups of applications）</strong>，所有k8s中的其他组件都和k8s进行交互。控制器调用API Server中的API去获得Object的状态。</p><p>好处：可拓展性强，增加新的应用的时候只需要增加新的控制器和新的State Object。</p><p><strong>k8s的控制器的工作方式</strong>：</p><p><img src="/.io//123-1667564276682-4.png" class="lazyload placeholder" data-srcset="/.io//123-1667564276682-4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>部署ZooKeeper集群的时候，用户<strong>首先创建</strong>一个ZooKeeper的对象，这个<strong>对象</strong>指定了用户想要的集群的状态，例如多少个，版本，存储的大小，然后<strong>ZooKeeper Controller</strong>接受到ZooKeeper对象被创建的消息，就想方设法达到用户制定的状态，首先创建一个StatefulSet的对象（有状态应用的抽象），然后一个a StatefulSet controller随后被告知一个StatefulSet的对象被创建了，然后轮流的创建pod和volumn，之后scheduler，storage controller，worker都被创建去带来实际的container和volumn。如果这个时候用户编辑了ZooKeeper对象时候，每一个控制器就都会微调达到对应的状态。</p><p><strong>这个测试很重要，控制器的可靠性很难保证，当前的测试方法不行</strong>。</p><h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p><strong>Sieve tests controllers with the following workflow：</strong></p><ul><li>Collecting reference traces：学习控制器在没有错误时候的表现（测试工作流下），记录这个时候的状态转移，从而对控制器和集群状态交互的接口进行检测。</li><li>生成测试计划：根据参考轨迹生成测试计划（描述了具体的干扰），描述了要注入什么错误，什么时候注入错误。</li><li>避免无效的测试计划：删除多余或者无用的测试计划（例如明显不会导致错误的测试计划）。</li><li>执行测试计划：使用test coordinator，执行每一个测试计划，<strong>test coordinator</strong>监控集群测试时候的状态转变，注入错误。</li><li>检查测试结果：generic, effective, differential oracles to automatically check test results</li></ul><h4 id="如何Perturbing控制器的state-view"><a href="#如何Perturbing控制器的state-view" class="headerlink" title="如何Perturbing控制器的state view"></a>如何Perturbing控制器的state view</h4><p>注入特定的错误（crash、delay、connection change）。</p><p><strong>decouple policy from mechanism</strong>：有利于拓展当前的策略，通过编排潜在的干扰机制增加新的策略。<strong>策略</strong>：a view Sieve exposes to the controller at a particular condition. <strong>机制</strong>：说明了如何注入错误去create a view。</p><h4 id="这三种干扰分别是什么？"><a href="#这三种干扰分别是什么？" class="headerlink" title="这三种干扰分别是什么？"></a>这三种干扰分别是什么？</h4><p><strong>Intermediate states</strong>：中间状态指的是控制器还没完成所有状态更新之前中间的一些状态，在controller失败后，k8s就会开一个新的实例，恢复之前的中间状态。</p><p><img src="/.io//Snipaste_2022-11-04_21-35-34.png" class="lazyload placeholder" data-srcset="/.io//Snipaste_2022-11-04_21-35-34.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>例如RabbitMQ控制器，找到了一个bug：</p><p>workload：尝试将存储从10GB-&gt;15GB,1）首先更新VolCur为15GB，然后更新VolReq为15GB（触发k8s）resize大小。在更新VolCur和VolReq的时候Sieve挂了，但是却不能正确恢复，已经被700行go代码修复了。</p><p><strong>Stale states</strong>：</p><p>图2可以看出来，控制器不直接和consist data stores去交互，而是和API server去交互，API server的话里面的状态可能会受到delayed notifications的影响。</p><p>假如有多个API Server都可用的时候，可能有一些API Server的状态比较新，有的状态不是很新，要是控制器刚开始连的是一个比较新的API Server，然后又连接了一个比较旧的Server（由于负载均衡等原因），控制器可能会做重新配置，但是控制器不应该这么做，应该正确识别这些错误。</p><p>Percona’s MongoDB，找到一个新bug：</p><p><img src="/.io//Snipaste_2022-11-04_21-45-19.png" class="lazyload placeholder" data-srcset="/.io//Snipaste_2022-11-04_21-45-19.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>关闭MongoDB集群的时候，controller等着看见MongoDB的state object的删除的时间戳，控制器看到这个变化的时候，就会删除所有的pods和volumes。</p><p>Sieve让这个控制器就错误的删除了一个live的MongoDB的集群。有一个工作流首先关闭了MongoDB的集群然后重新创建了一个同样的集群，等到新的集群创建完成后，Sieve就引入了一个这样的错误，使得controller把这个集群给删了。</p><p><strong>Unobserved states</strong>：现在的controller设计成level-triggered systems(opposed to being edge-triggered)，和别的设计不同的是，别的设计观测所有的集群状态的改变，做这所有的状态的改变，但是level-triggered systems就只根据当前的状态去控制转变集群的状态.</p><p>Instaclustr’s Cassandra controller，找到了一个bug导致资源泄露和服务失败：</p><p><img src="/.io//Snipaste_2022-11-04_21-56-52.png" class="lazyload placeholder" data-srcset="/.io//Snipaste_2022-11-04_21-56-52.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>在scale-down的时候，controller应该得知pods被删除后移除了所有的volumns，而pods的生命周期是由StatefulSet Controller管理的，Sieve就暂停了了发送给Cassandra控制器的消息，因此Cassandra不知道pods被删除了，这导致Cassandra控制器没有删除对应的volumns。</p><h4 id="如何收集Reference-Traces"><a href="#如何收集Reference-Traces" class="headerlink" title="如何收集Reference Traces"></a>如何收集Reference Traces</h4><p>通过封装API Server的函数，封装了10个函数，在这里面注入干扰。</p><p>由此跑一下所有开发者提供的workload，通过学习每一个controller收到的集群状态改变的提醒，和控制器对集群状态的任何读和写的操作，或者是对API Server的client（也就是controller机器自己）维护的local cache去收集两方面的reference traces：</p><ul><li><strong>Controller trace</strong>：一系列使用client的API就可以观测的时间，例如状态改变的notification，reconciliation cycle的entry和exits，client-API的invocation（调用）</li><li><strong>Cluster state trace</strong>：初始化的集群状态和状态改变的序列。</li></ul><h4 id="如何使用Reference-Traces生成测试计划："><a href="#如何使用Reference-Traces生成测试计划：" class="headerlink" title="如何使用Reference Traces生成测试计划："></a>如何使用Reference Traces生成测试计划：</h4><p>每一个测试计划做一种干扰。</p><p><strong>测试计划</strong>：每一个测试计划由self-contained的文件构成，这个文件描述了<strong>测试工作流、一些列的注入的错误、注入错误这一行为的触发条件</strong>。</p><p>目前支持：</p><ul><li>crash/restart a controller</li><li>disconnect/reconnect a controller to an API server</li><li>block/unblock a controller from processing events</li><li>block/unblock an API server from processing events</li></ul><p>根据测试计划就可以复现bug。</p><p><img src="/.io//image-20221104221955496.png" class="lazyload placeholder" data-srcset="/.io//image-20221104221955496.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221104221955496"></p><h4 id="测试计划生成"><a href="#测试计划生成" class="headerlink" title="测试计划生成"></a>测试计划生成</h4><p>制定了一系列规则，在这些规则里生成计划，<strong>这个说法很有意思，不说如何制定测试状态，因为大家看到你是怎么制定的就感觉就这，但是避免无用的测试计划这个说法让我感觉很牛</strong>。</p><ul><li><strong>Intermediate-state rule</strong>：集群从一个状态更新到另一个状态要若干步骤，对于若干个步骤$U_{1}, U_{2}…$，做一个步骤就让Controller崩溃一遍，生成一个测试计划。</li><li><strong>Stale-state rule</strong>：不停的让controller travel back in time，然后给他看以前的状态，方法就是不断的做一些可能会带来冲突效果的操作，去观察controller是否正常。具体的做法是，首先观测到更新U的N个结果，然后搜索之后可能的N’个结果，尽量用例如先删了一个object，再创建一个object这样可能会导致冲突的操作。</li><li><strong>Unobserved-state rule</strong>：跳过可能的正常状态的改变，对于状态对(N, N’)，这样生成测试计划1）先不让控制器看到状态N 2）当N’到来的时候让控制器看到N。</li></ul><h4 id="如何避免无用的测试计划"><a href="#如何避免无用的测试计划" class="headerlink" title="如何避免无用的测试计划"></a>如何避免无用的测试计划</h4><p>依据<strong>测试计划生成产生了大量的测试计划</strong>，太多了，光stale-state就在MongoDB Controller上生成了140000+测试计划。</p><p><strong>a guiding principle</strong>：</p><ul><li>prune a test plan if the test plan does not introduce an intermediate-, a stale- or an unobservedstate that can affect the controller’s outputs.</li><li>the introduced state is identical to states introduced by other test plans.</li></ul><p><strong>3.4.1 Pruning by Causality</strong></p><p>根据<strong>因果关系</strong>（更新U是基于状态N做出来的，那么N和U是因果相关的）筛选。因果关系很难做、现在的k8s不好做因果分析。</p><p>所以制定了<strong>两个规则（个人认为：基于局部性原理）</strong>，可能会导致<strong>false positive</strong>的错误：</p><ul><li>Read-before-update rule：the object pertaining to N is read by the controller before it issues U。更新U是在读取N之后作出的。<strong>个人理解：</strong>刚读过状态就去更新，那么更新U大概率和N相关。</li><li>Earliest-reconciliation rule：N and U happen in the same or adjacent reconciliation cycles.  N和U发生在相同或者相近的reconciliation周期中，可以理解。</li></ul><p><strong>例如只保留至少一个U和N因果相关的测试计划</strong>。</p><p><strong>3.4.2 Pruning Unsuccessful Updates</strong></p><p><strong>忽略任何不改变当前集群状态的更新</strong>。</p><p>理由就是：如果一个更新没有改变当前的集群的状态，就不大可能导致新的状态出现。</p><h4 id="Test-Plan-Execution"><a href="#Test-Plan-Execution" class="headerlink" title="Test Plan Execution"></a>Test Plan Execution</h4><p>由Sieve测试协调器执行。</p><h4 id="Differential-Test-Oracles"><a href="#Differential-Test-Oracles" class="headerlink" title="Differential Test Oracles"></a>Differential Test Oracles</h4><p><strong>3.6.1 Checking End States</strong></p><p><strong>3.6.2 Checking State-Update Summaries</strong></p>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Serverless </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率论部分内容复习</title>
      <link href="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/"/>
      <url>/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="概率论部分内容复习"><a href="#概率论部分内容复习" class="headerlink" title="概率论部分内容复习"></a>概率论部分内容复习</h1><h2 id="卡方分布"><a href="#卡方分布" class="headerlink" title="卡方分布"></a>卡方分布</h2><p>设$X_{1}, X_{2}, …, X_{n}$是来自总体$N(0, 1)$的样本，则称统计量</p><p>$$\chi^2=X_{1}^{2}+X_{2}^{2}+…+X_{n}^{2}$$</p><p>服从自由度为n的$\chi^{2}$分布。记为$\chi^{2}\sim \chi^{2}(n)$.</p><ol><li><p><strong>满足可加性：即从总体中分两次抽取$n1$和$n2$个样本的平方和，和一次抽取$n1+n2$个样本的平方和是相等的。</strong></p><p>$$\chi_{1}^{2}+ \chi_{2}^{2} \sim \chi^{2}(n_{1}+n_{2})$$</p></li></ol><h2 id="t分布"><a href="#t分布" class="headerlink" title="t分布"></a>t分布</h2><p>若$X \sim N(0, 1)$, $Y\sim\chi^{2}(n)$，且X, Y相互独立，则称随机变量:</p><p>$t=\frac{X}{\sqrt{Y/n}}$服从自由度为n的t分布。记为$t \sim t(n)$。</p><p><strong>t分布怎么来的？</strong></p><p><strong>源自对$\frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0, 1)$的研究。</strong></p><p>也就是根据中心极限定理，从总体X中抽出n个样本，样本的均值为$\overline{X}$，总体的均值为$\mu$，方差为$\sigma$，样本的方差为$\sigma_{\overline{X}}$：</p><p>$\overline{X}\sim N(\mu, \sigma_{\overline{X}}^{2})$即$\frac{\overline{X}-\mu}{\sigma_{\overline{X}}}\sim N(0, 1)$即$t=\frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0, 1)$，为自由度问$n-1$的t分布。</p><h2 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h2><p>和的分布收敛于正态分布的定理如：</p><p><strong>样本均值$\overline{X}\sim N(\mu,\frac{\sigma^{2}}{n})$。</strong></p><h2 id="大数定律"><a href="#大数定律" class="headerlink" title="大数定律"></a>大数定律</h2><p><strong>样本均值依概率收敛于期望值</strong></p><h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2><ul><li>原假设$H_0$：实验之前已有的假设，AB两次测试的差距为0.</li><li>备择假设$H_{1}$：对立于原假设。</li><li><strong>先对总体的特征作出某种假设，然后通过抽样研究的统计推理，对此假设应该被拒绝还是接受作出判断。</strong></li></ul><h3 id="两类错误"><a href="#两类错误" class="headerlink" title="两类错误"></a>两类错误</h3><ul><li><p>原假设为真，即$\mu=\mu_{0}$，无显著性差异。我们却不接受结果，叫<strong>弃真错误，第一类错误</strong>，犯错概率为$\alpha$。<strong>即实际为真，但是样本却抽取到了让我们判断结果为假的情况。</strong></p></li><li><p><strong>显著性水平（犯错的概率）</strong>越大，那么原假设被拒绝的可能性就越大，犯第一类错误的可能性也就越大。<strong>p值</strong>即为在观测数据下拒绝原假设的最小<strong>显著性水平</strong>。</p></li><li><p>原假设为假，即$\mu\neq\mu_{0}$，有显著性差异。我们却接受结果，叫<strong>取伪错误，第二类错误</strong>，犯错概率为$\beta$。<strong>即实际为假，但是样本却抽取到了让我们判断结果为真的情况。</strong></p></li></ul><p><img src="/.io//123.png" class="lazyload placeholder" data-srcset="/.io//123.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="如何减少两类错误"><a href="#如何减少两类错误" class="headerlink" title="如何减少两类错误"></a>如何减少两类错误</h3><p><img src="/.io//123-1667132397600-2.png" class="lazyload placeholder" data-srcset="/.io//123-1667132397600-2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="显著性水平"><a href="#显著性水平" class="headerlink" title="显著性水平"></a>显著性水平</h3><p><img src="/.io//123-1667132490168-4.png" class="lazyload placeholder" data-srcset="/.io//123-1667132490168-4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="p值计算"><a href="#p值计算" class="headerlink" title="p值计算"></a>p值计算</h3><p><img src="/.io//123-1667133450982-6.png" class="lazyload placeholder" data-srcset="/.io//123-1667133450982-6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="置信水平和区间"><a href="#置信水平和区间" class="headerlink" title="置信水平和区间"></a>置信水平和区间</h3><p><img src="/.io//123-1667133632548-8.png" class="lazyload placeholder" data-srcset="/.io//123-1667133632548-8.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p><img src="/.io//123-1667133747607-10.png" class="lazyload placeholder" data-srcset="/.io//123-1667133747607-10.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p><strong>理解：置信区间可以理解成总体均值等量用样本表示时候，往往在什么样的范围内，在这个范围内的概率就是置信水平。</strong></p><ul><li>为什么要置信区间，因为<strong>误差不可避免</strong>，<strong>置信区间即为统计量的误差范围</strong>。比如用$[a,b]$表示样本估计总体均值的误差范围，那么这一结果具有的<strong>可信程度</strong>，就是置信度。</li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>t检验和z检验都可以用来判断样本，几次样本之间的<strong>均值</strong>是否显著，卡方检验用于判断样本偏差是否合理，即用来判断检验抽样是否合理。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>2021.01.18: &lt;&lt;概率论与数理统计&gt;&gt;</li><li>2021.08.22: <a href="https://zhuanlan.zhihu.com/p/346602966">https://zhuanlan.zhihu.com/p/346602966</a>, <a href="https://www.zhihu.com/question/24801731">https://www.zhihu.com/question/24801731</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> About Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Reading </tag>
            
            <tag> About Math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百年孤独</title>
      <link href="/2022/02/05/bai-nian-gu-du/"/>
      <url>/2022/02/05/bai-nian-gu-du/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><ul><li>第一次听说《百年孤独》，是从初中英语老师马ruhong老师那听说的，她有几天的英语课一直在给我们说这本书很棒，叫我们一定要去读。这个英语老师非常的严厉，我还记得当时我整天沉迷于漫画书，小说，网游，她脸上抑制不住的怒其不争，后来视力在一次又一次的通宵达旦中涨到了300°。《百年孤独》从此留在了我的脑海中。</li><li>看过两次《百年孤独》，我看过的这本书是在街边的小摊上买的，封面上面是白矩形，下面是黑矩形，书中有很多的错别字，翻译的也十分的糟糕。</li><li>第一次读《百年孤独》，是在初中暑假，那时候纯粹是当作小黄文看，看完过后，只记得书中的刺激了，后来干脆这一部分也忘掉了。</li><li>第二次读《百年孤独》，是在高中，具体时间记不得了，看完过后，多记得了点东西，这本书很魔幻。</li><li>还没有迎来第三次阅读，下次一定要认认真真的，慢慢悠悠地读完。</li><li>这次的记录完全是因为最近半个月心情异常的郁闷与糟糕，胡子拉碴，邋遢不堪，虽然是过年，但是没有走亲戚。突然想起这本书，又没有更多的时间去仔细看，便去找了解说。</li></ul><h1 id="感谢Up主的分享"><a href="#感谢Up主的分享" class="headerlink" title="感谢Up主的分享"></a>感谢<a href="https://www.bilibili.com/video/BV1tP4y1A7kM">Up主</a>的分享</h1><blockquote><ul><li>无论在什么地方，你都要记住，过去都是假的。回忆没有归路，春天总是一去不复返，最疯狂执着的爱情，也终究是过往云烟。</li><li>家族的第一个人被堃在树上，最后一个人正在被蚂蚁吃掉。而奥雷迪亚诺，他不可能再走出整个房间了。这座镜子之城，将在小奥读完羊皮卷时被飓风抹去，从世人的记忆中根除，羊皮卷上所记载的一切，自永远至永远不会再重复，因为注定经受百年孤独的家族，不会有第二次机会出现在大地上。</li><li>拉丁美洲的历史，是一系列代价而徒劳的奋斗集合，是一幕幕事先要注定被人遗忘的戏剧集合，至今在我们中间健忘症仍然存在，只要时过境迁，谁也不会记得香蕉工人横遭屠杀的惨案，谁也不会想起奥雷迪亚诺上校。<img src="/.io//IMG_1433.PNG" class="lazyload placeholder" data-srcset="/.io//IMG_1433.PNG" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></blockquote><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><blockquote><ul><li><p>我们完成了很多的事情，但最后这些事情似乎注定都是会被遗忘，我们一无所有的来，也终将一无所有的离去。</p></li><li><p><strong>记忆才是抵御孤独最大的武器</strong>。<br><img src="/.io//IMG_1441.PNG" class="lazyload placeholder" data-srcset="/.io//IMG_1441.PNG" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p></li><li><p>回忆会比经历要长的多。可怕的不是孤独，可怕的是我们从来不敢独自去面对孤独，可怕的是我们以为我们自己从不孤独。<br><img src="/.io//IMG_1443.PNG" class="lazyload placeholder" data-srcset="/.io//IMG_1443.PNG" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p></li><li><p><font color="#FF0000"><strong>能够抵御孤独的，唯有过往的记忆，还有对未来的期许！</strong></font></p></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> About Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cfw</title>
      <link href="/2022/01/30/cfw/"/>
      <url>/2022/01/30/cfw/</url>
      
        <content type="html"><![CDATA[<h1 id="最近配置cfw走系统代理"><a href="#最近配置cfw走系统代理" class="headerlink" title="最近配置cfw走系统代理"></a>最近配置cfw走系统代理</h1><p><a href="https://github.com/Fndroid/clash_for_windows_pkg/issues">issues</a><br><a href="https://docs.cfw.lbyczf.com/">一个很好的教程</a></p>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About VPN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于论文写作</title>
      <link href="/2022/01/25/guan-yu-lun-wen-xie-zuo/"/>
      <url>/2022/01/25/guan-yu-lun-wen-xie-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="毕业论文"><a href="#毕业论文" class="headerlink" title="毕业论文"></a>毕业论文</h1><h2 id="格式上注意点"><a href="#格式上注意点" class="headerlink" title="格式上注意点"></a>格式上注意点</h2><ul><li>导言，n级标题和n+1级标题之间一般加上一段导言。例如：小节标题<strong>相关工作</strong>和具体内容之间加上一段说明哪些方式介绍相关工作。会议论文duck不必。</li><li>段落，论文的段落，开头可以空可以不空，但是最好保持整体风格的一致。</li></ul><h2 id="内容上注意点"><a href="#内容上注意点" class="headerlink" title="内容上注意点"></a>内容上注意点</h2><ul><li>相关工作&gt;=3页。会议论文duck不必。可以顺便讲一下和本文工作之间的关系。</li><li></li></ul><h2 id="word撰写论文"><a href="#word撰写论文" class="headerlink" title="word撰写论文"></a>word撰写论文</h2><ol><li>首行缩进，不用空格和Tab，尽量右键点击选择段落，首行缩进。</li><li>排比中间用顿号。</li><li>尽量使用word提供的标号方式。</li><li>论文的引用，按住ctrl+鼠标左键可以跳过去。word中体现在<strong>交叉引用</strong>，全选，按F9可以立马更新交叉引用的域。</li></ol><h1 id="通用"><a href="#通用" class="headerlink" title="通用"></a>通用</h1><h2 id="关注的问题不要太大"><a href="#关注的问题不要太大" class="headerlink" title="关注的问题不要太大"></a>关注的问题不要太大</h2><ol><li>集中经历分析某个具体的问题。</li></ol><h2 id="一般可以采用总分结构"><a href="#一般可以采用总分结构" class="headerlink" title="一般可以采用总分结构"></a>一般可以采用总分结构</h2><ol><li>一个模块，比如背景模块，开头总写，段落分写。</li><li>一个段落，比如介绍App分析工作的一个段落，开头总写，内容分写。</li><li>一个概念，先给出定义，后进行解释。</li><li>一种架构，先给出架构，后进行剖析。</li><li>一种优点，先给出总纲，后进行分析。<br>……</li></ol><h2 id="论文的背景部分"><a href="#论文的背景部分" class="headerlink" title="论文的背景部分"></a>论文的背景部分</h2><ol><li>当前学术界、工业界研究状况，绘制出当前发展状况，未来发展前景的美好画卷。</li><li>需要指出当前研究工作的不足，自己的工作就是用来解决、缓解这些不足的。</li><li>常常需要指出自己的工作对谁有好处，有意义。</li></ol><h2 id="论文的研究目标、研究内容、研究方法"><a href="#论文的研究目标、研究内容、研究方法" class="headerlink" title="论文的研究目标、研究内容、研究方法"></a>论文的研究目标、研究内容、研究方法</h2><ol><li>研究目标要切题，由小而大，引出研究内容所期待达到的目标，取得的意义。</li><li>研究内容要具体而不琐碎，清晰的理出几条研究路线。</li><li>研究方法需要具体而详实。</li></ol><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><ol><li>引用量一般四十上下，根据具体情况具体分析。</li><li>用词书面化，不要口语化。</li></ol>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Composition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Composition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux下中断程序导致写文件失败</title>
      <link href="/2022/01/19/linux-xia-zhong-duan-cheng-xu-dao-zhi-xie-wen-jian-shi-bai/"/>
      <url>/2022/01/19/linux-xia-zhong-duan-cheng-xu-dao-zhi-xie-wen-jian-shi-bai/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux下中断程序导致写文件失败"><a href="#Linux下中断程序导致写文件失败" class="headerlink" title="Linux下中断程序导致写文件失败"></a>Linux下中断程序导致写文件失败</h1><h2 id="起因，需要向硬盘中写文件，写入文件的时候遇到ctrl-c或者kill指令杀死静默的进程时，导致文件flush到一半"><a href="#起因，需要向硬盘中写文件，写入文件的时候遇到ctrl-c或者kill指令杀死静默的进程时，导致文件flush到一半" class="headerlink" title="起因，需要向硬盘中写文件，写入文件的时候遇到ctrl-c或者kill指令杀死静默的进程时，导致文件flush到一半"></a>起因，需要向硬盘中写文件，写入文件的时候遇到ctrl-c或者kill指令杀死静默的进程时，导致文件flush到一半</h2><p>导致文件错误。</p><h2 id="ctrl-c和kill"><a href="#ctrl-c和kill" class="headerlink" title="ctrl-c和kill"></a><a href="https://www.cnblogs.com/ShaneZhang/p/4438066.html">ctrl-c和kill</a></h2><p>案例：</p><p>一个普通linux C程序，执行期间会进行多次printf操作，利用bash脚本重定向功能，将stdout重定向到一个另一个文件中去。在运行途中用ctrl+C终止程序，发现定向文件始终为空，即写失败。</p><p>分析：</p><p>原本以为是bash重定向机制导致的问题，于是将重定向取消，改为使用fprintf，而非printf。即在C程序内部进行写文件。发现问题依旧。（排除fopen打开失败的因素）</p><p>仔细观察，发现问题集中在两个层面，一个是ctrl+c到底做了什么，二是写文件操作为什么失败。</p><p>首先，ctrl+C不代表杀死进程，仅仅代表中断。中断和杀死并不是完全一样的概念。google了一下，有以下发现：</p><p>“Linux中的kill命令用来终止指定的进程（terminate a process）的运行，是Linux下进程管理的常用命令。通常，终止一个前台进程可以使用Ctrl+C键，但是，对于一个后台进程就须用kill命令来终止，我们就需要先使用ps/pidof/pstree/top等工具获取进程PID，然后使用kill命令来杀掉该进程。kill命令是通过向进程发送指定的信号来结束相应进程的。在默认情况下，采用编号为15的TERM信号。TERM信号将终止所有不能捕获该信号的进程。对于那些可以捕获该TERM信号的进程就要用编号为9的kill信号，强行“杀掉”该进程。” </p><p> “</p><p>1、kill命令可以带信号号码选项，也可以不带。如果没有信号号码，kill命令就会发出终止信号(15)，这个信号可以被进程捕获，使得进程在退出之前可以清理并释放资源。也可以用kill向进程发送特定的信号。例如：</p><p>kill -2 123</p><p>它的效果等同于在前台运行PID为123的进程时按下Ctrl+C键。但是，普通用户只能使用不带signal参数的kill命令或最多使用-9信号。</p><p>2、kill可以带有进程ID号作为参数。当用kill向这些进程发送信号时，必须是这些进程的主人。如果试图撤销一个没有权限撤销的进程或撤销一个不存在的进程，就会得到一个错误信息。</p><p>3、可以向多个进程发信号或终止它们。</p><p>4、当kill成功地发送了信号后，shell会在屏幕上显示出进程的终止信息。有时这个信息不会马上显示，只有当按下Enter键使shell的命令提示符再次出现时，才会显示出来。</p><p>5、应注意，信号使进程强行终止，这常会带来一些副作用，如数据丢失或者终端无法恢复到正常状态。发送信号时必须小心，只有在万不得已时，才用kill信号(9)，因为进程不能首先捕获它。要撤销所有的后台作业，可以输入kill 0。因为有些在后台运行的命令会启动多个进程，跟踪并找到所有要杀掉的进程的PID是件很麻烦的事。这时，使用kill 0来终止所有由当前shell启动的进程，是个有效的方法。</p><p>6、只有第9种信号(SIGKILL)才可以无条件终止进程，其他信号进程都有权利忽略。 下面是常用的信号：</p><p>HUP    1    终端断线</p><p>INT     2    中断（同 Ctrl + C）</p><p>QUIT    3    退出（同 Ctrl + \）</p><p>TERM   15    终止</p><p>KILL    9    强制终止</p><p>CONT   18    继续（与STOP相反， fg/bg命令）</p><p>STOP    19    暂停（同 Ctrl + Z）</p><p>”</p><p>至此，可以得出一个结论：我们平常所按ctrl+C不等价于“终止进程”。ctrl+C一般情况下等价于kill -s SIGINT。即进程接受的是SIGINT信号。而接受了SIGINT信号并不是简单的杀死进程。</p><p>在一般程序编写中，可以利用signal.h中所包含的相关函数（如sigaction）来对这些不同类别的信号进行响应，从而进行对应的后处理，如资源的释放等等。</p><p>如此，在bash脚本中如若出现：</p><p>$cmd &amp;</p><p>spid=$!</p><p>kill -s SIGINT $spid</p><p>是无法杀死进程的。进程仍旧在后台运行。若改成kill -9 $spid。便可杀死进程。</p><p>扯的有点远了。回到之前的问题，无论采取上述哪种kill方式，都会导致写文件失败。</p><p>那面对这些kill，平常的程序到底如何应对写文件之类的操作呢？再google一下，得到以下有用信息：</p><p>“SIGINT can be caught and handled. It’s like telling the process “Please stop what you’re doing.” The process is free to ignore the signal, or implement a handler that does anything it wants. The default behavior is to terminate, and this is what most processes will do. It’s typical, but not required, for a process to handle SIGINT by gracefully terminating – closing any open files, network connections, or database handles and stopping the current operation in such a way that prevents data loss or corruption.</p><p>SIGKILL can’t be handled by the receiving process. If a process gets sent SIGKILL, it’s toast, period. If a process receives SIGKILL in the middle of a database transaction or file write (for instance), it has no chance to exit gracefully and data loss or corruption may occur.</p><p>When you do Ctrl-C in the terminal, the terminal sends SIGINT to the running process. Like I said before, most processes will gracefully terminate on receiving SIGINT. Once it’s terminated, it’s just as surely ended as if it had been SIGKILL’ed or exited normally – you shouldn’t expect to see it in ps because it’s still gone.</p><p>But there are some programs that don’t respond to SIGINT in that way. bash is one example; if you hit Ctrl-C at a shell prompt, it’ll just cancel whatever you’ve typed on that line – not terminate the whole shell. Again, this is because the behavior when a process receives SIGINT is determined by the process itself. vi is another program that doesn’t handle SIGINT by terminating.”</p><p>和我们上面分析的一样，大部分的程序都需要一个handler来应对SIGINT信号。文中提到了exit normally。看来只有正常退出才能保证写文件成功？</p><p>stackoverflow上面有一个问答正好解决了我的疑问。</p><p>Q:</p><p>I’ve read in a man page that when exit() is called all streams are flushed and closed automatically. At first I was skeptical as to how this was done and whether it is truly reliable but seeing as I can’t find out any more I’m going to accept that it just works — we’ll see if anything blows up. Anyway, if this stream closing behavior is present in exit() is such behavior also present in the default handler for SIGINT (the interrupt signal usually triggered with Ctrl+C)? Or, would it be necessary to do something like this:</p><p>#include &lt;signal.h&gt;<br>#include &lt;stdlib.h&gt;</p><p>void onInterrupt(int dummy) { exit(0); }</p><p>int main() {<br>   signal(SIGINT, onInterrupt);<br>   FILE *file = fopen(“file”, “a”);<br>   for (;;) { fprintf(file, “bleh”); } }<br>to get file to be closed properly? Or can the signal(SIG… and void onInterrupt(… lines be safely omitted?</p><p>Please restrict any replies to C, C99, and POSIX as I’m not using GNU libc. Thanks.</p><p>A:</p><p>1.So in C99, if it’s closed then it’s flushed.</p><p>2.You’ll have to handle the signal if you want your buffers flushed. Otherwise the process will be terminated and the file descriptors closed without flushing the stdio buffers.</p><p>By default, a SIGINT will terminate the process abnornally. Processes so terminated do not call exit() and so do not have buffers flushed.</p><p>也就是说，只有正常退出，才能做到flush。否则将写失败。</p><p>之后有百度了下中文资料，发现同样的结论。</p><p>“fflush库函数的作用是把文件流里的所有未写出数据立刻写出。例如，你可以用这个函数来确保在试图读入一个用户响应之前，先向终端送出一个交互提示符。使用这个函数还可以确保在程序继续执行之前重要的数据都已经被写到磁盘上。有时在调试程序时，还可以用它来确定程序是正在写数据而不是被挂起了。注意，调用fclose函数隐含执行了一次flush操作，所以不必在fclose之前调用fflush。</p><p>fclose库函数关闭指定的文件流stream，使所有尚未写出的数据都写出。因为stdio库会对数据进行缓冲，所以使用fclose是很重要的。如果程序需要确保数据已经全部写出，就应该调用fclose函数。虽然当程序正常结束时，会自动对所有还打开的文件流调用fclose函数，但这样做就没有机会检查由fclose报告的错误了。与文件描述符一样，可用文件流的数目也是有限制的。这个限制由头文件stdio.h中的FOPEN_MAX常量定义，最小为8。”</p><p>“所谓flush一个缓冲，是指对写缓冲而言，将缓冲内的数据全部写入实际的文件，并将缓冲清空，这样可以保证文件处于最新的状态。之所以需要flush，是因为写缓冲使得文件处于一种不同步的状态，逻辑上一些数据已经写入了文件，但实际上这些数据仍然在缓冲中，如果此时程序意外地退出（发生异常或断电等），那么缓冲里的数据将没有机会写入文件。flush可以在一定程度上避免这样的情况发生。”</p><p>所以说，平时咱写程序，需要谨慎和按流程来，fclose做的事情有很多，不要全指望main函数return后自动帮你close文件。因为一旦出现上述中断的情形，就会生问题。</p><p>解决方案：</p><p>分析了这么多，解决方案相比也很明了了。</p><p>方案一：</p><p>在C程序中加入SIGINT响应函数，保证程序正常退出。</p><p>方案二：</p><p>在C程序中加入fflush函数，保证所有输出第一时间写入文件。</p><p>方案一才是最好的解决方案，而方案二有些hack了。</p><p>就这样。</p><h2 id="在python中处理异常-stackoverflow"><a href="#在python中处理异常-stackoverflow" class="headerlink" title="在python中处理异常, stackoverflow"></a>在python中处理异常, <a href="https://stackoverflow.com/questions/1112343/how-do-i-capture-sigint-in-python">stackoverflow</a></h2><h3 id="处理类"><a href="#处理类" class="headerlink" title="处理类"></a>处理类</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> signal<span class="token keyword">class</span> <span class="token class-name">GracefulInterruptHandler</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sig<span class="token operator">=</span>signal<span class="token punctuation">.</span>SIGINT<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>sig <span class="token operator">=</span> sig    <span class="token keyword">def</span> <span class="token function">__enter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>interrupted <span class="token operator">=</span> <span class="token boolean">False</span>        self<span class="token punctuation">.</span>released <span class="token operator">=</span> <span class="token boolean">False</span>        self<span class="token punctuation">.</span>original_handler <span class="token operator">=</span> signal<span class="token punctuation">.</span>getsignal<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sig<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">handler</span><span class="token punctuation">(</span>signum<span class="token punctuation">,</span> frame<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>release<span class="token punctuation">(</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>interrupted <span class="token operator">=</span> <span class="token boolean">True</span>        signal<span class="token punctuation">.</span>signal<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sig<span class="token punctuation">,</span> handler<span class="token punctuation">)</span>        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">__exit__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token punctuation">,</span> value<span class="token punctuation">,</span> tb<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>release<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">release</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>released<span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">False</span>        signal<span class="token punctuation">.</span>signal<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sig<span class="token punctuation">,</span> self<span class="token punctuation">.</span>original_handler<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>released <span class="token operator">=</span> <span class="token boolean">True</span>        <span class="token keyword">return</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 一般处理</span><span class="token keyword">with</span> GracefulInterruptHandler<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> h<span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">xrange</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span> <span class="token string">"..."</span>        time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> h<span class="token punctuation">.</span>interrupted<span class="token punctuation">:</span>            <span class="token keyword">print</span> <span class="token string">"interrupted!"</span>            time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>            <span class="token keyword">break</span><span class="token comment"># 循环中的处理</span><span class="token keyword">with</span> GracefulInterruptHandler<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> h1<span class="token punctuation">:</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        <span class="token keyword">print</span> <span class="token string">"(1)..."</span>        time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> GracefulInterruptHandler<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> h2<span class="token punctuation">:</span>            <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>                <span class="token keyword">print</span> <span class="token string">"\t(2)..."</span>                time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                <span class="token keyword">if</span> h2<span class="token punctuation">.</span>interrupted<span class="token punctuation">:</span>                    <span class="token keyword">print</span> <span class="token string">"\t(2) interrupted!"</span>                    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>                    <span class="token keyword">break</span>        <span class="token keyword">if</span> h1<span class="token punctuation">.</span>interrupted<span class="token punctuation">:</span>            <span class="token keyword">print</span> <span class="token string">"(1) interrupted!"</span>            time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>            <span class="token keyword">break</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="也可以采用catch-keyboard-interrupt的方法去处理"><a href="#也可以采用catch-keyboard-interrupt的方法去处理" class="headerlink" title="也可以采用catch keyboard interrupt的方法去处理"></a>也可以采用catch keyboard interrupt的方法去处理</h3><ol><li>局限性：可能不适用于kill的方法。</li></ol>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
          <category> About Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python request+selenium爬虫经验</title>
      <link href="/2022/01/17/python-request-selenium-pa-chong-jing-yan/"/>
      <url>/2022/01/17/python-request-selenium-pa-chong-jing-yan/</url>
      
        <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><ol><li>从10.15-1.17，一直在使用python+selenium+request爬虫，中间重新获取了三次数据，浪费了很多时间，尤其是在python处理爬虫数据遇到的各种异常方面浪费了大量的时间，实在值得注意一下。</li></ol><h1 id="Experience"><a href="#Experience" class="headerlink" title="Experience"></a>Experience</h1><ol><li>selenium需要耗费大量的内存，8G的内存大概可以并行起8个selenium（chromedriver+chrome），因此，应该尽量使用request库. </li><li>如果实在嫌request麻烦，一定要注意内存，不要爆内存，我很久才意识到内存爆掉的问题，之前当一个selenium进程异常终止的时候，采取的方法是写一个自动检测python脚本是否正在运行，如果没在运行重启脚本的方法，后来发现当内存爆炸的时候，linux后台并不会杀死运行的python脚本，导致重启脚本失败，在获取数据的时候，浪费了大量的时间。</li><li>可以设置交换区防止意外的内存爆炸。</li><li>将cpu的利用率不要压到太满，70%足矣。</li><li>并行进程尽量使用pool.apply_async()，手动撰写bash一个一个启动python脚本，会多耗费内存。</li><li>处理大数据，尽量使用已有的云资源，云服务器在处理大数据上有独有的优势，一个是可以自定义配置设备属性（内存多大，硬盘多大，核多少等等），本地的PC/笔记本处理器数百G的大数据的能力实在捉襟见肘。（机械硬盘读取真的很慢，很慢，很慢）</li><li>国内机器访问国外网站应该使用加速器或者vpn，aws不需要使用，request需要配置走代理，否则访问不了，如果使用vpn, selenium可以不用配置（一半vpn软件都默认配置浏览器流量走代理）</li></ol><h1 id="一些shell脚本"><a href="#一些shell脚本" class="headerlink" title="一些shell脚本"></a>一些shell脚本</h1><h2 id="aws-ec2设置8G交换区"><a href="#aws-ec2设置8G交换区" class="headerlink" title="aws ec2设置8G交换区"></a>aws ec2设置8G交换区</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sudo dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;swapfile bs&#x3D;128M count&#x3D;64 # 8Gsudo chmod 600 &#x2F;swapfilesudo mkswap &#x2F;swapfilesudo swapon &#x2F;swapfilesudo swapon -s# echo -e &quot;...\n...\n&quot; | sudo passwd# 644sudo chown ubuntu:ubuntu &#x2F;etc&#x2F;fstab# echo &quot;...&quot; | su # 没用echo &quot;&#x2F;swapfile swap swap defaults 0 0&quot; &gt;&gt; &#x2F;etc&#x2F;fstabsudo chown root:root &#x2F;etc&#x2F;fstabcat &#x2F;etc&#x2F;fstab# aws设置8G交换区sudo dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;swapfile bs&#x3D;128M count&#x3D;64 # 8Gsudo chmod 600 &#x2F;swapfilesudo mkswap &#x2F;swapfilesudo swapon &#x2F;swapfilesudo swapon -ssudo vi &#x2F;etc&#x2F;fstab&#x2F;swapfile swap swap defaults 0 0# 详细解释1. 使用 dd 命令在根文件系统上创建交换文件。在该命令中，bs 是数据块大小，count 是数据块的数量。交换文件的大小是 dd 命令中的数据块大小选项乘以计数选项。调整这些值以确定所需的交换文件大小。您指定的数据块大小应小于实例上的可用内存，否则会收到“内存耗尽”错误。在此示例 dd 命令中，交换文件为 4GB (128MB x 32)：sudo dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;swapfile bs&#x3D;128M count&#x3D;322. 更新交换文件的读取权限和写入权限：sudo chmod 600 &#x2F;swapfile3. 设置 Linux 交换区域：sudo mkswap &#x2F;swapfile4. 通过将交换文件添加到交换空间，使交换文件立即可供使用：sudo swapon &#x2F;swapfile5. 确认过程已成功完成：sudo swapon -s6. 通过编辑 &#x2F;etc&#x2F;fstab 文件，在启动时启用交换文件。在编辑器中打开文件：sudo vi &#x2F;etc&#x2F;fstab在文件末尾添加以下新行，保存文件，然后退出：&#x2F;swapfile swap swap defaults 0 0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="kill掉部分进程"><a href="#kill掉部分进程" class="headerlink" title="kill掉部分进程"></a>kill掉部分进程</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># kill掉启动的所有的进程ps -ef|grep &#39;python3 -u get_jsons.py\|chromium\|\.bash&#39; | grep -v grep | awk &#39;&#123;print $2&#125;&#39; | xargs kill -9# kill含有&lt;defunct&gt;的进程，父进程，祖父进程ps -ef &gt; test.txt &amp;&amp; ps -ef|grep &#39;\&lt;defunct\&gt;&#39; | grep -v grep | awk &#39;&#123;print $3&#125;&#39; | xargs -I &#123;&#125; fgrep &#123;&#125; test.txt | awk &#39;&#123;print $3&#125;&#39; | xargs -I &#123;&#125; fgrep &#123;&#125; test.txt | sort | uniq | awk &#39;&#123;print $2&#125;&#39; | xargs kill -9 &amp;&amp; rm test.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="chromedriver-cannot-open-path-of-the-current-working-directory-Permission-denied"><a href="#chromedriver-cannot-open-path-of-the-current-working-directory-Permission-denied" class="headerlink" title="chromedriver: cannot open path of the current working directory: Permission denied"></a>chromedriver: cannot open path of the current working directory: Permission denied</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># chromedriver: cannot open path of the current working directory: Permission deniedunzip cp chromedriver &#x2F;usr&#x2F;bin&#x2F;[why &quot;cannot open path of the current working directory: Permission denied&quot;](https:&#x2F;&#x2F;utcc.utoronto.ca&#x2F;~cks&#x2F;space&#x2F;blog&#x2F;linux&#x2F;Ubuntu2004SnapsHomeIssue?showcomments)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="CPU-内存监控"><a href="#CPU-内存监控" class="headerlink" title="CPU, 内存监控"></a>CPU, 内存监控</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sudo apt-get install htophtop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="tail和multitail"><a href="#tail和multitail" class="headerlink" title="tail和multitail"></a>tail和multitail</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">tail -f loginfo.txt  # 动态更新查看信息tail loginfo.txt  # 查看当前信息sudo apt install multitailsudo multitail log&#x2F;loginfo15.txt log&#x2F;loginfo16.txt log&#x2F;loginfo17.txt log&#x2F;loginfo18.txt log&#x2F;loginfo19.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="headless-chrome-amp-chrome-driver"><a href="#headless-chrome-amp-chrome-driver" class="headerlink" title="headless chrome &amp; chrome driver"></a>headless chrome &amp; chrome driver</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sudo apt-get install libxss1 libappindicator1 libindicator7 # 依赖wget https:&#x2F;&#x2F;dl.google.com&#x2F;linux&#x2F;direct&#x2F;google-chrome-stable_current_amd64.debsudo dpkg -i google-chrome*.deb  # Might show &quot;errors&quot;sudo apt-get install -fgoogle-chrome --version # 查看版本Google Chrome 96.0.4664.45sudo apt install chromium-chromedriverchromedriver --version # 查看版本# 离线安装wget https:&#x2F;&#x2F;chromedriver.storage.googleapis.com&#x2F;96.0.4664.45&#x2F;chromedriver_linux64.zipsudo vim ~&#x2F;.profile # 配置环境变量export PATH&#x3D;&quot;$PATH:~&#x2F;application&#x2F;chromedriver&quot;source ~&#x2F;.profile# 测试google-chrome --headless --remote-debugging-port&#x3D;9222 https:&#x2F;&#x2F;chromium.org --disable-gpu # 测试安装curl http:&#x2F;&#x2F;localhost:9222 # 另一个终端测试<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="查看分区"><a href="#查看分区" class="headerlink" title="查看分区"></a>查看分区</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">lsblk # 查看磁盘信息命令sudo fdisk -l # 查看磁盘信息命令sudo blkid # 查看硬盘的UUID命令<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="格式化，挂载分区"><a href="#格式化，挂载分区" class="headerlink" title="格式化，挂载分区"></a>格式化，挂载分区</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sudo vim &#x2F;etc&#x2F;fstab # 永久性挂载分区，修改分区文件mkfs -t ext4 &#x2F;dev&#x2F;nvme1n1 # 创建文件系统mkdir ~&#x2F;mntsudo mount &#x2F;dev&#x2F;nvme0n1 ~&#x2F;mnt # 挂载分区<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="连接aws"><a href="#连接aws" class="headerlink" title="连接aws"></a>连接aws</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">ssh -i ~&#x2F;.ssh&#x2F;SHY.pem ubuntu@这里是公有ipv4地址<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="scp-aws下载-上传文件"><a href="#scp-aws下载-上传文件" class="headerlink" title="scp,aws下载,上传文件"></a>scp,aws下载,上传文件</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp -i ~&#x2F;.ssh&#x2F;SHY.pem ubuntu@这里是ipv4地址:~&#x2F;shy&#x2F;different-sort.rar &#x2F;mnt&#x2F;d&#x2F;Source&#x2F;data&#x2F; # scp下载aws上的文件sudo scp -o &quot;ProxyCommand ssh -W 127.0.0.1:7890&quot; -i ~&#x2F;.ssh&#x2F;SHY.pem ubuntu@3.92.184.246:~&#x2F;RobloxDiscover&#x2F;data&#x2F;creators-group.zip &#x2F;mnt&#x2F;d&#x2F;Source&#x2F;aws&#x2F;45-55-group.zip # scp下载aws上的文件, 无用scp -i ~&#x2F;.ssh&#x2F;SHY.pem &#x2F;mnt&#x2F;d&#x2F;Source&#x2F;data&#x2F;different-sort.rar  ubuntu@ec2-3-87-217-66.compute-1.amazonaws.com:~&#x2F;shy&#x2F; # 往aws传文件aws s3 cp creators-group.zip s3:&#x2F;&#x2F;roblox-data-bucket # aws复制的的用法aws s3 sync creators-group.zip s3:&#x2F;&#x2F;roblox-data-bucket # aws复制的的用法setx HTTP_PROXY http:&#x2F;&#x2F;10.15.20.25:1234setx HTTPS_PROXY http:&#x2F;&#x2F;10.15.20.25:5678setx HTTP_PROXY http:&#x2F;&#x2F;username:password@proxy.example.com:1234setx HTTPS_PROXY http:&#x2F;&#x2F;username:password@proxy.example.com:5678&#x3D;aws s3 cp --recursive s3:&#x2F;&#x2F;roblox-data-bucket .&#x2F; --no-verify-ssl<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> About Experience </category>
          
          <category> About Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Experience </tag>
            
            <tag> About Web </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mobile App Squatting</title>
      <link href="/2021/12/17/mobile-app-squatting/"/>
      <url>/2021/12/17/mobile-app-squatting/</url>
      
        <content type="html"><![CDATA[<h1 id="Mobile-App-Squatting"><a href="#Mobile-App-Squatting" class="headerlink" title="Mobile App Squatting"></a>Mobile App Squatting</h1><h2 id="What-is-Mobile-App-Squatting"><a href="#What-is-Mobile-App-Squatting" class="headerlink" title="What is Mobile App Squatting?"></a>What is Mobile App Squatting?</h2><p>In “App Squatting”, attackers release apps with identifiers (e.g., app name or package name) that are confusingly similar to those of popular apps or well-known Internet brands.</p><h2 id="Where-this-idea-from"><a href="#Where-this-idea-from" class="headerlink" title="Where this idea from?"></a>Where this idea from?</h2><ul><li>mainly from domain squatting<br>We start by performing a preliminary study of app squatting on 10 popular apps</li></ul><h2 id="Main-study-steps"><a href="#Main-study-steps" class="headerlink" title="Main study steps"></a>Main study steps</h2><ul><li><p>performing <strong>a preliminary study</strong> of app squatting on 10 popular apps (Section 3). We generate 3,283 potential “squatting names” for these 10 apps, employing the rules of existing domain squatting tools. </p></li><li><p>We then <strong>verify whether the potential squatting apps exist in the wild</strong>(using Koodous [20], a mobile app corpus containing more than 50 million Android apps). </p></li><li><p>We find that app squatting abuse is, indeed, highly prevalent for Android apps. Based on the verified squatting apps, we then <strong>identify the common patterns that attackers leverage in app squatting</strong> (Section 4). We then <strong>use these rules to design and implement AppCrazy</strong>, a tool to systematically generate squatting identifier names for mobile apps. Given an Android app as the input, AppCrazy automatically generates confusingly similar app names and package names that could be leveraged by attackers. </p></li><li><p>Exploiting the capabilities of AppCrazy, we then conduct <strong>a large scale empirical analysis</strong>: we apply it to <em>426 additional apps crawled from Google Play and generate more than 200K potential squatting names</em> (Section 5). From these, *we discover 10,553 squatting apps, confirming that this threat far exceeds the top 10 apps alone. <strong>Finally, we characterize the impact introduced by app squatting</strong> (Section 6), including <em>the prevalence of squatting apps in major app markets</em>, and <em>the number of app installs of the squatting apps.</em></p></li></ul><h2 id="What-i-get"><a href="#What-i-get" class="headerlink" title="What i get?"></a>What i get?</h2><ul><li><p>discover something that people don’t touched.<br>  去找人们没有做过的东西。</p></li><li><p>去已经做过的东西中发现新的点子。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Software Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Software Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>区块链技术与应用</title>
      <link href="/2021/12/06/qu-kuai-lian-ji-zhu-yu-ying-yong/"/>
      <url>/2021/12/06/qu-kuai-lian-ji-zhu-yu-ying-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="课时3，Data-structure"><a href="#课时3，Data-structure" class="headerlink" title="课时3，Data structure"></a>课时3，Data structure</h2><ol><li><p>hash pointer<br>存储hash值 &amp; 指针</p><ul><li>概念<ul><li>hash值代表结构体的hash值</li><li>指针指示这个区块在内存中的位置</li><li>H()表示一个hash指针。</li></ul></li><li>作用：<ul><li>不仅可以得到位置</li><li>也可以判断这个区块是否被改掉</li></ul></li></ul></li><li><p>Block Chain is a linked list using hash pointer<br><img src="/.io//pic1.png" class="lazyload placeholder" data-srcset="/.io//pic1.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><ul><li>genesis block: 创世区块</li><li>改动某一个区块的值，其后区块的hash值都对不上，因此，改一个区块就要改掉后面的所有的区块。</li><li><strong>系统中的某一个节点，不再需要存储整个区块链的所有的区块，只需要存储部分，就可以验证之前的区块是否被修改过</strong>。<ul><li>存储部分，要用到前面额区块的话，就去问别的节点要，只需要验证别的节点给你的区块的hash值，就可以验证别的节点给你的区块是不是真实的区块。</li></ul></li></ul></li><li><p>Merkle Tree</p><ul><li>Merkle Tree用hash pointer代替了普通指针<br><img src="/.io//pic2.png" class="lazyload placeholder" data-srcset="/.io//pic2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>最下层的是数据块，每一个块是一个transaction。</li><li>对每一个数据块的篡改都会影响到root hash，因此保护了整棵树。</li><li>Merkle Proof. <ul><li><img src="/.io//pic3.png" class="lazyload placeholder" data-srcset="/.io//pic3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>全节点保存整个区块的内容。</li><li>轻节点只保存block header。<ul><li>proof of membership/inclusion(证明某个交易存在过)<ul><li>轻节点想知道一个transaction是否发生过？首先我们知道这个transaction发生的位置（如何知道的？），也就是知道这个transactions发生在哪一个区块。比如上图黄色transaction.</li><li>现在轻节点向全节点请求，全节点把三个红色的hash发送给轻节点，那么那三个绿色的hash值轻节点就可以自己算出来了，于是可以得到这一颗Merkel Tree的根节点的hash值。根hash值和轻节点保存的hash值进行比较，就可以判断这个交易是否真实的发生过。</li><li>O(log(n))</li></ul></li><li>proof of non-membership<ul><li>直接证明的复杂度是O(n)</li><li>我们的交易可能出现在这个叶节点的各个位置，因此没有更好的方法。</li><li>但是如果Merkel Tree中的hash值按照hash值的大小从大到小排序，那么我们在验证这个节点不存在的时候，我们只需要先算出这个要验证的交易的hash值，然后就可以二分的找出这个hash值可能存在的位置，最后根据这个位置再去验证根的hash值是否是有效的。</li><li><img src="/.io//pic4.png" class="lazyload placeholder" data-srcset="/.io//pic4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>代价是需要去排序。</li><li><strong>排序的Merkle Tree</strong>叫做<strong>sorted Merkle Tree</strong></li><li>bitcoin中不需要做这种不存在性证明。</li></ul></li></ul></li></ul></li><li><strong>Block Chain和Merkle Tree的关系</strong>：<ul><li>Block Chain<ul><li>Block Header(存储Merkel Tree的root hash值)</li><li>Block Body(包含有所有的一个区块内的transactions)</li></ul></li></ul></li></ul></li><li><p>hash pointer不好应用于环</p></li></ol><ul><li><img src="/.io//pic5.png" class="lazyload placeholder" data-srcset="/.io//pic5.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>不好确定哪一个作为开始。</li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Block Chain </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Block Chain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding the Evolution of Mobile App Ecosystems: A Longitudinal Measurement Study of Google Play</title>
      <link href="/2021/11/04/understanding-the-evolution-of-mobile-app-ecosystems/"/>
      <url>/2021/11/04/understanding-the-evolution-of-mobile-app-ecosystems/</url>
      
        <content type="html"><![CDATA[<h1 id="Understanding-the-Evolution-of-Mobile-App-Ecosystems-A-Longitudinal-Measurement-Study-of-Google-Play"><a href="#Understanding-the-Evolution-of-Mobile-App-Ecosystems-A-Longitudinal-Measurement-Study-of-Google-Play" class="headerlink" title="Understanding the Evolution of Mobile App Ecosystems: A Longitudinal Measurement Study of Google Play"></a>Understanding the Evolution of Mobile App Ecosystems: A Longitudinal Measurement Study of Google Play</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li><strong>Mobile App Ecosystem Analysis, App Evolution Analysis, Mobile App Analysis</strong>, app分析的相关工作见<strong>2 RELATED WORK</strong></li></ul><h2 id="Motivation-amp-Significance"><a href="#Motivation-amp-Significance" class="headerlink" title="Motivation &amp; Significance"></a>Motivation &amp; Significance</h2><ul><li>little is known at a comprehensive level on the evolution of mobile app ecosystems.</li></ul><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><ul><li>5.3 million app records, three snapshots.</li><li>the first snapshot: started from top 500 Google Play apps in each category, (considered as seeds), and 12,500 apps that belong to 25 general categories in total, <strong>a breadth-first-search approach</strong>, (1) “Similar Apps” shown on the app web pages recommended by Google Play and (2) other apps released by the same developer by retrieving the developers’ Google Play web pages.</li><li>the second and the third snapshot, take the previous 1.5 million crawled apps as our searching seeds</li></ul><h2 id="Direction-amp-Details"><a href="#Direction-amp-Details" class="headerlink" title="Direction &amp; Details"></a>Direction &amp; Details</h2><ul><li><strong>collected</strong> three snapshots of Google Play, 2014, March 2015 and September 2017, 5.3 million Android app entities, all the metadata and apks. </li><li>characterization<ul><li><strong>characterization</strong> of all the apps and analyze the evolving trends, including the <strong>distribution of free and paid apps</strong>(It is a trend that the number of paid apps has decreased significantly in Google Play.), </li><li>the evolution of <strong>app downloads</strong> and <strong>app rating</strong>(the popularity and quality of apps published in Google Play is getting better), </li><li><strong>the ranking and competitiveness of app categories</strong>,</li><li>the evolution of <strong>permission usage and privacy policy declaration</strong><ul><li> how sensitive permission usage by Google Play apps has evolved over time, For each app, we extract its requested permissions, only focus on the so-called dangerous permissions as listed by Android [8], i.e., the 26 system permissions used to guard accesses to sensitive data.</li><li>compare the developers’ practices on privacy policy declaration along with the timeline. (1) searching keywords “Privacy Policy” or (2) inspecting the privacy policy field shown on app pages), </li></ul></li><li>and the presence of <strong>third-party tracking and advertising libraries</strong> (Section 4). <ul><li> third-party tracking services, especially the mobile advertisement services, may bring security and privacy issues to mobile users.</li><li> whether more Google Play apps have embedded advertisement services during our investigation period</li></ul></li></ul></li><li><img src="/.io//Figure1.png" class="lazyload placeholder" data-srcset="/.io//Figure1.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>Category Ranking by Total Installs.</li><li>Category Ranking by Average Downloads.</li><li>Ranking by Competitiveness</li><li><img src="/.io//Figure2.png" class="lazyload placeholder" data-srcset="/.io//Figure2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li><img src="/.io//Figure3.png" class="lazyload placeholder" data-srcset="/.io//Figure3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li><img src="/.io//Figure4.png" class="lazyload placeholder" data-srcset="/.io//Figure4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li><img src="/.io//Figure5.png" class="lazyload placeholder" data-srcset="/.io//Figure5.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>743,530 <strong>long-lasting apps</strong> (appeared in all three snapshots) in our dataset, we further investigate their <strong>growth rate, app updates, and permission changes</strong> (Section 5). <ul><li>5.1 Growth Rate<ul><li>we first classify the number of app installs into 10 ranges, e.g., [1, 10), [10, 100), [100, 1K), etc.</li><li>how many of them keep staying in the original ranges</li><li>how many of them have evolved to higher install ranges.</li><li><img src="/.io//Figure6.png" class="lazyload placeholder" data-srcset="/.io//Figure6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>5.2 App Updates<ul><li>Percentage of Updated Apps.</li><li>App Updates across Categories.</li><li>App Updates vs. App Popularity</li><li><img src="/.io//Figure7.png" class="lazyload placeholder" data-srcset="/.io//Figure7.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>5.3 Permission Evolution<ul><li>how many permissions each individual app had added or removed during the evolution</li><li>(1) new permissions, (2) removed permissions, (3) both new and removed permissions, (4) permissions unchanged.</li><li>Changes in Permission Usage.</li><li>Changed Permission</li><li><img src="/.io//Figure8,9.png" class="lazyload placeholder" data-srcset="/.io//Figure8,9.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li></ul></li><li>Third, we provide an in-depth analysis of <strong>malicious and deceptive behaviors</strong> across the three snapshots, discussing <strong>the presence of malicious apps and aggressive app promotion behaviors</strong> (Section 6). <ul><li>security risks and developing misbehaviors: the presence of malicious apps, the spamming apps using ranking fraud techniques.</li><li>6.1 Malicious Apps<ul><li>Bouncer service</li><li>machine-learning based approaches</li><li>whether Google Play is moving towards higher security levels during its evolution</li><li>VirusTotal, a popular online malware analysis service that aggregates more than 60 anti-virus engines.</li><li>Distribution of Malicious Apps</li><li>Evolution of Malware Families</li></ul></li><li>6.2 Spamming Apps<ul><li>resort to the most straightforward strategy to identify spamming apps with irrelevant descriptions</li><li>we first collect the top 100 popular app names, then we count the number of popular app names mentioned in the descriptions of all the apps we harvested</li></ul></li></ul></li><li>At last, we analyze <strong>the evolution from the developers’ perspective.</strong> We have classified the developers into different groups based on the number of apps they released and analyzed their evolution, and further <strong>pinpoint the spamming and malicious developers</strong> (Section 7).<ul><li>study the developer popularity.</li><li>7.1 Top Developers, the number of accumulated installs</li><li>7.2 Developer Categorization<ul><li>Conservative (1 app), Moderate (2 to 9 apps), Active (10 to 49 apps) and Aggressive</li><li>follow this classification to study the evolution of app developers</li><li>Spamming Developers.</li><li>Removal of Spamming Developers.<br>(more than 50 apps).</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Software Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Software Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Explorative Study of the Mobile App Ecosystem from App Developers’ Perspective</title>
      <link href="/2021/10/30/an-explorative-study-of-the-mobile-app-ecosystem-from/"/>
      <url>/2021/10/30/an-explorative-study-of-the-mobile-app-ecosystem-from/</url>
      
        <content type="html"><![CDATA[<h1 id="An-Explorative-Study-of-the-Mobile-App-Ecosystem-from-App-Developers’-Perspective"><a href="#An-Explorative-Study-of-the-Mobile-App-Ecosystem-from-App-Developers’-Perspective" class="headerlink" title="An Explorative Study of the Mobile App Ecosystem from App Developers’ Perspective"></a>An Explorative Study of the Mobile App Ecosystem from App Developers’ Perspective</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>App分析的两条线路：</p><ul><li>分析移动app和移动app市场。特征和发展：<strong>app markets, security and privacy analysis</strong> using either <strong>static analysis or dynamic analysis, app repackaging detection</strong>, and mining useful information from <strong>app reviews, etc.</strong></li><li>分析用户：mobile users, including <strong>mining and prediction of user behavior and demographics</strong>, investigating <strong>users’ mobile privacy concerns and preferences</strong>, and <strong>mobile app recommendation</strong>, etc.</li><li>可以根据这篇论文的综述去找相关的论文。</li><li><font color="#FF0000">本篇文章有一个地方值得学习，就是对特例的分析，比如对the most aggressive developer的分析，对most不注意隐私的开发者的分析，都拿出来说了一遍</font><h2 id="Motivation-amp-Significance"><a href="#Motivation-amp-Significance" class="headerlink" title="Motivation &amp; Significance"></a>Motivation &amp; Significance</h2></li><li>a study of the mobile app ecosystem <strong>from the perspective of app developers.</strong></li></ul><h2 id="Direction"><a href="#Direction" class="headerlink" title="Direction"></a>Direction</h2><p><font color="#FF0000">收集数据，分析数据（分析发布app多的开发者），分析开发者的隐私行为</font></p><ul><li>classifified developers into difffferent groups based on the number of apps they have released</li><li>compared their characteristics.</li><li>the group of aggressive developers who have released more than 50 apps, trying to understand how and why they create so many apps.</li><li>investigated the privacy behaviors of app developers， some developers have a habit of producing apps with low privacy ratings.</li></ul><h2 id="Datasets-and-Analysis"><a href="#Datasets-and-Analysis" class="headerlink" title="Datasets and Analysis"></a>Datasets and Analysis</h2><ul><li>How to collect?<ul><li>crawled the Google Play webpages in March 2015, and created an index of more than 1.5 million Android apps</li><li>We crawled the metadata of these apps, including the app names, developer names, app ratings, the number of installs, etc.</li><li>downloaded all the apk fifiles of free apps through the Google Play API</li></ul></li><li>Overview of App<ul><li>one million Android apps and 320,000 developers from Google Play</li><li>App Distribution</li><li>App Installs and App Ratings</li><li>Popular Apps</li><li><img src="/.io//Figure1.png" class="lazyload placeholder" data-srcset="/.io//Figure1.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li><img src="/.io//Table2.png" class="lazyload placeholder" data-srcset="/.io//Table2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>APP DEVELOPER STATISTICS<ul><li> Developer Distribution</li><li> App Installs Distribution for Developers</li><li> <img src="/.io//Table3.png" class="lazyload placeholder" data-srcset="/.io//Table3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li> <img src="/.io//Table4.png" class="lazyload placeholder" data-srcset="/.io//Table4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li> Popular Developers</li><li> <img src="/.io//Figure3.png" class="lazyload placeholder" data-srcset="/.io//Figure3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>DEVELOPER CHARACTERISTICS<ul><li> <img src="/.io//Table7.png" class="lazyload placeholder" data-srcset="/.io//Table7.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li> categorized the developers based on the number of apps they have created</li><li> <img src="/.io//Table5.png" class="lazyload placeholder" data-srcset="/.io//Table5.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li> App Popularity vs. Developer Groups</li><li> <img src="/.io//Figure5,6.png" class="lazyload placeholder" data-srcset="/.io//Figure5,6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li> App Ratings vs. Developer Groups</li><li> <img src="/.io//Figure7.png" class="lazyload placeholder" data-srcset="/.io//Figure7.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li> Who are These Developers?</li><li> <img src="/.io//Table6.png" class="lazyload placeholder" data-srcset="/.io//Table6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>How do They Create Their Apps?<ul><li> <strong>using app clone detection tool[40],third-party library detection tool[31],identify shared code, comparing the layout and UI by installing and testing sample apps on smartphones.</strong></li><li> four types</li></ul></li><li> Do They Create Popular Apps?</li><li>Identifying Spamming Developers<ul><li> <img src="/.io//Figure8.png" class="lazyload placeholder" data-srcset="/.io//Figure8.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li> <img src="/.io//Figure9.png" class="lazyload placeholder" data-srcset="/.io//Figure9.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li></ul></li><li>PRIVACY RATINGS OF DEVELOPERS<ul><li> <strong>PrivacyGrade</strong> to analyze the sensitive behaviors of Android apps and assign privacy grades to each app, crowdsourcing and machine-learning techniques to analyze the privacy-related behaviors of mobile apps.</li><li> investigated the developers with lowest privacy scores。</li><li> <img src="/.io//Figure10.png" class="lazyload placeholder" data-srcset="/.io//Figure10.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li> <img src="/.io//Figure11.png" class="lazyload placeholder" data-srcset="/.io//Figure11.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Software Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Software Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AppNet: Understanding App Recommendation in Google Play</title>
      <link href="/2021/10/30/appnet-understanding-app-recommendation-in-google-play/"/>
      <url>/2021/10/30/appnet-understanding-app-recommendation-in-google-play/</url>
      
        <content type="html"><![CDATA[<h1 id="AppNet-Understanding-App-Recommendation-in-Google-Play"><a href="#AppNet-Understanding-App-Recommendation-in-Google-Play" class="headerlink" title="AppNet: Understanding App Recommendation in Google Play"></a>AppNet: Understanding App Recommendation in Google Play</h1><h2 id="Motivation-amp-Significance"><a href="#Motivation-amp-Significance" class="headerlink" title="Motivation &amp; Significance"></a>Motivation &amp; Significance</h2><font color="#FF0000">总结一下看的几篇论文，其实做的主要的工作内容是：<ul><li>收集一些有意义的数据集</li><li>分析数据集特征，从中得出有意义的结论<strong>数据挖掘的工作</strong>，发现这对推荐，提升系统性能，安全与隐私，监管等方面的各种作用</li><li>本篇文章可能由于数据集等原因，缺少一些其它工作的分析预测，推荐建模的工作。</li></ul></font><font color="#FF0000"><p>动机和意义：</p><ol><li>探索推荐系统的”app net”的连接特点</li><li>发现这种特点对于：识别app欺诈行为，提升推荐系统，提升app曝光度方面的作用。</li><li>第一次做这个工作，具有引导意义。</li></ol></font><ul><li><p>some characteristics of human networks, i.e., a large portion of the apps (more than 69%) have no incoming edges (no apps link to them), while a small group of apps dominate the network with each having thousands of incoming edges.<br>147K (7%) apps form a fully connected cluster, while covering 97% of all the edges</p></li><li><p><strong>identifying fraudulent app promotion behaviors, improving the recommendation system, and enhancing the exposure of apps.</strong></p></li><li><p><strong>Unknown:</strong> how efficient is the recommendation mechanism and whether it could be exploited by spamming or malicious developers.</p></li><li><p><strong>first</strong> large-scale explorative study to characterize and understand the app recommendation network formed by app recommending relationships at Google Play.</p><h2 id="Research-Direction"><a href="#Research-Direction" class="headerlink" title="Research Direction"></a>Research Direction</h2><font color="#FF0000">主要的研究方向是：</font></li><li><p>收集一些有意义的数据集</p></li><li><p>进行数据集分析</p></li></ul><ul><li>build “AppNet”,over 2 million nodes, 100 million “similar app” more than 100 million edges</li><li>characterize the AppNet, explore its network structures (cf. Section 5).</li><li>detailed properties AppNet on the distribution of app recommendations, how the recommendation relationship is affected by app properties such as category and the number of downloads.</li></ul><h2 id="Discoveries"><a href="#Discoveries" class="headerlink" title="Discoveries"></a>Discoveries</h2><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><font color="#FF0000"><ul><li>数据集的收集方法要重点介绍</li><li>这个可以参考。</li></ul></font><ul><li>first used a list of <strong>1.5 million package names [12] as the searching seeds, then use a breadth-first-search (BFS) approach to crawl (1) a list of “similar apps” recommended for each app of our seeds by Google Play</strong> and (2) other apps released by the same developer.</li><li>keywords-based searching approach to crawl apps by summarizing a list of keywords from app descriptions. <strong>identify apps or small groups that have poor connectivity with other apps</strong></li><li>2.08 million of apps, 131 million app relations in Google Play.</li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Software Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Software Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bitcoin: A Peer-to-Peer Electronic Cash System</title>
      <link href="/2021/10/30/bitcoin-a-peer-to-peer-electronic-cash-system/"/>
      <url>/2021/10/30/bitcoin-a-peer-to-peer-electronic-cash-system/</url>
      
        <content type="html"><![CDATA[<h1 id="Bitcoin-A-Peer-to-Peer-Electronic-Cash-System"><a href="#Bitcoin-A-Peer-to-Peer-Electronic-Cash-System" class="headerlink" title="Bitcoin: A Peer-to-Peer Electronic Cash System"></a>Bitcoin: A Peer-to-Peer Electronic Cash System</h1><!-- vscode-markdown-toc --><ul><li><ol><li><a href="#">作者信息</a></li></ol></li><li><ol start="2"><li><a href="#-1">行文思路</a></li></ol></li><li><ol start="3"><li><a href="#-1">参考</a></li></ol></li><li><ol start="4"><li><a href="#Abstract">Abstract</a></li></ol></li><li><ol start="5"><li><a href="#-1">去中心化的系统</a></li></ol></li><li><ol start="6"><li><a href="#Proof-of-work">Proof-of-work（工作量证明）</a></li></ol></li><li><ol start="7"><li><a href="#Network">Network</a></li></ol></li><li><ol start="8"><li><a href="#Incentive">Incentive(激励措施)</a></li></ol></li><li><ol start="9"><li><a href="#ReclaimingDiskSpace">Reclaiming Disk Space（减小磁盘空间的消耗）</a></li></ol></li><li><ol start="10"><li><a href="#SimplifiedPaymentVerification.">Simplified Payment Verification(简化支付认证).</a></li></ol></li><li><ol start="11"><li><a href="#CombiningandSplittingValue">Combining and Splitting Value</a></li></ol></li><li><ol start="12"><li><a href="#Privacy:">Privacy:</a></li></ol></li><li><ol start="13"><li><a href="#Calculationsbitcoin">Calculations, 计算复杂性，本部分验证了攻克bitcoin的困难性。</a></li></ol></li></ul><!-- vscode-markdown-toc-config    numbering=true    autoSave=true    /vscode-markdown-toc-config --><!-- /vscode-markdown-toc --><h2 id="1-作者信息"><a href="#1-作者信息" class="headerlink" title="1. 作者信息"></a>1. <a name></a>作者信息</h2><ul><li>Satoshi Nakamoto</li><li><a href="mailto:&#x73;&#97;&#x74;&#111;&#115;&#x68;&#x69;&#x6e;&#x40;&#103;&#x6d;&#x78;&#x2e;&#x63;&#111;&#x6d;">&#x73;&#97;&#x74;&#111;&#115;&#x68;&#x69;&#x6e;&#x40;&#103;&#x6d;&#x78;&#x2e;&#x63;&#111;&#x6d;</a></li><li><a href="http://www.bitcoin.org/">www.bitcoin.org</a></li></ul><h2 id="2-行文思路"><a href="#2-行文思路" class="headerlink" title="2. 行文思路"></a>2. <a name="-1"></a>行文思路</h2><p>本文主要介绍了区块链系统，并验证了这个系统的优点和好处。<br>目录：</p><ol><li>Introduction</li><li>Transactions, 介绍peer-to-peer之间去中心化的交易方式，两个问题1. A和B交易，如何保证交易确实来自A, 2. 如何避免double-spending </li><li>Timestamp Server，如何保证区块链的顺序。</li><li>Proof-of-Work，如何产生货币？使用CPU的算力作为Proof-of-Work</li><li>Networks, 如果两个机器同时找到打包区块的答案，谁获得奖励？解决办法是什么？</li><li>Incentive，激励措施，奖励的货币量需要动态变化。</li><li>Reclaiming Disk Space，减少block的磁盘空间。</li><li>Simplified Payment Verification，简化交易验证的方式。</li><li>Combining and Splitting Value，一次交易的组成，有什么好处？</li><li>Privacy，区块链对隐私的保护。</li><li>Calculations，区块链安全性上的证明。</li><li>Conclusion，总结。</li></ol><h2 id="3-参考"><a href="#3-参考" class="headerlink" title="3. 参考"></a>3. <a name="-1"></a>参考</h2><p>主要参考<a href="https://zhuanlan.zhihu.com/p/118120800">知乎</a></p><h2 id="4-Abstract"><a href="#4-Abstract" class="headerlink" title="4. Abstract"></a>4. <a name="Abstract"></a>Abstract</h2><ul><li>digital signatures:<ul><li>可以解决peer-to-peer传输的问题</li><li>不能解决double-spending的问题</li></ul></li><li>提出一个peer-to-peer网络：<ul><li>不需要一个trusted third party去阻止double spending的问题。  </li><li>有一个不停增长的链，这个链承载着hash-based的proof-of-work，新的交易通过挖矿(proof-of-work)去不停的在这个链上增加新的block。</li><li> 最长的chain作为a proof of the sequence of events witnessed，也证明it came from the largest pool of CPU power.</li><li> As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they’ll generate the longest chain and outpace attackers.</li></ul></li></ul><h2 id="5-去中心化的系统"><a href="#5-去中心化的系统" class="headerlink" title="5. 去中心化的系统"></a>5. <a name="-1"></a>去中心化的系统</h2><ul><li>假如有7个人叫A、B、C、D、E、F、G    这7个人每个人手上都有一本账本（分布式账本）    如果A想把100个比特币转给B    那么A需要把“A转账给B 100个比特币”这条记录广播出去，即“A转账给B 100个比特币”的这条信息通过网络让参与这个系统的C、D、E、F、G也一并知道，并且 A、B、C、D、E、F、G 都在自己的账本上记录“A转账给B 100个比特币”。    若假如某一天B想赖账，说A并没有支付给他100个比特币，那么由于A、C、D、E、F、G的账本上都有记录“A转账给B 100个比特币”的这条交易信息，按照“少数服从多数”的原则，B的赖账并不会被系统认可，其他所有参与这个系统的人都是这笔交易的公证人，因此B的赖账无效。<a href="https://zhuanlan.zhihu.com/p/118120800">来源</a></li><li>数字签名（初步理解，可能有错）：</li><li>防止交易信息被篡改</li><li>保证交易信息来自发送方</li><li><img src="/.io//pic1.png" class="lazyload placeholder" data-srcset="/.io//pic1.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>上图的三个框框分别即位ABC</li><li>以A-&gt;B为例，A想和B交易，会产生一条交易信息，以为info(A,B)，需要验证以下几点：<ul><li>交易信息不能被篡改（info(A,B)进行一次hash运算，生成一个摘要message(A,B)）</li><li>保证交易信息来自A，需要A的数字签名，生成这个数字签名的参数来自message(A,B)和A的私钥，这样B（也包括其它的任何的一个节点）在收到这个签名后，可以利用A的公钥进行解密，得到这个message(A,B)，确认这个消息确实来自A，还需要根据广播出去的info(A,B)进行相同的hash运算，生成一个摘要message1(A,B)，这个时候去比一下是否有message(A,B)==message1(A,B)就可以了。</li><li>这里的交易信息info(A,B)包括之前的所有的transactions和B的公钥，之前的所有的transactions来自最长链，B的公钥代表这一笔交易是发给B的。</li></ul></li><li>我们还需要确保没有double-spend，need a way for the payee to know that the previous owners did not sign any earlier transactions.To accomplish this without a trusted party, <strong>transactions must be publicly announced [1]</strong>, and we need a system for participants to agree on a single history of the order in which they were received. <strong>The payee needs proof that at the time of each transaction, the majority of nodes agreed it was the first received</strong>。也就是要证明哪一个交易发生是有效，有效的交易记录在block chain中，无效的交易舍弃，不被承认。</li><li>假如A有50个比特币，支付给B 10个比特币，并且把这条交易信息广播出去。那么所有在比特币系统上的人都可以按照串链起来的交易信息区块去回溯，根据A的每一次交易记录检查A的现在的余额是否充足，若余额充足，则所有比特币系统上的人接受这条信息，并记录在自己的账本里。若A在自己仅有50个比特币的前提下，想支付给B 60个比特币（这个情景类似于前面提到的B把车卖给A后，A是这辆车的所有者。实际上B已经没有这辆车了，但还是把车卖给了C），A把这条交易信息广播出去后，比特币系统上的每一个用户接收到这条消息，同样去追溯A的余额，检查到A余额不足后，拒绝这条交易记录，A此次的交易便失效。<a href="https://zhuanlan.zhihu.com/p/118120800">来源</a></li><li>设想如果A与B交易，之后又与C交易，A与B的交易被大多数人接受了，A与C的交易就会失效</li><li>一个用户的账本（这个用户有多少钱），不是像中心化系统那样去处理的，有一个你的账户，这个账户上有一个你的余额，而是根据和你相关的所有的交易记录去确定你有多少的余额。</li><li>Timestamp Server</li><li>为了确保交易被记录之后，这个账本在之后不被篡改。</li><li><img src="/.io//pic2.png" class="lazyload placeholder" data-srcset="/.io//pic2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>我们假设有这么几个区块，区块A、区块B、区块C、区块D、区块E…按照如下顺序排列构成一条区块链：    区块A-&gt;区块B-&gt;区块C-&gt;区块D-&gt;区块E-&gt;…假设A是创世区块，就是区块链上的第一个区块。    那么B区块的时间戳哈希值便包含了A区块时间戳的哈希值。我们可以知道，由于B区块包含了A区块的时间戳，我们就可以判断B区块在后，A区块在前。同理，C区块的时间戳又包含了A、B区块的时间戳，D区块又包含了A、B、C区块的时间戳…    时间戳服务就如同区块链上的这条链。保证了区块按照顺序连接成区块链。如果我们要根据区块D来找到区块A就可以运用计算机科学上面的递归思想：根据区块D的哈希值找到D的上一个区块，区块C，又根据区块C找到区块B，根据B找到A，最后由于A是第一个创世区块，没有上一个区块，因此这个递归过程结束，找到了区块A。    传统的时间戳服务，如商品包装盒上的生产日期，想改动的话只需要把包装盒上原来的生产日期抹除，再印上一个新的生产日期即可。而比特币的时间戳服务意味着要想篡改某一个区块的内容，就必须把这个区块前面以及后面的区块统统改掉。    而比特币从发布至今，已经过了十几年，并且系统规定每10分钟链接一个新的区块。那条公认的区块链上面的某一个区块可能前后皆连接了数千万甚至上亿个的区块，因此想要改动所要付出的计算量已经庞大到了一个不可能的程度。    所以，比特币的时间戳服务保证了每个区块里面的交易记录无法被篡改。一个区块只要链接到那条公认的区块链上，就可以保证这个区块里面的所有交易记录皆是真实有效且无法篡改的。<a href="https://zhuanlan.zhihu.com/p/118120800">来源</a></li></ul><h2 id="6-Proof-of-work（工作量证明）"><a href="#6-Proof-of-work（工作量证明）" class="headerlink" title="6. Proof-of-work（工作量证明）"></a>6. <a name="Proof-of-work"></a>Proof-of-work（工作量证明）</h2><ul><li>为什么工作量证明在bitcoin中叫做挖矿，因为消耗的工作量在区块链中，会生成货币，区块链系统会给工作量奖励货币。</li><li>工作量证明的算法设计：<ul><li>找到答案需要花费exponential的时间，但是验证他只需要一次hash</li><li>the hash begins with a number of zero bits. The average work required is exponential in the number of zero bits required and can be verified by executing a single hash.</li><li>Proof-of-work is essentially one-CPU-one-vote. 也就是谁的算力强，谁就越有可能去解出这个算法，获得打包区块的奖励。</li><li>If a majority of CPU power is controlled by honest nodes, the honest chain will grow the fastest and outpace any competing chains.只要大部分的cpu都是honest nodes，那么最有可能打包区块的就是honest nodes，那么这份历史交易记录就不会被篡改。attacker攻击修改交易记录的成功的概率会随着（attacker的算力低于honest nodes的算力的程度）呈现指数级的下降，这一点在下面会证明出来。</li><li>比特币系统是这么规定这道数学题的：    </li><li>1.所有用户打包好的，未接到区块链上的区块，里面包含一串字符串，该字符串包含以下内容：前块的头部+账单的信息+时间戳+随机数。    </li><li>2.对字符串做两次SHA256的哈希运算，得到哈希值（我们前面提到过哈希函数，SHA256哈希函数即是计算出的结果是一个长度为256位的二进制数（如：01001011010111…（256位）））    </li><li>3.比特币系统要求你步骤2的计算前n位必须是0，由于二进制数每个位上只有两种情况：0或1，因此算对这道数学题的概率是1/2的n次方，假如系统规定n=4,那么算对的概率就是1/16。    </li><li>4.由于比特币系统规定每过10分钟，区块链上增加一个新的区块。因此n的大小取决于矿工们的算力，即算力越强，系统设定的n值越大，这道数学题的计算难度越高。直到有人计算出符合系统设定的，前n位为0的256位哈希值，便可以把自己打包好的区块，串链到公认的区块链上。<a href="https://zhuanlan.zhihu.com/p/118120800">来源</a></li></ul></li><li>挖矿的矿机会不断的去尝试Nonce的值去获得答案，去找到问题的答案，获得打包的资格，从而获得挖矿的奖励。</li><li>To compensate for increasing hardware speed and varying interest in running nodes over time, the proof-of-work difficulty is determined by a moving average targeting an average number of blocks per hour. 为了补偿增加的算力等等，问题的难度会动态调整。</li></ul><h2 id="7-Network"><a href="#7-Network" class="headerlink" title="7. Network"></a>7. <a name="Network"></a>Network</h2><ul><li>区块链生成的顺序：</li><li><img src="/.io//pic3.png" class="lazyload placeholder" data-srcset="/.io//pic3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>最长链原则（如果两台机器同时找出了答案，接受谁的打包，奖励给谁？）<ul><li>当同时出现多个用户打包出符合系统要求的区块时，那么先暂时不把打包好的区块串链到公认的区块链上，而是让那些用户继续打包新的区块，并把打包好的区块暂时链接到自己的区块上，形成一个自己的支链。当某一个时刻出现一条最长支链时，那个拥有最长支链的用户，就可以把自己的这条最长支链，链接到公认的区块链上。其他用户打包出的，符合系统要求，但是比较短的支链，则会被系统舍弃。<a href="https://zhuanlan.zhihu.com/p/118120800">来源</a></li></ul></li></ul><h2 id="8-Incentive-激励措施"><a href="#8-Incentive-激励措施" class="headerlink" title="8. Incentive(激励措施)"></a>8. <a name="Incentive"></a>Incentive(激励措施)</h2><ul><li>the first transaction in a block is a special transaction that starts a new coin owned by the creator of the block.第一个区块拥有一些初始货币。</li><li>The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation. In our case, it is CPU time and electricity that is expended.需要给矿工奖励货币，去生成新的货币。</li><li>a greedy attacker按照规则挣钱挣得更多，选择攻击会让他亏本，所以他不会去攻击。</li><li>1.你打包的该区块中的交易记录，某些交易记录需要交易方付出一定比例的比特币作为手续费给打包区块的人。    </li><li>2.打包区块并且成功接入到公认的区块链后。系统规定，每10钟区块链上加入一个新的区块，并且会给打包交易记录区块，并成功接入区块链的用户比特币作为打包成功的奖励。一开始系统每10分钟奖励50个比特币，为了保证比特币的稀缺性，防止通货膨胀（由于货币数量太多导致货币贬值），每过4年，系统奖励的比特币减半，到今天，系统每10分钟奖励的比特币数量为12.5。依此类推，开采出的比特币总量是有限的，为2100万枚。    </li><li>根据最长链原则，公认的区块链就是那条最长的区块链（the longest chain），这条区块链上面的每一个区块，里面的交易记录都是真实有效的。假如有一个人想攻击这个系统，那他必须重做之后的全部工作量证明，伪造出一条比公认的区块链更长的区块链，但前提是他掌控超过全网51%的算力。换句话说，破坏者要和全世界的人作对。根据比特币系统的激励措施，破坏这个系统所付的代价远远大于破坏这个系统得到的收益。    所以，只要是一个智力正常的人，都会按照比特币系统给定的规则老老实实的挖矿，而不会选择去破坏这个系统。<a href="https://zhuanlan.zhihu.com/p/118120800">来源</a></li></ul><h2 id="9-Reclaiming-Disk-Space（减小磁盘空间的消耗）"><a href="#9-Reclaiming-Disk-Space（减小磁盘空间的消耗）" class="headerlink" title="9. Reclaiming Disk Space（减小磁盘空间的消耗）"></a>9. <a name="ReclaimingDiskSpace"></a>Reclaiming Disk Space（减小磁盘空间的消耗）</h2><ul><li>为了减少每一个block的空间的消耗（因为需要记录每一个交易的hash，先后关系），transactions are hashed in a Merkle Tree, only the root included in the block’s hash.这样一个block的消耗：<ul><li>A block header with no transactions would be about 80 bytes， blocks are generated every 10 minutes，80 bytes * 6 * 24 * 365 = 4.2MB per year.</li></ul></li><li>好处：<ul><li>节省空间</li><li>可以<strong>保证交易信息不被篡改，修改任何一个交易记录，其它的hash值都会被改掉</strong></li></ul></li></ul><h2 id="10-Simplified-Payment-Verification-简化支付认证"><a href="#10-Simplified-Payment-Verification-简化支付认证" class="headerlink" title="10. Simplified Payment Verification(简化支付认证)."></a>10. <a name="SimplifiedPaymentVerification."></a>Simplified Payment Verification(简化支付认证).</h2><ul><li>也就是验证交易数据是否是正确，是否真实存在，</li><li>用户只需要复制下最长链的所有区块头信息，然后获取和某一笔支付相关的默克尔树的分叉哈希值。用户无法独自核实一笔交易是否有效，但是可以通过将交易关联到链上来检验，他就可以看其他的网络节点曾经接受过这笔交易，还可以看后续生成的区块也进一步确认全网已经接受了这笔交易。<a href="https://zhuanlan.zhihu.com/p/118120800">来源</a></li><li>由于验证需要下载最长链上的所有的header节点，需要很大的空间，因此区块链把节点分为 全节点，轻节点和SPV（Simplified Payment Verification）节点。</li><li>Businesses that receive frequent payments will probably still want to run their own nodes for more independent security and quicker verification.</li></ul><h2 id="11-Combining-and-Splitting-Value"><a href="#11-Combining-and-Splitting-Value" class="headerlink" title="11. Combining and Splitting Value"></a>11. <a name="CombiningandSplittingValue"></a>Combining and Splitting Value</h2><ul><li>每一个交易的形式：</li><li><img src="/.io//pic4.png" class="lazyload placeholder" data-srcset="/.io//pic4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>比特币采用的是全民记账，并且在比特币交易上的人,都是通过哈希加密匿名的，只能通过地址交易。矿工的记账只是记录每一个地址的交易信息，而并不计算地址的余额。因为比特币每一个账本上的内容都是相同的，所以比特币的记账过程可以举例为：    <ul><li>A通过挖矿获得了12.5个比特币，并且用了0.1个比特币向B买了部 iphone11pro ,那么A交易信息包含的输入（in）和 输出(out)分别为：    </li><li>in：空用户给了A 12.5个比特币（因为12.5个比特币是系统的奖励，不是某个用户给A的，因此可以把付款方看成是一个空用户）。   </li><li> out:A给了B 0.1 个比特币，A给了A 12.4个比特币（这里相当于A给A自己找零）    </li></ul></li><li> 比特币钱包只需要根据比特币系统中A的所有交易记录，便可以推算出A的余额。</li></ul><h2 id="12-Privacy"><a href="#12-Privacy" class="headerlink" title="12. Privacy:"></a>12. <a name="Privacy:"></a>Privacy:</h2><ul><li><img src="/.io//pic5.png" class="lazyload placeholder" data-srcset="/.io//pic5.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>省去了trusted third party，trusted third party需要我们对第三方有着绝对的信任。</li><li>bitcoin的交易记录完全是公开的，知道哪个公钥和哪个公钥之间产生了交易记录，但是却不知道这个公钥对应着谁。</li></ul><h2 id="13-Calculations-计算复杂性，本部分验证了攻克bitcoin的困难性。"><a href="#13-Calculations-计算复杂性，本部分验证了攻克bitcoin的困难性。" class="headerlink" title="13. Calculations, 计算复杂性，本部分验证了攻克bitcoin的困难性。"></a>13. <a name="Calculationsbitcoin"></a>Calculations, 计算复杂性，本部分验证了攻克bitcoin的困难性。</h2><ul><li>数学方法上的证明。</li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Block Chain </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Block Chain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Demystifying Illegal Mobile Gambling Apps</title>
      <link href="/2021/10/29/demystifying-illegal-mobile-gambling-apps/"/>
      <url>/2021/10/29/demystifying-illegal-mobile-gambling-apps/</url>
      
        <content type="html"><![CDATA[<h1 id="Demystifying-Illegal-Mobile-Gambling-Apps"><a href="#Demystifying-Illegal-Mobile-Gambling-Apps" class="headerlink" title="Demystifying Illegal Mobile Gambling Apps"></a>Demystifying Illegal Mobile Gambling Apps</h1><p>更加重要的是<strong>对文章整体思路的把握</strong>，包括这篇文章的<strong>研究动机是什么、为什么这个事情值得研究、作者从哪些方面开展研究的、整体的研究思路和方法是什么，等等</strong>。可以先用几句话提纲挈领的把这些骨干的内容都总结好，然后再关注到具体的细节，就可以实现整体和局部的统一。</p><h2 id="Motivation-amp-Significance"><a href="#Motivation-amp-Significance" class="headerlink" title="Motivation &amp; Significance"></a>Motivation &amp; Significance</h2><ul><li>Mobile gambling app has become one of the most popular and lucrative underground businesses in the mobile app ecosystem</li><li>mobile gambling apps have not been investigated by our research community</li><li>demonstrates the urgency for detecting and regulating illegal gambling apps.<h2 id="Research-Direction"><a href="#Research-Direction" class="headerlink" title="Research Direction"></a>Research Direction</h2></li><li>perform a 5-month dataset collection process to harvest illegal gambling apps in China</li><li>characterize the gambling apps from various perspectives including app distribution channels(Section 4), network infrastructure, malicious behaviors, abused third-party and payment services(Section 5), relations among the illegal gambling apps, and identify the illegal campaigns that create and operate the gambling apps(Section 6)</li><li>propose a “guilt-by-association” expansion method to identify new suspicious gambling services.<h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2></li><li>Datasets<ul><li><font color="#FF0000">主要记录了一下数据搜集的方法</font></li><li>Because illegal gambling apps have <strong>a strong correlation with illegal online gambling websites</strong>, so <strong>analyze over 40 million domains by cooperating with a major Internet Service Provider (ISP) in China, a machine-learning based approach to identify illegal online gambling websites, a semi-automated approach to download illegal gambling apps</strong><ul><li>website<ul><li>DNS request data of all the users in a major city from August 2019 to January 2020</li><li>To identify gambling domains, we first filtered irrelevant domains using the ICP license data and Alexa top 100,000 sites. <strong>speed up the detection proces</strong>, a list of gambling keywords (over 100) in both Chinese and English. <strong>keep the domains that have embedded at least one keyword in their web content</strong>.</li><li>we borrow the idea of <strong>an existing method [56] to identify illegal gambling websites</strong>, each web page is parsed to extract all the shown texts. After removing stop words, we represent each web page as a feature vector containing a number of keywords. A SVM classifier is then used to identify</li><li>manually confirmed illegal gambling websites.</li></ul></li><li>app<ul><li>analyze these websites to download their corresponding gambling apps</li><li>semi-automated approach</li><li>we first analyze the contents to identify and click any potential download links automatically.</li><li>we further visit them and download apps manually</li><li>a semi-automated process (download, launch, and explore the apps) to select true gambling apps, ecorded the screenshots of the app runtime UIs</li></ul></li></ul></li><li>3,366 unique gambling apps with 5,344 different versions, 1,136 different developer signatures, downloaded from 1,415 unique websites.</li><li><a href="https://mobile-app-research.github.io/">https://mobile-app-research.github.io/</a></li></ul></li><li>PRELIMINARY STUDY OF ILLEGAL GAMBLING APPS<ul><li><font color="#FF0000">前期调研做的一些工作，记录一下</font></li><li><img src="/.io//Figure1.jpg" class="lazyload placeholder" data-srcset="/.io//Figure1.jpg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>manual observation, working process of illegal gambling apps in Figure 1.</li><li>2.1 Gambling Websites and Gambling Apps</li><li>2.2 App Distribution Channels</li><li>2.3 App Server/Network Infrastructure</li><li>2.4 Third-Party Services</li><li>2.5 Payment Services</li><li>2.6 Creators and Illicit Campaigns</li></ul></li><li>app distribution channels(Section 4)<ul><li><font color="#FF0000">下面主要是一些对数据的分析，不同的工作不一定，大概记录一下</font></li><li><img src="/.io//Figure3.png" class="lazyload placeholder" data-srcset="/.io//Figure3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"><ul><li>gambling apps (blue), gambling websites (green), type-1 download services (red), and type-2 download services (orange).</li><li>two kinds of edges in the graph:<ul><li><ol><li>The edge between a gambling website and a type-2 download service indicates that the gambling website uses the download service to distribute gambling apps</li></ol></li><li><ol start="2"><li>The edge between a download service (type-1 or type-2) and a gambling app indicates that the gambling app was downloaded from the service.</li></ol></li></ul></li></ul></li><li>none of them was found on Androzoo</li><li>the relationship among gambling websites, gambling apps, and their distribution channels.<ul><li>345 download services have identical domain names with the gambling websites (Type-1)</li><li>different domain names with gambling websites(Type-2).</li><li>some distribution sites have been linked by hundreds of gambling websites, providing download services for all of them.</li><li>Relation Graph</li><li><img src="/.io//Table2.png" class="lazyload placeholder" data-srcset="/.io//Table2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>Abusing third-party app distribution channel<ul><li>some download services (52 in total) are common services</li></ul></li></ul></li></ul></li><li>network infrastructure, malicious behaviors, abused third-party and payment services(Section 5)<ul><li><font color="#FF0000">下面主要是分析app的各种行为，记录了一些分析的工具和方法</font></li><li>Connected Server Addresses<ul><li>collect the network traffic<ul><li> <strong>DroidBo, a widely used Android app automated input generation tool</strong>, run all the collected gambling apps on <strong>real phones</strong>, collect its contacted server addresses, <strong>tcpdump</strong> record all the network traffic and netstat to distinguish gambling apps traffic from other apps.</li></ul></li><li>Filtering Common Server Addresses. <ul><li>the common server addresses would be connected by a number of apps embedded in the corresponding third-party libraries.</li></ul></li><li>Domain Analysis<ul><li>Top Level Domains</li><li>IP Addresses</li><li>Registrants and Registrars</li></ul></li></ul></li><li>Malicious Behaviors<ul><li>VirusTotal, to identify potential malware among the gambling apps</li><li>AVClass, malware family labeling tool, to assign a family name for the malicious gambling apps reported by VirusTotal.</li></ul></li><li>Third-party Services<ul><li>collected the generated traffic of gambling apps during runtime, common domains, either visiting the domains or investigating them in search engines.</li><li>LibRadar, third party library detection tool, embedded third party libraries in gambling apps</li></ul></li><li>Payment Services<ul><li>We interact with these apps to reach the payment UI and intercept the network traffic to pinpoint the fourth-party payment platforms</li><li>placing a large number of orders but not making actual payments. In this way, we can get a number of virtual merchant accounts</li></ul></li></ul></li><li>relations among the illegal gambling apps, and identify the illegal campaigns that create and operate the gambling apps(Section 6)<ul><li><font color="#FF0000">下面主要是根据特征进行推断，预测</font></li><li>Inference based on HTTPS certificates<ul><li>the information collected from the gambling domains, use VirusTotal to collect the latest certificate information of all the gambling domain names we identified and then extract the SAN data in the certificate</li></ul></li><li>Inference based on app developer signatures<ul><li>AOSP project</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Software Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Software Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Longitudinal Study of Removed Apps in iOS App Store.md</title>
      <link href="/2021/10/20/a-longitudinal-study-of-removed-apps-in-ios-app-store-md/"/>
      <url>/2021/10/20/a-longitudinal-study-of-removed-apps-in-ios-app-store-md/</url>
      
        <content type="html"><![CDATA[<p>目前的笔记内容更多是对文章主体内容部分的总结，细节有些多。但对于一个读书笔记或者阅读一篇论文来说，更加重要的是<strong>对文章整体思路的把握</strong>，包括这篇文章的<strong>研究动机是什么、为什么这个事情值得研究、作者从哪些方面开展研究的、整体的研究思路和方法是什么，等等</strong>。可以先用几句话提纲挈领的把这些骨干的内容都总结好，然后再关注到具体的细节，就可以实现整体和局部的统一。</p><h1 id="Motivation-amp-Significance"><a href="#Motivation-amp-Significance" class="headerlink" title="Motivation &amp; Significance"></a>Motivation &amp; Significance</h1><ol><li><strong>ineffectiveness of app vetting</strong>, making potentially harmful and policy-violation apps <strong>sneak into the market from time to time</strong>, market maintainers <strong>have to remove undesired apps from the market periodically in a reactive manner</strong>.</li><li>Solutions<ul><li>design an <strong>automated approach to flagging the removed apps</strong>.<br>even without accessing to the bytecode of mobile apps, we can identify the removed apps with good performance (F1=83%)</li><li><strong>a whistle blower that pinpoints policy-violation behaviors timely</strong></li></ul></li></ol><h1 id="Research-Direction"><a href="#Research-Direction" class="headerlink" title="Research Direction"></a>Research Direction</h1><ol><li><p>Data Collection: </p><ul><li>a large-scale and longitudinal study of removed apps in iOS app store <strong>1.5 years</strong></li><li><strong>By comparing each two consecutive snapshots</strong>, we have <strong>collected the information of over 1 million removed apps</strong> with their accurate removed date.</li></ul></li><li><p>Phenomenon</p><ul><li>We observe that, although most of the removed apps are low-quality apps (e.g., outdated and abandoned), a number of the removed apps are quite popular.</li><li><strong>removal of such popular apps</strong>， ranking fraud, fake description, and content issues</li></ul></li><li><p>Solutions</p><ul><li>design <strong>an automated approach to flagging the removed apps</strong><br>even without accessing to the bytecode of mobile apps, we can identify the removed apps with good performance (F1=83%)</li><li><strong>a whistle blower that pinpoints policy-violation behaviors timely</strong></li></ul></li><li><p>Details</p><ul><li><strong>Dataset</strong>: To the best of our knowledge, this is the largest and most comprehensive dataset of removed apps studied in the research community.</li><li><a href="https://github.com/LuckyFQ/iOS-Removed-Apps-Dataset">Dataset</a></li><li>App:<ul><li>App Objective Information</li><li>App Subjective Quality</li><li>App Popularity Information(be reflected as its ranking in the market, app search optimization (ASO) capability, This kind of information was obtained from the cooperated app intelligence company by daily monitoring the search results of massive keywords in iOS app store, be used in Section 5 and Section 6 for characterizing removed apps with ranking fraud)</li></ul></li></ul></li><li><p>Research direction</p></li></ol><ul><li><p>How many apps were removed from iOS App Store? What is the distribution of removed apps across time (e.g., daily) and space(e.g., category and app popularity) dimensions?</p><ul><li><p>Overall trend of removed apps</p><ul><li>Overall Statistics</li><li>Category Distribution</li></ul></li><li><p>Popularity of Removed Apps</p><ul><li>App Ranking</li><li>The number of user ratings</li></ul></li><li><p>Developers of Removed Apps</p><ul><li>Proportion of removed apps per developer</li><li>The most aggressive developers.</li></ul></li><li><p>Life-cycle of Removed Apps</p><ul><li>Update Date VS. Removal Date</li><li>Release Date VS. Removal Date</li><li>Removal Date VS. Re-launch Date</li></ul></li></ul></li><li><p>Why were they removed from iOS App Store?</p><ul><li>This observation motivates us to investigate the practical reasons behind the removal of such popular apps.</li><li></li></ul></li><li><p>Can we identify the removed apps in advance?</p></li><li><p>What have been done: </p><ul><li><strong>characterize</strong> the overall landscape of removed apps</li><li><strong>machine learning techniques</strong> identifying suspicious apps that should be removed.</li><li>comprehensively measured the overall landscape of removed apps including the <strong>daily trend, app popularity, and app life-cycle</strong>, etc</li></ul></li></ul><h1 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h1><h2 id="Data-collection"><a href="#Data-collection" class="headerlink" title="Data collection"></a>Data collection</h2><ul><li>we crawl the iOS app market to maintain a snapshot of the whole market everyday. By comparing the two consecutive snapshots, it is easy to know which apps were removed, and their accurate removal date.</li><li>1,129,615 app removal records, which correspond to 1,033,488 different mobile apps<ul><li>App Objective Information</li><li>App Subjective Quality</li><li>App Popularity Information, <strong>be reflected as its ranking in the market</strong>, This kind of information was obtained from the cooperated app intelligence company by daily monitoring the search results of massive keywords in iOS app store.</li></ul></li></ul><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li><p><font color="#FF0000">下面主要是对已有数据集的分析部分，属于数据挖掘的范畴</font></p></li><li><p><strong>the life-cycle of the removed apps, their correlation with other app meta information, and the developers of them</strong></p><ul><li>Overall trend of removed apps<ul><li>Overall Statistics</li><li>Category Distribution</li><li><img src="/.io//Figure1,2.png" class="lazyload placeholder" data-srcset="/.io//Figure1,2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>Popularity of Removed Apps<ul><li>App Ranking. <ul><li>make efforts to harvest the daily app ranking information across the 25 categories.</li><li>For the removed apps, over 10% of them are popular apps in categories including Finance, Books, Sports, Reference, Navigation, Weather, and Magazines &amp; Newspapers.</li></ul></li><li>The number of user ratings</li><li><img src="/.io//Figure3,4.png" class="lazyload placeholder" data-srcset="/.io//Figure3,4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>Developers of Removed Apps<ul><li> are there any developers who tend to release policy-violation apps? It can be perceived by analyzing the proportion of removed apps in all the apps they released</li><li> are the removed apps dominated by a small number of aggressive developers?</li><li> Proportion of removed apps per developer</li><li> The most aggressive developers.</li><li> <img src="/.io//Figure5.png" class="lazyload placeholder" data-srcset="/.io//Figure5.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>Life-cycle of Removed Apps<ul><li> Update Date VS. Removal Date</li><li> Release Date VS. Removal Date</li><li> Removal Date VS. Re-launch Date</li><li> <img src="/.io//Figure6.png" class="lazyload placeholder" data-srcset="/.io//Figure6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li></ul></li><li><p>DEMYSTIFYING REMOVED POPULAR APPS</p><ul><li>manual efforts to label some apps as the benchmark.</li><li>select normal popular apps that ranked in top-10 comparison</li><li>manually analyze all user comments before app removal</li><li>labelled by two authors independently</li><li><strong>almost all the mis-behaviors in these removed popular apps can be reflected in app review, app description, and ASO keywords</strong><ul><li><img src="/.io//Table2.png" class="lazyload placeholder" data-srcset="/.io//Table2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>app review (Section 5.2), <ul><li><img src="/.io//Table3.png" class="lazyload placeholder" data-srcset="/.io//Table3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li><img src="/.io//Figure7.png" class="lazyload placeholder" data-srcset="/.io//Figure7.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>more than M words and appear N times in the dataset are considered as abnormal reviews.</li><li>a large portion of users in the removed popular apps are suspicious to be abnormal users, while the proportion of potential<br>abnormal users is quite low in normal apps.</li></ul></li><li>ASO keywords (Section 5.3), <ul><li>the characteristics of ASO keywords are quite different between normal apps and removed apps</li><li>One of the app:<ul><li><img src="/.io//Figure8.png" class="lazyload placeholder" data-srcset="/.io//Figure8.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>the number of keywords of removed apps is more likely to increase sharply, while that of normal apps tends to keep stable</li></ul></li></ul></li><li>and app description (Section 5.4)<ul><li>App Description VS. ASO Keywords.</li><li>the description introduces the main function of an app, the keywords should be covered by the descriptions</li><li><img src="/.io//Table4.png" class="lazyload placeholder" data-srcset="/.io//Table4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li></ul></li><li><p><font color="#FF0000">下面主要是根据数据挖掘得到的信息，进行建模预测的部分</font></p><ul><li><img src="/.io//Table5.png" class="lazyload placeholder" data-srcset="/.io//Table5.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>three sets of features.<ul><li>The first set of features represents characteristics of app reviews.</li><li>The second set of features is extracted from ASO keywords</li><li>The third set of features is extracted from app description</li></ul></li><li>Machine learning models.<ul><li>five representative mod els for the classification tasks: Logistic Regression, Sup port Vector Machine (a.k.a, SVM), K-Nearest Neighbors (a.k.a, KNN), Random Forest, and Gradient Boosting Decision Tree (a.k.a, GBDT).</li><li>based on scikit-learn4 and LightGBM</li><li>10-fold cross validation</li></ul></li><li>the feature importance in the classification.<ul><li>The number of abnormal users is the most important feature while predicting the removal of an app.</li><li>Keyword Coverage Percentage, Keyword Standard Deviation, and Review Standard Deviation are also important while predicting removal apps.</li></ul></li><li>Predicting the Removed Apps in Advance<ul><li>flag the policy-violation apps in advance</li><li>i.e., predicting whether an app will be removed from one day in advance to six days in advance, and predicting whether an app will be removed once its mis-behaviors appear in app metadata.</li><li>Predicting the removed apps in advance<ul><li>exclude the information in the last day and extract the features from the remaining information. Our expectation is that the policy-violation apps may release some initial signals that can be caught by our model.</li><li><img src="/.io//Figure9.png" class="lazyload placeholder" data-srcset="/.io//Figure9.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li><li>Predicting the removed apps once the mis-behaviors appear.<ul><li>manual efforts to label the ground truth dataset.</li><li>label the date when mis-behaviors first appear according to reviews and keywords. Each removed app is labelled by two authors independently.</li></ul></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Software Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Software Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Build a blog like this</title>
      <link href="/2021/10/14/build-a-blog-like-this/"/>
      <url>/2021/10/14/build-a-blog-like-this/</url>
      
        <content type="html"><![CDATA[<h1 id="How-to-build-a-blog-like-this"><a href="#How-to-build-a-blog-like-this" class="headerlink" title="How to build a blog like this?"></a>How to build a blog like this?</h1><ol><li>How to build a blog like this? What you need is this <a href="https://fuhanshi.github.io/2018/10/03/Hexo-Github%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E7%82%AB%E9%85%B7%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">blog</a> .(There may exists some typos, but it’s not a big problem.)</li><li>Then how to replace a theme? What you need is this <a href="https://www.jianshu.com/p/ef7a29e3ee8e">blog</a>.</li><li>Then where to find many many wonderful themes? What you need is this <a href="https://hexo.io/themes/">website</a>.</li><li>About how to config this website? <a href="https://yuang01.github.io/">clicke here</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> About Me </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Blogs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rise of the Planet of the Apps:A Systematic Study of the Mobile App Ecosystem</title>
      <link href="/2021/10/13/rise-of-the-planet-of-the-apps-a-systematic-study-of-the-mobile-app-ecosystem/"/>
      <url>/2021/10/13/rise-of-the-planet-of-the-apps-a-systematic-study-of-the-mobile-app-ecosystem/</url>
      
        <content type="html"><![CDATA[<!-- <figure class="third">    <img src="./my_sketch.jpg" class="lazyload placeholder" data-srcset="./my_sketch.jpg" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" height="200"></figure> --><h1 id="Rise-of-the-Planet-of-the-Apps-A-Systematic-Study-of-the-Mobile-App-Ecosystem"><a href="#Rise-of-the-Planet-of-the-Apps-A-Systematic-Study-of-the-Mobile-App-Ecosystem" class="headerlink" title="Rise of the Planet of the Apps:A Systematic Study of the Mobile App Ecosystem"></a>Rise of the Planet of the Apps:A Systematic Study of the Mobile App Ecosystem</h1><h2 id="3-APP-Popularity"><a href="#3-APP-Popularity" class="headerlink" title="3. APP Popularity"></a>3. APP Popularity</h2><h3 id="3-1-Pareto-Effect"><a href="#3-1-Pareto-Effect" class="headerlink" title="3.1 Pareto Effect"></a>3.1 Pareto Effect</h3><ul><li><strong>web content</strong>[25] and<strong>video downloads</strong> [21] usually follow the Pareto Principle: 20% of the objects are responsible for 80% of the downloads。</li><li>app ranking : ranked from the most popular to the least popular</li><li>in AppChina and Anzhi: about 10% of the apps are responsible for close to 90% of all downloads, 10% of the1Mobile apps are responsible for more than 85% of the downloads, 10% of the SlideMe apps are responsible for more than 70% of all downloads in this appstore</li><li>We see that this uneven distribution of popularity goes all the way into the top 1% of the applications, which are responsible for more than 70% of downloads in Anzhi, more than 60% in AppChina, more than 55% in 1Mobile, and more than 30% in SlideMe.</li><li>Pareto effect is signifificant</li><li><ul><li><img src="/.io//Figure2.png" class="lazyload placeholder" data-srcset="/.io//Figure2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="APP Popularity distribution"></li></ul></li></ul><h3 id="3-2-Power-law-Behavior"><a href="#3-2-Power-law-Behavior" class="headerlink" title="3.2 Power-law Behavior"></a>3.2 Power-law Behavior</h3><ul><li>power law distribution</li><li>their main “trunk” has a linear slope indicating a Zipf distribution, which is truncated at both ends</li><li><img src="/.io//Figure3.png" class="lazyload placeholder" data-srcset="/.io//Figure3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="APP Popularity distribution"></li><li>The truncation for <strong>small x values</strong> (i.e., most popular apps) seems to follow similar patterns shown in the <strong>downloads of file sharing systems [34] and video downloads in YouTube</strong> [21]<ul><li><strong>fetch-at-most-one principle</strong><ul><li>That is, objects shared in a file sharing system, such as music, videos, movies, etc., tend to be downloaded at most once by each, the curve for very popular content (i.e., small values in the x axis) tends to flflatten out and reach a value close to the number of users in the system.</li><li>To validate that the fetch-at-most-once property, we draw this figure:<ul><li><img src="/.io//Figure3.png" class="lazyload placeholder" data-srcset="/.io//Figure3.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="APP Popularity distribution"></li><li>fetch-at-most-once property applies to mobile app market, as apps are not updated often.</li></ul></li></ul></li></ul></li><li>Figure 3 clearly shows that there is a significant curvature for large x values (less popular apps) as well。<ul><li>has been thought to be attributed to search engines and recommendation systems</li><li><strong>we feel that this curvature is an instance of a more general phenomenon</strong>, to which we refer as the <strong>clustering effect</strong>.</li></ul></li></ul><h2 id="4-THE-CLUSTERING-EFFECT"><a href="#4-THE-CLUSTERING-EFFECT" class="headerlink" title="4. THE CLUSTERING EFFECT"></a>4. THE CLUSTERING EFFECT</h2><h3 id="4-1-Users-Focus-on-a-Few-Categories"><a href="#4-1-Users-Focus-on-a-Few-Categories" class="headerlink" title="4.1 Users Focus on a Few Categories"></a>4.1 Users Focus on a Few Categories</h3><ul><li>(A) apps are categorized into clusters: Most appstores al ready group apps into clusters</li><li>(B)<strong>once users have downloaded an app from one cluster, it is more likely to download another app from the same cluster</strong></li><li><font color="#FF0000">如何验证上面的(B)这个结论？</font></li></ul><ol><li>验证用户下载了一个东西，他就更可能下载这个类别中的另一个东西的 最好的方法是去获取用户的下载数据，但是<strong>这个数据不好获取</strong>。</li><li><strong>去获取更好获取的数据</strong></li></ol><ul><li>If a user posts a publicly available comment for a particular app, we assume that the user has download the app. Thus, publicly available comments provide us with access to a sub set of the download patterns of individual users.</li><li><code>&lt;font color=#FFFF00&gt;</code>取一个用户下载数据的子集<strong>评论数据集</strong><code>&lt;/font&gt;</code><ul><li>361,282 users to 60,196 apps in 34 categories.</li><li><img src="/.io//Figure5.png" class="lazyload placeholder" data-srcset="/.io//Figure5.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="评论数据集的数据集特征"></li><li>most users tend to download apps from a single or very few categories,<strong>which implies a clustering effect!</strong></li></ul></li></ul><h3 id="4-2-Temporal-Affinity-to-App-Categories"><a href="#4-2-Temporal-Affinity-to-App-Categories" class="headerlink" title="4.2 Temporal Affinity to App Categories"></a>4.2 Temporal Affinity to App Categories</h3><ul><li><p>there are no dominant categories in terms of downloads.</p><p><code>&lt;font color=#00CCFF&gt;</code>人们对那些类型数据的偏爱并没有导致人们偏向于下载那一类型内的app的论证是必要的。<code>&lt;/font&gt;</code></p></li><li><p>按照时间顺序对用户的评论排列形成一个串，例如下载的app分别为a1,a2,a3…,an，首先对这个序列去重，只保留第一个出现的ai，例如两个a1出现，只保留第一个a1,然后求得每一个a的类别ci得到一个序列，然后定义以下:</p><ul><li>affinity:<img src="/.io//Equ1.png" class="lazyload placeholder" data-srcset="/.io//Equ1.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="评论数据集的数据集特征"></li><li>给定一个app的类型序列c1,c2,c3…cn，以步长step为1移动窗口，计算每个窗口内是否产生了affinity，进行累加</li></ul></li><li><p>如果用户随机的进行评论，也会得到一个affinity，只有序列的affinity大于这个数字，才能说明CLUSTERING EFFECT确实存在：</p></li><li><p>扩大公式1的窗口的长度，从1扩大到d：</p></li><li><p>扩大公式2的适用范围，增加深度的概念，也就是公式2适用于random walk 2下的情况，这次我们要random walk多次：</p></li><li><p>应用上面的计算序列的affinity的公式和计算随机affinity的公式得到下图：</p><ul><li><img src="/.io//Figure6.png" class="lazyload placeholder" data-srcset="/.io//Figure6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="评论数据集的数据集特征"></li><li>每一个用户/每一个类型的的comment在图上表现为一个点。</li><li>随着depth的大或者小，都有CLUSTERING EFFECT的现象。</li></ul></li></ul><h3 id="4-3-Users-Exhibit-a-Strong-Temporal-Affinity-to-App-Categories"><a href="#4-3-Users-Exhibit-a-Strong-Temporal-Affinity-to-App-Categories" class="headerlink" title="4.3 Users Exhibit a Strong Temporal Affinity to App Categories"></a>4.3 Users Exhibit a Strong Temporal Affinity to App Categories</h3><ul><li>一直到depth到3的时候，都有超过一半的用户身上产生了CLUSTERING EFFECT。</li><li><font color="#FF0033">总结一下，分析app评论的数据，首先需要排除一些干扰的因素，比如这里的对app某种类型的偏爱，然后需要对已有的数据进行分析建立模型，<strong>用到了随机过程的知识</strong>，然后分析可能的原因，得出结果。</font></li></ul><h2 id="5-A-MODEL-FOR-APPSTORE-WORKLOADS"><a href="#5-A-MODEL-FOR-APPSTORE-WORKLOADS" class="headerlink" title="5.  A MODEL FOR APPSTORE WORKLOADS"></a>5.  A MODEL FOR APPSTORE WORKLOADS</h2><p><img src="/.io//Table2.png" class="lazyload placeholder" data-srcset="/.io//Table2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="评论数据集的数据集特征"></p><h3 id="5-1-Model-Description-and-Analysis"><a href="#5-1-Model-Description-and-Analysis" class="headerlink" title="5.1 Model Description and Analysis"></a>5.1 Model Description and Analysis</h3><ul><li>C clusters</li><li>When a user <strong>downloads the first</strong> app, this download is <strong>drawn from a Zipf-like distribution $Z_{G}$</strong></li><li><strong>subsequent downloads</strong> will be <strong>from the same category</strong> of a previous download with probability <strong>p</strong>.</li><li><strong>from a different category</strong> with probability <strong>1−p</strong></li><li>Downloads <strong>from the same category of a previous download</strong> also <strong>follow a Zipf like distribution $Z_{c}$</strong>.</li><li>two rankings: <ul><li>overall ranking i, values between 1 and A</li><li>its ranking j, values between 1 and the number of apps in that category.</li></ul></li><li><strong>users download things like this:</strong><ul><li><img src="/.io//Figure-a.png" class="lazyload placeholder" data-srcset="/.io//Figure-a.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="评论数据集的数据集特征"></li></ul></li><li>Analysis:<ul><li><strong>U users</strong></li><li><strong>each user downloads d apps</strong></li><li><strong>(1 − p) × d</strong> are app selections based on a <strong>pure Zipf distribution</strong></li><li>an app with <strong>rank i</strong> has a probability to be selected for downloading equal to <strong>$(1/i^{z_{r}} )/(\sum_{k=1}^{A}1/k^{z_{r}})$</strong></li><li>when <strong>apps are selected from a specifific cluster $C_{a}$</strong>, <strong>an app with rank j</strong> in this cluster will be selected with <strong>probability equal to $(1/j^{z_{c}} )/(\sum_{l=1}^{S_{C_{a}}}1/l^{z_{c}})$</strong>, $S_{C_{a}}$ the size of cluster $C_{a}$</li><li>综合上面的分析，我们得到公式<ul><li><img src="/.io//Equ5.png" class="lazyload placeholder" data-srcset="/.io//Equ5.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="综合上面的分析，我们得到公式"></li><li>一个app的下载的数目和用户数量U有关，假设每一个用户下载这个app的概率为p，那么总的下载的数目就是$\sum_{i=1}^{U}p$。</li><li>$1-xxx$代表总的减去不下载的概率</li><li> (1 − p) × d Zipf-based selections</li><li> p × d clustering-based selections</li><li> ？？？(没大看懂)</li></ul></li></ul></li></ul><h3 id="5-2-Simulation-based-Model-Validation"><a href="#5-2-Simulation-based-Model-Validation" class="headerlink" title="5.2 Simulation-based Model Validation"></a>5.2 Simulation-based Model Validation</h3><ul><li>To validate our model and understand <strong>the impact of clustering effect on the app downloads distribution</strong>, we developed <strong>three Monte Carlo simulators of an appstore, using ZIPF, ZIPF-at-most-once, and APP-CLUSTERING models</strong>.</li><li>In the <strong>ZIPF simulator</strong> all downloads are <strong>drawn from ZG</strong>. </li><li>In the <strong>ZIPF-at-most-once simulator</strong> all downloads are <strong>drawn from ZG</strong>, but <strong>each user can never download the same app more than once</strong>. </li><li>valide model:<ul><li><img src="/.io//Equ6.png" class="lazyload placeholder" data-srcset="/.io//Equ6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="综合上面的分析，我们得到公式"></li></ul></li></ul><h4 id="5-2-1-Comparing-Modeled-and-Actual-Downloads"><a href="#5-2-1-Comparing-Modeled-and-Actual-Downloads" class="headerlink" title="5.2.1 Comparing Modeled and Actual Downloads"></a>5.2.1 Comparing Modeled and Actual Downloads</h4><p><img src="/.io//Figure8.png" class="lazyload placeholder" data-srcset="/.io//Figure8.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="模拟得到的结果"></p><ul><li>上面是模拟得到的结果图</li><li>下面是模拟得到的结果</li><li><img src="/.io//Figure9.png" class="lazyload placeholder" data-srcset="/.io//Figure9.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="模拟得到的结果"></li><li><img src="/.io//Figure10.png" class="lazyload placeholder" data-srcset="/.io//Figure10.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="模拟得到的结果"></li><li>结论：<strong>APP-CLUSTERING model最好，其次Zip-at-most-once，最后Zip-at-most</strong></li></ul><h2 id="6-APP-PRICING-AND-INCOME"><a href="#6-APP-PRICING-AND-INCOME" class="headerlink" title="6. APP PRICING AND INCOME"></a>6. APP PRICING AND INCOME</h2><h3 id="The-Influence-of-Cost-on-App-Popularity"><a href="#The-Influence-of-Cost-on-App-Popularity" class="headerlink" title="The Influence of Cost on App Popularity"></a>The Influence of Cost on App Popularity</h3><p><img src="/.io//Figure11.png" class="lazyload placeholder" data-srcset="/.io//Figure11.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="The Influence of Cost on App Popularity"></p><ul><li>paid apps clearly follow a pure<strong>power-law distribution</strong></li><li>Free apps follow a similar distribution as the one observed in Section 3.2</li><li>group together all apps that their price is within a range of one dollar in Figure 12</li></ul><h3 id="Developers’-Income"><a href="#Developers’-Income" class="headerlink" title="Developers’ Income"></a>Developers’ Income</h3><p>For simplicity in our measurements we assume that developers get the whole amount from each download of a paid app<br><img src="/.io//Figure13.png" class="lazyload placeholder" data-srcset="/.io//Figure13.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="Developers’ Income"></p><h3 id="Revenue-Stategies"><a href="#Revenue-Stategies" class="headerlink" title="Revenue Stategies"></a>Revenue Stategies</h3><p><img src="/.io//Figure15.png" class="lazyload placeholder" data-srcset="/.io//Figure15.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="Revenue Stategies"></p><h2 id="Implications"><a href="#Implications" class="headerlink" title="Implications"></a>Implications</h2><h3 id="New-replacement-policies-for-improved-app-caching"><a href="#New-replacement-policies-for-improved-app-caching" class="headerlink" title="New replacement policies for improved app caching"></a>New replacement policies for improved app caching</h3><p>simulated an appstore: 60000 apps, 30 categories, 600000 users, 2 million downloads, average size: 3.5M</p><p>three models:</p><ul><li>ZIPF<ul><li>ZIPF exponent $z_r=1.7$ for overall app ranking</li><li>ZIPF exponent $z_c=1.4$ for each cluster’s app ranking</li><li>The percentage ofdownloads based on clustering was set to 90%</li></ul></li><li>ZIPF-at-most-once</li><li>APP-CLUSTERING</li></ul><p>cache strategy: LRU</p><p>cache size: initialized with the respective number of most popular  apps.</p><p>results:</p><ul><li>app caching with an LRU policy achieves a high hit ratio</li><li>the observed<strong>clustering-based user behavior, results in a significantly reduced hit ratio compared to the other two models</strong>.</li><li>While ZIPF-based workload generation (e.g., like web downloads) leads to a hit ratio higher than 99% for all cache sizes</li><li>ZIPF-at-most-once workloads (e.g., like peer to-peer fifile sharing downloads) result in a high hit ratio starting at 94.5% and exceeding 99% for cache sizes larger than 10% of the total apps</li><li>we see<strong>a signifificantly lower hit ratio for APP-CLUSTERING:</strong> from 67.1% up to 96.3% when the cache size varies from 1% to 20% of total apps</li></ul><h3 id="Effective-prefetching"><a href="#Effective-prefetching" class="headerlink" title="Effective prefetching."></a>Effective prefetching.</h3><p>the most popular apps from this category that have not been downloaded by the user can be prefetched to a local place in order to improve user experience and app delivery performance.</p><h3 id="Better-recommendation-systems"><a href="#Better-recommendation-systems" class="headerlink" title="Better recommendation systems"></a>Better recommendation systems</h3><p>new strategy: recommend app in the same catergory based on temporal affifinity of users to app categories.</p><h3 id="Identify-and-help-problematic-apps"><a href="#Identify-and-help-problematic-apps" class="headerlink" title="Identify and help problematic apps"></a>Identify and help problematic apps</h3><h3 id="Larger-category-diversity"><a href="#Larger-category-diversity" class="headerlink" title="Larger category diversity"></a>Larger category diversity</h3><h3 id="Understand-and-improve-app-popularity"><a href="#Understand-and-improve-app-popularity" class="headerlink" title="Understand and improve app popularity"></a>Understand and improve app popularity</h3><h3 id="Maximize-income"><a href="#Maximize-income" class="headerlink" title="Maximize income"></a>Maximize income</h3><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Workload-Characterization"><a href="#Workload-Characterization" class="headerlink" title="Workload Characterization"></a>Workload Characterization</h3><p>target: <strong>workload characterization</strong> of <em>networks and distributed systems</em></p><h4 id="web-workloads"><a href="#web-workloads" class="headerlink" title="web workloads"></a>web workloads</h4><p><font color="#FF0000">分析数据，建立模型拟合分布，解释出现这种现象的原因</font></p><!-- <font color=#008000 >绿色</font><font color=#0000FF >蓝色</font> --><ul><li>Breslau et al. [20]<ul><li><strong>web requests</strong> follow a <strong>Zipf-like distribution</strong></li><li>propose <strong>a simple model</strong> based on distribution to <strong>explain the observed behavior</strong></li></ul></li><li>Crovella and Bestavros [25]<ul><li><strong>web traffic</strong> exhibits <strong>self-similar characteristics</strong></li><li>can be modeled using <strong>heavy-tailed distributions</strong>.</li></ul></li><li>Barford and Crovella [19] <ul><li>Based on <strong>web workload characteristics</strong>,</li><li>propose a model for <strong>representative webworkload generation</strong>.</li></ul></li><li>Our job<ul><li><strong>mobile app popularity</strong> follows a <strong>Zipf-like distribution with truncated ends</strong></li><li>we propose a model that <strong>approximates the observed distribution</strong>.</li></ul></li></ul><h4 id="other-systems’-workload-characterization"><a href="#other-systems’-workload-characterization" class="headerlink" title="other systems’ workload characterization"></a>other systems’ workload characterization</h4><ul><li>Gummadi et al. [34]<ul><li>popularity<strong>distribution in peer-to-peer file sharing systems deviate from Zipf for very popular objects</strong></li><li>introduce a model that explains this behavior</li></ul></li><li>popularity distribution follows<strong>power-law with an exponential cutoff</strong>:<ul><li>Youtube, user-generated video</li><li>live streaming media networks</li><li>protein, e-mail, actor, and collaboration networks</li><li>explanationand understanding of the power-law distributions</li></ul></li></ul><h3 id="Systematic-studies-of-mobile-apps"><a href="#Systematic-studies-of-mobile-apps" class="headerlink" title="Systematic studies of mobile apps"></a>Systematic studies of mobile apps</h3><p><font color="#FF0000">分析app数据，给出优化建议和方法，建立检测和优化系统</font></p><p>collecting and analyzing large sets of mobile apps from multiple marketplaces:</p><ul><li>security and privacy-related analysis:<ul><li>malware detection [45], malware analysis [44], overprivilege identification [30], detection of privacy leaks [26, 32], malicious ad libraries [33], and vulnerability assessment [27].</li></ul></li><li>our job:<br>our analysis focuses on<strong>app popularity and pricing strategies</strong>.</li><li>Xu et al. [42]<strong>the usage behavior of smartphone apps</strong><ul><li>analyzing<strong>IP-level traces</strong> from a tier-1 cellular network provider.</li><li>spatial and temporal locality, geographic coverage, and diurnal usage patterns.</li></ul></li><li>mobile traffific analysis<ul><li>Maier et al. [36]<strong>residential DSL lines of a European ISP</strong><ul><li>mobile devices’ traffic is dominated by multimedia content and applications’ downloads.</li></ul></li><li>Falaki et al. [28] conduct<strong>a traffic analysis</strong> on 43 different smartphones.<ul><li>They show that browsing contributes most of the traffic, and lower layer protocols impose high overheads due to small transfer sizes.</li><li>the factors that<strong>impact performance and power consumption</strong> on smartphone communications and propose improvements</li></ul></li><li>Falaki et al. [29] analyze user behavior in<strong>two different smartphone platforms</strong><ul><li>in order to understand and characterize user activities and their<strong>impact on network and battery</strong>.</li></ul></li><li>Wei et al. [41]<strong>multi-layer system</strong> for<strong>monitoring and profiling Android apps</strong></li><li>Vallina-Rodriguez et al. [40]<ul><li>characterize the<strong>mobile ad traffic</strong></li><li>analyzing a<strong>large network trace</strong> of mobile users.</li><li>the volume, frequency, and content of the mobile ad traffic</li><li>the distribution of ad traffic among the different platforms, ad networks, and mobile apps.</li><li>the energy implications of mobile advertising in smartphone devices.</li></ul></li><li>Tongaonkar et al. [39] study mobile apps’ usage patterns based on mobile ad traffic found in network traces.</li><li>Zhong and Michahelles [43]<br>study app popularity patterns using data from appaware</li></ul></li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ol><li><p>方式：分析应用市场的app popularity and app priceing.</p></li><li><p>发现：</p><ul><li><p>有关用户下载app的分析</p><ul><li>highly-skewed Pareto effect: 10% of the app accouts for 70-90% of the total downloads.</li><li>The app popularity follows a Zipf-like distribution with truncated ends, unlike web and peer-to-peer workloads, but similar to user-generated video content.</li><li>a new download pattern we refer to as “clustering effect.”:<br>users will download the next apps from the same categories as their previous downloads.</li><li>introduced a new model for app downloads based on clustering effect, and we validated with simulations that it approximates very close the actual downloads.</li></ul></li><li><p>有关app价格对app popularity的影响</p><ul><li>paid apps follow a clear power law distribution, probably due to the more selective behavior of users when they are paying to download an app</li><li>results show that the large majority of developers have very low income, while a very small percentage of them make a significantly higher income</li><li>the number of developer’s apps do not increase developer’s income</li><li>free apps with ads need to make just $0.21 per download to match the income of paid apps, which seems a more promising strategy especially for popular apps.</li></ul></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About Software Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Software Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>About Me</title>
      <link href="/2021/08/07/about-me/"/>
      <url>/2021/08/07/about-me/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> About Me </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Me </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
