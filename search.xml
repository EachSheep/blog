<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Data-Free Knowledge Distillation for Heterogeneous Federated Learning</title>
      <link href="/2022/11/20/pmlr21-data-free-knowledge-distillation-for-pmlr21-heterogeneous-federated-learning/"/>
      <url>/2022/11/20/pmlr21-data-free-knowledge-distillation-for-pmlr21-heterogeneous-federated-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="PMLR21-Data-Free-Knowledge-Distillation-for-Heterogeneous-Federated-Learning"><a href="#PMLR21-Data-Free-Knowledge-Distillation-for-Heterogeneous-Federated-Learning" class="headerlink" title="PMLR21-Data-Free-Knowledge-Distillation-for-Heterogeneous-Federated-Learning"></a>PMLR21-Data-Free-Knowledge-Distillation-for-Heterogeneous-Federated-Learning</h1>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
          <category> About FL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About FL </tag>
            
            <tag> About  </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FL</title>
      <link href="/2022/11/12/fl/"/>
      <url>/2022/11/12/fl/</url>
      
        <content type="html"><![CDATA[<h1 id="FL"><a href="#FL" class="headerlink" title="FL"></a>FL</h1><h2 id="Statistical-heterogeneity"><a href="#Statistical-heterogeneity" class="headerlink" title="Statistical heterogeneity"></a>Statistical heterogeneity</h2><h3 id="Adapt-the-global-model-to-accommodate-personalized-local-models-for-non-IID-data"><a href="#Adapt-the-global-model-to-accommodate-personalized-local-models-for-non-IID-data" class="headerlink" title="Adapt the global model to accommodate personalized local models for non-IID data"></a>Adapt the global model to accommodate personalized local models for non-IID data</h3><h4 id="Meta-learning"><a href="#Meta-learning" class="headerlink" title="Meta learning"></a>Meta learning</h4><ul><li><p><strong>arXivâ€™19</strong>-Improving federated learning personalization via model agnostic meta learning-(Yihan Jiang-uw, Jakub KoneÄnÃ½-Google research, Keith Rush, Sreeram Kannan-uw)</p></li><li><p><strong>NIPSâ€™19</strong>-Adaptive gradient-based metalearning methods-(Mikhail Khodak-CMU, Maria-Florina Balcan-CMU, Ameet Talwalkar-CMU)</p></li><li><p><strong>NIPSâ€™20</strong>-Personalized federated learning with theoretical guarantees: A modelagnostic meta-learning approach</p></li><li><p><strong>ICLRâ€™20</strong>-Differentially private metalearning</p></li></ul><h4 id="Multi-task-learning"><a href="#Multi-task-learning" class="headerlink" title="Multi-task learning"></a>Multi-task learning</h4><ul><li><p><strong>NIPSâ€™17</strong>-MOCHA: Federated multi-task learning-(Virginia Smith-stanford, Chao-Kai Chiang-usc, Maziar Sanjabi-usc, Ameet S. Talwalkar-CMU)</p></li><li><p><strong>arXivâ€™19</strong>-Variational federated multi-task learning</p></li><li><p><strong>ICMLâ€™19</strong>-Semi-cyclic stochastic gradient descent</p></li><li><p><strong>NIPSâ€™19</strong>-Adaptive gradient-based metalearning methods</p></li></ul><h4 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h4><ul><li><p><strong>arXivâ€™19</strong>-Federated evaluation of on-device personalization-(Kangkang Wang-Google, Rajiv Mathews-Google, ChloÃ© Kiddon-Google, Hubert Eichner-Google, FranÃ§oise Beaufays-Google, Daniel Ramage-Google)</p></li><li><p><strong>arXivâ€™20</strong>-Three approaches for personalization with applications to federated learning-(Yishay Mansour-Google, Mehryar Mohri-Google, Jae Ro-Google, Ananda Theertha Suresh-Google)</p></li></ul><h4 id="Knowledge-distillation"><a href="#Knowledge-distillation" class="headerlink" title="Knowledge distillation"></a>Knowledge distillation</h4><ul><li><input disabled type="checkbox"> <strong>arXivâ€™19</strong>-Fedmd: Heterogenous federated learning via model distillation-(Daliang Li, Junpu Wang Harvard-Yale-Pennsylvania)</li></ul><h4 id="Lottery-ticket-hypothesis"><a href="#Lottery-ticket-hypothesis" class="headerlink" title="Lottery ticket hypothesis"></a>Lottery ticket hypothesis</h4><ul><li><input disabled type="checkbox"> <strong>arXivâ€™20</strong>-Lotteryfl: Personalized and communication-efficient federated learning with lottery ticket hypothesis on non-iid datasets-(Ang Li-Duke, Jingwei Sun-Duke, Binghui Wang-Duke, Lin Duan-Duke, Sicheng Li-Alibaba, Yiran Chen-Duke, Hai Li-Duke)</li></ul><h3 id="Client-clustering"><a href="#Client-clustering" class="headerlink" title="Client clustering"></a>Client clustering</h3><ul><li><input disabled type="checkbox"> <strong>NIPSâ€™20</strong>-An efficient framework for clustered federated learning-()-()-Client clustering.</li></ul><h3 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h3><ul><li><input disabled type="checkbox"> <strong>NIPSâ€™19Workshop</strong>-Federated learning with local and global representations-(Paul Liang-CMU, Terrance Liu-CMU)</li><li><input disabled type="checkbox"> <strong>arXivâ€™21</strong>-Fed-ensemble: Improving generalization through model ensembling in federated learning</li><li><input disabled type="checkbox"> <strong>ICMLâ€™20</strong>-SCAFFOLD: Stochastic controlled averaging for federated learning</li></ul><h2 id="System-heterogeneity"><a href="#System-heterogeneity" class="headerlink" title="System heterogeneity"></a>System heterogeneity</h2><h3 id="Asynchronous-communication"><a href="#Asynchronous-communication" class="headerlink" title="Asynchronous communication"></a>Asynchronous communication</h3><h3 id="Active-sampling-of-clients"><a href="#Active-sampling-of-clients" class="headerlink" title="Active sampling of clients"></a>Active sampling of clients</h3><ul><li><input checked disabled type="checkbox"> <strong>SysMLâ€™19</strong>-Towards federated learning at scale: System design-(Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub KoneÄnÃ½, Stefano Mazzocchi, H. Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, Jason Roselander)</li><li><input disabled type="checkbox"> <strong>ICCâ€™19</strong>-Client selection for federated learning with heterogeneous resources in mobile edge-(Takayuki Nishio-Kyoto, Ryo Yonetani-Kyoto)</li><li><input disabled type="checkbox"> <strong>OSDIâ€™21</strong>-Oort: Efficient federated learning via guided participant selection</li><li><input disabled type="checkbox"> <strong>NSDIâ€™20</strong>-Sol: A federated execution engine for fast distributed computation over slow networks</li></ul><h2 id="Communication"><a href="#Communication" class="headerlink" title="Communication"></a>Communication</h2><h3 id="Data-compression-techniques"><a href="#Data-compression-techniques" class="headerlink" title="Data compression techniques"></a>Data compression techniques</h3><h4 id="Quantization-and-sketching"><a href="#Quantization-and-sketching" class="headerlink" title="Quantization and sketching"></a>Quantization and sketching</h4><ul><li><input disabled type="checkbox"> <strong>arXivâ€™16</strong>-Federated learning: Strategies for improving communication efficiency-(Jakub KoneÄnÃ½-Google, H. Brendan McMahan-Google, Felix X. Yu-Google, Peter RichtÃ¡rik-KAUST, Ananda Theertha Suresh-Google, Dave Bacon-Google)</li><li><input disabled type="checkbox"> <strong>NIPSâ€™17</strong>-Qsgd: Communication-efficient sgd via gradient quantization and encoding-(Dan Alistarh-ETH, Demjan Grubic-Eth&amp;Google, Jerry Z. Li-MIT, Ryota Tomioka-Microsoft Research, Milan Vojnovic-London School of Economics)</li><li><input disabled type="checkbox"> <strong>NIPSâ€™19</strong>-Communication efficient distributed sgd with sketching-(Nikita Ivkin-Amzaon, Daniel Rothchild-UCB, Enayat UIIah-JHU, Vladimir Braverman-JHU, Ion Stoica-UCB, Raman Arora-JHU)</li><li><input disabled type="checkbox"> <strong>arXivâ€™19</strong>-Error feedback fixes signsgd and other gradient compression schemes</li><li><input disabled type="checkbox"> <strong>arXivâ€™16</strong>-Federated learning: Strategies for improving communication efficiency</li><li><input disabled type="checkbox"> <strong>arXivâ€™18</strong>-Expanding the reach of federated learning by reducing client resource requirements</li><li><input disabled type="checkbox"> <strong>NIPSâ€™18</strong>-ATOMO: Communication-efficient learning via atomic sparsification</li><li><input disabled type="checkbox"> <strong>PISITâ€™18</strong>-Gradient coding using the stochastic block model</li></ul><h3 id="Local-training"><a href="#Local-training" class="headerlink" title="Local training"></a>Local training</h3><ul><li><p><strong>ICAISâ€™17</strong>-Communication-efficient learning of deep networks from decentralized data</p></li><li><p><strong>ICLRâ€™19</strong>-Local SGD converges fast and communicates little</p></li><li><p><input disabled type="checkbox">  CoCoA: A general framework for communication-efficient distributed optimization</p></li><li><p><strong>NSDIâ€™20</strong>-Sol: A federated execution engine for fast distributed computation over slow networks</p></li></ul><h3 id="Split-learning"><a href="#Split-learning" class="headerlink" title="Split learning"></a>Split learning</h3><ul><li><input disabled type="checkbox"> <strong>arXivâ€™20</strong>-Splitfed: When federated learning meets split learning-(Chandra Thapa-Lehigh University, M.A.P. Chamikara-Lehigh University, Seyit Camtepe-Lehigh University, Lichao Sun-Lehigh University)</li></ul><h3 id="Decentralized-training"><a href="#Decentralized-training" class="headerlink" title="Decentralized training"></a>Decentralized training</h3><ul><li><input disabled type="checkbox"> <strong>ICLRâ€™19</strong>-Anytime Minibatch: Exploiting stragglers in online distributed optimization</li><li><input disabled type="checkbox"> <strong>NIPSâ€™19</strong>-Robust and communication-efficient collaborative learning</li></ul><h3 id="Asynchronous-and-synchronous"><a href="#Asynchronous-and-synchronous" class="headerlink" title="Asynchronous and synchronous"></a>Asynchronous and synchronous</h3><ul><li><input disabled type="checkbox"> <strong>NIPSâ€™15</strong>-Deep learning with elastic averaging SGD</li><li><input disabled type="checkbox"> <strong>NIPSâ€™11</strong>:HOGWILD!: A lock-free approach to parallelizing stochastic gradient descent</li></ul><h2 id="Data-privacy"><a href="#Data-privacy" class="headerlink" title="Data privacy"></a>Data privacy</h2><h3 id="Break-privacy"><a href="#Break-privacy" class="headerlink" title="Break privacy"></a>Break privacy</h3><ul><li><input disabled type="checkbox"> <strong>NIPSâ€™20</strong>-Inverting gradients - how easy is it to break privacy in federated learning</li><li><input disabled type="checkbox"> <strong>NIPSâ€™20</strong>-Attack of the tails: Yes, you really can backdoor federated learning</li><li><input disabled type="checkbox"> <strong>arXivâ€™19</strong>-Can you really backdoor federated learning?</li></ul><h3 id="Differentially-private"><a href="#Differentially-private" class="headerlink" title="Differentially private"></a>Differentially private</h3><ul><li><p><strong>PCCCSâ€™17</strong>-Practical secure aggregation for privacy-preserving machine learning</p></li><li><p><strong>NIPSâ€™17</strong>-Differentially private federated learning: A client level perspective</p></li><li><p><strong>ICLRâ€™18</strong>-Learning differentially private recurrent language models</p></li><li><p><strong>MobiComâ€™20</strong>-Billion-Scale Federated Learning on Mobile Clients: A Submodel Design with Tunable Privacy</p></li></ul><h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><ul><li><p><strong>NIPSâ€™17</strong>-â€œCan decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent</p></li><li><p><strong>arXivâ€™18</strong>-Communication-efficient distributed strongly convex stochastic optimization: Non-asymptotic rates</p></li><li><p><strong>SPâ€™19</strong>-Exploiting unintended feature leakage in collaborative learning</p></li><li><p><strong>NIPSâ€™19</strong>-Deep leakage from gradients</p></li><li><p><strong>arXivâ€™20</strong>-idlg: Improved deep leakage from gradients</p></li><li><p><strong>arXivâ€™20</strong>-Threats to federated learning: A survey</p></li><li><p><strong>arXivâ€™21</strong>-Practical and private (deep) learning without sampling or shuffling</p></li></ul><h2 id="Scales"><a href="#Scales" class="headerlink" title="Scales"></a>Scales</h2><ul><li><input disabled type="checkbox"> <strong>arXivâ€™18</strong>-Applied federated learning: Improving Google keyboard query suggestions.</li></ul><h2 id="FLå®šåˆ¶"><a href="#FLå®šåˆ¶" class="headerlink" title="FLå®šåˆ¶"></a>FLå®šåˆ¶</h2><ul><li><input checked disabled type="checkbox"> <strong>ICLRâ€™21</strong>-HETEROFL:  computation and communication efficient federated learning for heterogeneous clients-(Enmao Diao-Duke, Jie Ding-UMN, Vahid Tarokh-Duke)-<a href="https://github.com/dem123456789/HeteroFL-Computation-and-Communication-Efficient-Federated-Learning-for-Heterogeneous-Clients">Code</a></li><li><input checked disabled type="checkbox"> <strong>MobiComâ€™20</strong>-Billion-Scale Federated Learning on Mobile Clients: A Submodel Design with Tunable Privacy</li></ul><h2 id="FLç³»ç»Ÿ"><a href="#FLç³»ç»Ÿ" class="headerlink" title="FLç³»ç»Ÿ"></a>FLç³»ç»Ÿ</h2><h4 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h4><ul><li><input disabled type="checkbox"> <strong>NIPSâ€™19</strong>-Leaf: A benchmark for federated settings-()-<a href="https://leaf.cmu.edu/">Data</a></li><li><input disabled type="checkbox"> <strong>TensorFlow Federated (TFF)</strong>- â€œTensorFlow federated: Machine learning on decentralized data.â€-()-[Data](<a href="https://www.tensorflow/">https://www.tensorflow</a> .org/federated)</li><li><input disabled type="checkbox"> <strong>arXivâ€™20</strong>-FedML: A research library and benchmark for federated machine learning</li><li><input disabled type="checkbox"> <strong>arXivâ€™21</strong>-Flower: A friendly federated learning framework</li><li><input disabled type="checkbox"> <strong>MLSysâ€™20</strong>-Mlperf training benchmark</li><li><input disabled type="checkbox"> PySyft (pys)</li><li><input checked disabled type="checkbox"> <strong>ICML22â€™</strong>-FedScale: Benchmarking Model and System Performance of Federated Learning at Scale</li></ul><h4 id="Others-1"><a href="#Others-1" class="headerlink" title="Others"></a>Others</h4><ul><li><input disabled type="checkbox"> <strong>MLSysâ€™20</strong>-Federated optimization in heterogeneous networks-()-<a href>FedProx</a></li><li><input disabled type="checkbox"> <strong>arXivâ€™20</strong>-Adaptive federated optimization-()-<a href>FedYoGi</a></li><li><input disabled type="checkbox"> <strong>OSDIâ€™21</strong>-Oort: Efficient federated learning via guided participant selection</li></ul><h2 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h2><ul><li><input checked disabled type="checkbox"> <strong>IEEE Signal Processing Magazineâ€™20</strong>-Federated learning: Challenges, methods, and future directions-(Tian Li-CMU, Anit Kumar Sahu, Ameet Talwalkar, Virginia Smith)</li><li><input disabled type="checkbox"> <strong>arXivâ€™21</strong>-Advances and Open Problems in Federated Learning-()</li><li><input disabled type="checkbox"> <strong>ACM Computing Surveys (CSUR)â€™19</strong>-Demystifying parallel and distributed deep learning: An in-depth concurrency analysis</li></ul><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><ul><li><input disabled type="checkbox"> <strong>arXivâ€™18</strong>-Federated learning for mobile keyboard prediction</li><li><input disabled type="checkbox"> <strong>arXivâ€™18</strong>-LoAdaBoost: Lossbased adaboost federated machine learning on medical data</li></ul><h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><h3 id="Novel-models-of-asynchrony"><a href="#Novel-models-of-asynchrony" class="headerlink" title="Novel models of asynchrony"></a>Novel models of asynchrony</h3><p>bulk synchronous approaches and asynchronous approaches, it is worth studying the effects of this more realistic device-centric communication scheme</p><h3 id="Extreme-communication-schemes"><a href="#Extreme-communication-schemes" class="headerlink" title="Extreme communication schemes"></a>Extreme communication schemes</h3><p>optimization methods used for machine learning can tolerate a lack of precision. this error can, in fact, help with generalization. </p><h3 id="Communication-reduction-and-the-Pareto-frontier"><a href="#Communication-reduction-and-the-Pareto-frontier" class="headerlink" title="Communication reduction and the Pareto frontier"></a>Communication reduction and the Pareto frontier</h3><p>how these techniques such as local updating and model compression compose with one another and to systematically analyze the tradeoff between accuracy and communication for each approach. In particular, the most useful techniques will demonstrate improvements at the Pareto frontier.</p><h3 id="Heterogeneity-diagnostics"><a href="#Heterogeneity-diagnostics" class="headerlink" title="Heterogeneity diagnostics"></a>Heterogeneity diagnostics</h3><p>quantify statistical heterogeneity through metrics such as local dissimilarity, however these metrics cannot be easily calculated over the federated network before training occurs</p><h3 id="Granular-privacy-constraints"><a href="#Granular-privacy-constraints" class="headerlink" title="Granular privacy constraints"></a>Granular privacy constraints</h3><h3 id="Beyond-supervised-learning"><a href="#Beyond-supervised-learning" class="headerlink" title="Beyond supervised learning"></a>Beyond supervised learning</h3><h3 id="Productionizing-federated-learning"><a href="#Productionizing-federated-learning" class="headerlink" title="Productionizing federated learning"></a>Productionizing federated learning</h3><ul><li><input disabled type="checkbox"> concept drift(when the underlying data-generation model changes over time)</li><li><input disabled type="checkbox"> diurnal variations(when the devices exhibit different behavior at different times of the day or week)<ul><li><input disabled type="checkbox"> <strong>ICML19â€™</strong>-Semi-cyclic stochastic gradient descent</li></ul></li><li><input disabled type="checkbox"> cold-start problems(when new devices enter the network)</li></ul><h3 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks"></a>Benchmarks</h3>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICLR21-HETEROFL-COMPUTATION AND COMMUNICATION EFFICIENT FEDERATED LEARNING FOR HETEROGENEOUS CLIENTS</title>
      <link href="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/"/>
      <url>/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/</url>
      
        <content type="html"><![CDATA[<h1 id="ICLR21-HETEROFL-COMPUTATION-AND-COMMUNICATION-EFFICIENT-FEDERATED-LEARNING-FOR-HETEROGENEOUS-CLIENTS"><a href="#ICLR21-HETEROFL-COMPUTATION-AND-COMMUNICATION-EFFICIENT-FEDERATED-LEARNING-FOR-HETEROGENEOUS-CLIENTS" class="headerlink" title="ICLR21-HETEROFL-COMPUTATION AND COMMUNICATION EFFICIENT FEDERATED LEARNING FOR HETEROGENEOUS CLIENTS"></a>ICLR21-HETEROFL-COMPUTATION AND COMMUNICATION EFFICIENT FEDERATED LEARNING FOR HETEROGENEOUS CLIENTS</h1><ul><li><p>ä½œè€…ï¼š</p><ul><li><a href="https://diaoenmao.com/%EF%BC%88Enmao">https://diaoenmao.com/ï¼ˆEnmao</a> Diaoï¼‰ï¼ˆåˆæ©èŒ‚ï¼‰ï¼ˆDuke Universityï¼‰</li><li><a href="https://jding.org/%EF%BC%88Jie">https://jding.org/ï¼ˆJie</a> Dingï¼‰ï¼ˆUniversity of Minnesota-Twin Citiesï¼‰</li><li><a href="https://ece.duke.edu/faculty/vahid-tarokh%EF%BC%88Vahid">https://ece.duke.edu/faculty/vahid-tarokhï¼ˆVahid</a> Tarokhï¼‰ï¼ˆDuke Universityï¼‰</li></ul></li><li><p>ä¸­å¤®æœåŠ¡å™¨ç”¨çš„æ¨¡å‹å’Œclientç”¨çš„æ¨¡å‹è¿™æ ·çš„ä¸€ä¸ªFLçš„å‰æå‡è®¾ä¼šä½¿å¾—FLçš„åº”ç”¨å¾—åˆ°æå¤§çš„é™åˆ¶ï¼ŒåŒæ—¶ä¼šå¸¦æ¥clientçš„æ— è°“çš„è®¡ç®—å’Œé€šä¿¡å¼€é”€ã€‚äºæ˜¯ä½œè€…æå‡ºåœ¨clientsä¸Šéƒ¨ç½²å‡ ç§ä¸åŒå¤æ‚åº¦çš„æ¨¡å‹ï¼Œä¸åŒå¤æ‚åº¦çš„æ¨¡å‹åˆ†åˆ«æ›´æ–°serverä¸Šæ¨¡å‹çš„ä¸åŒéƒ¨åˆ†ï¼Œå»ºç«‹çš„æ•°å­¦å…¬å¼è¾ƒä¸ºä¼˜ç¾ã€‚</p></li><li><p>HETEROFLï¼šaddress <strong>heterogeneous clients</strong> equipped with very different computation and communication capabilities</p></li><li><p><strong>For the first time</strong>, our method <strong>challenges the underlying assumption of existing work that local models have to share the same architecture as the global model</strong></p></li><li><p><strong>several strategies</strong> to <strong>enhance FL training</strong> and conduct extensive empirical evaluations, <strong>including five computation complexity levels of three model architecture on three datasets</strong></p></li></ul><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><ul><li><p>A widely accepted assumption is that local models have to share the same architecture as the global model (Li et al., 2020b).</p></li><li><p>It is crucial to address heterogeneous clients equipped with very different computation and communication capabilities.</p></li><li><p><strong>HeteroFL</strong>: This model heterogeneity differs significantly from the classical distributed machine learning framework where local data are trained with the same model architecture.</p></li><li><p>Contributions:</p><ul><li><p>æ¨¡å‹è‡ªå®šä¹‰ï¼špropose an easy-to-implement framework <strong>HeteroFL</strong> that can train <strong>heterogeneous local models</strong> and aggregate them stably and effectively into a single global inference model. Outperforms <strong>state-of-the-art results</strong> <strong>without</strong> introducing <strong>additional computation overhead</strong>.</p></li><li><p>è®¾å¤‡å¼‚æ„ï¼šaddresses <strong>various heterogeneous settings</strong>. the learning result <strong>stable and effective</strong>, <strong>the communication costs</strong> are small. a<strong>llow local clients to adaptively contribute to the training of global models</strong>ï¼ŒSystem heterogeneity and communication efficiency can be well addressed.</p></li><li><p>æ•°æ®å¼‚æ„ï¼šseveral strategies <strong>robust against the balanced non-IID statistical heterogeneity</strong>, <strong>reduce the number of communication rounds</strong>.</p><p>  <strong>â€Masking Trickâ€</strong> for balanced non-IID data partition in n classification problems</p><p>  a modification of Batch Normalization (BN)</p></li></ul></li></ul><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><ul><li>train massively distributed models at a large scale (Bonawitz et al., 2019)</li><li><strong>FedAvg</strong> by McMahan et al. (2017) is currently t<strong>he most widely adopted FL baseline</strong>, which r<strong>educes communication cost by allowing clients to train multiple iterations locally.</strong></li><li><strong>communication efficiency</strong>:<ul><li>data compression techniques(quantization and sketching), split learning</li></ul></li><li><strong>system heterogeneity</strong><ul><li>asynchronous communication</li><li>active sampling of clients</li></ul></li><li><strong>statistical heterogeneity</strong>(major battleground)<ul><li><strong>adapt the global model to accommodate personalized local models for non-IID data</strong><ul><li>integrating FL with other frameworks such as assisted learning, metalearning, multi-task learning, transfer learning, knowledge distillation, lottery ticket hypothesis, but <strong>often introduce additional computation and communication overhead</strong> that may not be necessary.</li></ul></li></ul></li><li><strong>privacy</strong><ul><li>model gradient updates can reveal sensitive information or even local training data</li></ul></li></ul><h2 id="HETEROGENEOUS-FEDERATED-LEARNING"><a href="#HETEROGENEOUS-FEDERATED-LEARNING" class="headerlink" title="HETEROGENEOUS FEDERATED LEARNING"></a>HETEROGENEOUS FEDERATED LEARNING</h2><h3 id="HETEROGENEOUS-MODELS"><a href="#HETEROGENEOUS-MODELS" class="headerlink" title="HETEROGENEOUS MODELS"></a>HETEROGENEOUS MODELS</h3><ul><li>consider <strong>local models</strong> to <strong>have similar architecture</strong> but can <strong>shrink their complexity within the same model class</strong>.</li><li><strong>new challenges</strong>: the optimal way to select subsets of global model parameters, compatibility of the-state-of-art model architecture, and minimum modification from the existing FL framework</li><li>we can modulate the size of deep neural networks by <strong>varying the width and depth of networks</strong>(Zagoruyko &amp; Komodakis, 2016; Tan &amp; Le, 2019):<ul><li>Because we <strong>aim to reduce the computation complexity of local models</strong>, we choose to <strong>vary the width of hidden channels</strong></li><li><strong>locally distributed data</strong>: ${X_1,â€¦,X_m}$ï¼Œm clientsã€‚</li><li><strong>model parameters</strong>: ${W_1, â€¦, W_m}$, m clientsã€‚</li><li><strong>global model</strong>: $W_g$</li><li>each round:<ul><li>$W_g^t=\frac{1}{m}\sum_{i=1}^{m}W_i^t$</li><li>$W_i^{t+1}=W^t_g$</li></ul></li><li>å¯¹äºå…¨å±€å‚æ•°$W_g \in \bold{R}^{d_g \times k_g}$ä¸­çš„æŸä¸€ä¸ªéšè—å±‚$W_l$ï¼Œ$d_gå’Œk_g$åˆ†åˆ«æ˜¯è¾“å…¥è¾“å‡ºçš„å‚æ•°ï¼Œ$W_l$çš„å‚æ•°çš„éƒ¨åˆ†ï¼š$W_l^p \subset W_l^{p-1} â€¦ \subset W_l^{1}$ï¼Œé‚£ä¹ˆ$W_l$è¿™äº›éƒ¨åˆ†çš„å‚æ•°ç›¸å¯¹$W_l$æœ‰ä¸€ä¸ªshrinkï¼Œå³$d_l^p=r^{p-1}d_g$ and $k_l^p=r^{p-1}k_g$ ï¼Œäºæ˜¯æœ‰$|W_l^p|=r^{2(p-1)}|W_g|$, shinkage ratio: $R=\frac{W_l^p}{W_g}=r^{2(p-1)}$</li><li>ä¸‹å›¾å±•ç¤ºçš„å°±æ˜¯ç»™ä¸åŒçš„clentåˆ†é…ä¸åŒå¤§å°çš„modelçš„ä¸€ä¸ªç¤ºæ„å›¾ï¼š</li><li><img src="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123.png" class="lazyload placeholder" data-srcset="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>å…¨å±€èšåˆï¼š<ul><li><img src="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123-1668237191694-2.png" class="lazyload placeholder" data-srcset="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123-1668237191694-2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li><img src="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123-1668237570920-4-1668237816488-6.png" class="lazyload placeholder" data-srcset="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123-1668237570920-4-1668237816488-6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li><img src="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123-1668237829864-8.png" class="lazyload placeholder" data-srcset="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123-1668237829864-8.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li><li>ç®—åŠ›æˆ–è€…ç½‘ç»œçŠ¶å†µä¸å¥½çš„æœºå™¨å°±è·‘å°æ¨¡å‹ï¼Œå¥½çš„å°±è·‘å¤§æ¨¡å‹ï¼Œå…¬å¹³é«˜æ•ˆã€‚</li></ul></li></ul></li></ul><h3 id="STATIC-BATCH-NORMALIZATION"><a href="#STATIC-BATCH-NORMALIZATION" class="headerlink" title="STATIC BATCH NORMALIZATION"></a>STATIC BATCH NORMALIZATION</h3><p>classical FedAvg and most recent works <strong>avoid BN</strong>. A major concern of BN is that <strong>it requires running estimates of representations at every hidden layer</strong>ï¼ˆï¼Ÿï¼‰. <strong>Uploading these statistics to the server will cause higher communication costs and privacy issues</strong> Andreux et al. (2020) proposes to track running statistics locallyã€‚</p><p>batch normalizationåœ¨non-IIDæ•°æ®ä¸Šçš„è¡¨ç°å¹¶ä¸å¥½ï¼š<a href="https://zhuanlan.zhihu.com/p/374432534%EF%BC%8Chttps://zhuanlan.zhihu.com/p/309381344%EF%BC%8C%E5%90%8C%E6%97%B6%E5%AD%98%E5%9C%A8privacy">https://zhuanlan.zhihu.com/p/374432534ï¼Œhttps://zhuanlan.zhihu.com/p/309381344ï¼ŒåŒæ—¶å­˜åœ¨privacy</a> concernsï¼Œå› ä¸ºè®¡ç®—ç”¨åˆ°äº†å…¨å±€çš„æ•°æ®ã€‚</p><p>**static Batch Normaliztion (sBN)**ï¼š</p><ul><li>During the training phase, <strong>sBN</strong> does not track running estimates and <strong>simply normalize batch data</strong>.</li><li>We do not track the local running statistics as the size of local models may also vary dynamically. (ï¼Ÿ)</li><li>This method is suitable for HeteroFL as every communication round is independent. ï¼ˆï¼Ÿï¼‰</li><li>After the training process finishes, the server sequentially query local clients and cumulatively update global BN statistics. ï¼ˆï¼Ÿï¼‰</li><li>empirically found this trick s<strong>ignificantly outperforms other forms of normalization methods</strong> including the InstanceNorm (Ulyanov et al., 2016), GroupNorm (Wu &amp; He, 2018), and LayerNorm (Ba et al., 2016)</li></ul><h3 id="SCALER"><a href="#SCALER" class="headerlink" title="SCALER"></a>SCALER</h3><ul><li>å¾ˆæœ‰æ„æ€ï¼š</li><li>local model parameters at different computation complexity levels will digress to various scalesï¼š å±€éƒ¨æ¨¡å‹è‡ªå·±éƒ½æœ‰ä¸€äº›ä¸ªæ€§åŒ–çš„ç‰¹ç‚¹ï¼Œç›´æ¥ç”¨å…¨å±€æ¨¡å‹çš„å‚æ•°åœ¨æœ¬åœ°åšæ¨å¯¼å¯èƒ½å°±å¿½ç•¥äº†è¿™äº›ç‰¹ç‚¹ï¼š<ul><li><strong>To directly use the full model during the inference phase</strong>, inverted dropout with dropout rate q scales representations with $\frac{1}{1-q}$ during the training phaseã€‚drop outé€šå¸¸åœ¨æ¿€æ´»å‡½æ•°åé¢ï¼Œæˆ–æ˜¯åœ¨<strong>sBNå’Œæ¿€æ´»å±‚</strong>åŠ å…¥<strong>Scalar</strong>å±‚ï¼Œå°†representionæ”¾å¤§$\frac{1}{r^{p-1}}$ï¼Œè¿™æ ·çš„è¯å…¨å±€èšåˆä¹‹åï¼Œæœ¬åœ°å¯ä»¥ç›´æ¥ç”¨æœ¬åœ°çš„æ•°æ®å»åšinferenceï¼Œåšäº†<strong>æ¶ˆèå®éªŒ</strong>ã€‚</li><li><img src="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123-1668240923021-10.png" class="lazyload placeholder" data-srcset="/2022/11/12/iclr21-heterofl-computation-and-communication-efficient-federated-learning-for-heterogeneous-clients/123-1668240923021-10.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></li></ul></li></ul><h2 id="EXPERIMENTAL-RESULTS"><a href="#EXPERIMENTAL-RESULTS" class="headerlink" title="EXPERIMENTAL RESULTS"></a>EXPERIMENTAL RESULTS</h2><ul><li><p>600 individual models</p></li><li><p>datasets</p><ul><li>MNIST and CIFAR10 image classification tasks</li><li>WikiText2 language modeling task</li></ul></li><li><p>three different models</p><ul><li>CNN for MNIST</li><li>preactivated ResNet (PreResNet18) for CIFAR10</li><li>Transformer for WikiText2</li></ul></li><li><p>replace BN in CNN and PreResNet18 with our proposed sBN and attach the Scaler module after each convolution layer</p></li><li><p>data partition the same as in (McMahan et al., 2017; Liang et al., 2020).</p></li><li><p><strong>100</strong> clients, fraction <strong>C</strong> of active clients per communication round is <strong>0.1</strong> throughout our experiments</p><ul><li>For <strong>IID data partition</strong>, we uniformly assign the same number of data examples for each client</li><li>For <strong>balanced non-IID data partition</strong>, we assume that <strong>the label distribution is skewed</strong>, where clients will only have examples at most from two classes and the number of examples per class is balanced.</li><li>other kinds of non-IID data partition: the unbalanced non-IID data partition where clients may <strong>hold unbalanced labeled dataset and the feature distribution skew</strong> where clients may hold different features.</li><li><strong>masked language modeling task</strong> with a <strong>15% masking rate</strong> and <strong>assign balanced data examples for each client</strong>, each client will roughly have 3000 different words in their local dataset,</li></ul></li><li><p><strong>five different computation complexity levels</strong> {a, b, c, d, e} with the hidden channel shrinkage <strong>ratio r = 0.5</strong>, we found that it is most illustrative to use the discrete complexity levels <strong>0.5, 0.25, 0.125, and 0.0625</strong></p></li><li><p><strong>Each local client</strong> is assigned an <strong>initial computation complexity level</strong></p></li><li><p>To <strong>demonstrate the effect of dynamically varying computation and communication capabilities</strong>, we uniformly sample from various combinations of computation complexity levels</p></li></ul><h3 id="Masked-Cross-Entropy-Loss"><a href="#Masked-Cross-Entropy-Loss" class="headerlink" title="Masked Cross-Entropy Loss"></a>Masked Cross-Entropy Loss</h3><ul><li><strong>instead of</strong> a full Cross-Entropy Loss <strong>for all classes</strong>, we are motivated to <strong>train each local model only with their corresponding classes</strong>, each local model will train a <strong>sub-task</strong> given locally available label information</li><li><strong>Masked Cross-Entropy Loss</strong>: <strong>mask out the output of the model before passing it Cross-Entropy Loss</strong></li><li>We experimented with several different ways of masking, we find <strong>replacing the last layer outputs that are not associated with local labels with zero achieves both stable and comparable local and global results</strong></li><li>When aggregating local model parameters, we do not aggregate the untrained parameters in the last classification layers</li><li>Masked Cross-Entropy Loss <strong>significantly improve local performance and moderately global performance of balanced non-IID data partition task</strong></li></ul><h2 id="CONCLUSIONS-AND-FUTURE-WORK"><a href="#CONCLUSIONS-AND-FUTURE-WORK" class="headerlink" title="CONCLUSIONS AND FUTURE WORK"></a>CONCLUSIONS AND FUTURE WORK</h2>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobiCom20-Billion-Scale Federated Learning on Mobile Clients A Submodel Design with Tunable Privacy</title>
      <link href="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/"/>
      <url>/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/</url>
      
        <content type="html"><![CDATA[<h1 id="MobiCom20-Billion-Scale-Federated-Learning-on-Mobile-Clients-A-Submodel-Design-with-Tunable-Privacy"><a href="#MobiCom20-Billion-Scale-Federated-Learning-on-Mobile-Clients-A-Submodel-Design-with-Tunable-Privacy" class="headerlink" title="MobiCom20-Billion-Scale Federated Learning on Mobile Clients A Submodel Design with Tunable Privacy"></a>MobiCom20-Billion-Scale Federated Learning on Mobile Clients A Submodel Design with Tunable Privacy</h1><p>ä½œè€…ï¼š</p><ul><li><a href="https://niuchaoyue.github.io/%EF%BC%88Chaoyue">https://niuchaoyue.github.io/ï¼ˆChaoyue</a> Niuï¼‰ï¼ˆï¼‰ï¼ˆShanghai Jiao Tong Universityï¼‰</li><li><a href="https://www.cs.sjtu.edu.cn/~fwu/%EF%BC%88Fan">https://www.cs.sjtu.edu.cn/~fwu/ï¼ˆFan</a> Wuï¼‰ï¼ˆå´å¸†ï¼‰ï¼ˆShanghai Jiao Tong Universityï¼‰</li><li>etc</li></ul><p><img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112220418688.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112220418688.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221112220418688"></p><ul><li>é’ˆå¯¹è¿™æ ·çš„æ·˜å®æ¨èæ¨¡å‹åšçš„ä¸€ä¸ªFLçš„æ”¹ç¼–ã€‚</li><li>æ·˜å®çš„å•†å“å¤ªå¤šï¼Œæœ¬åœ°ç”¨æˆ·æƒ³çŸ¥é“è‡ªå·±æœ‰å…³çš„å•†å“çš„è¯åµŒå…¥ï¼Œè‚¯å®šä¸èƒ½ä¸€æ¬¡æ€§æŠŠå‡ åäº¿çš„å•†å“idå…¨éƒ¨æ”¾åˆ°è‡ªå·±çš„æœ¬åœ°ã€‚</li><li>ç”¨æˆ·ç›´æ¥å‘serverè¦å’Œè‡ªå·±ç›¸å…³çš„å•†å“çš„idä¼šæ³„éœ²éšç§ã€‚æœ¬ç¯‡æ–‡ç« å°±æ˜¯åœ¨æ¢è®¨<strong>å¦‚ä½•ä¸æ³„éœ²ç”¨æˆ·çš„éšç§</strong>ã€‚</li></ul><h2 id="Motivitation"><a href="#Motivitation" class="headerlink" title="Motivitation"></a>Motivitation</h2><ul><li>å½“å‰çš„è”é‚¦å­¦ä¹ å­˜åœ¨ä¸€äº›<strong>é—®é¢˜</strong>ï¼šè¦æ±‚å®¢æˆ·ç«¯å……åˆ†çš„<strong>åˆ©ç”¨æ•´ä¸ªæ¨¡å‹</strong>å»è®­ç»ƒï¼Œåœ¨<strong>å¤§è§„æ¨¡å­¦ä¹ ä»»åŠ¡å’Œèµ„æºå—é™çš„æ‰‹æœºè®¾å¤‡ä¸­</strong>æ•ˆç‡æä½ã€‚</li><li>æ·±åº¦å­¦ä¹ çš„è¾“å…¥ååˆ†ç¨€ç–ï¼Œå› æ­¤å¸¸å¸¸ç”¨ä¸€ä¸ª<strong>åµŒå…¥å±‚</strong>(embedding layer)ï¼Œå»æŠŠè¾“å…¥å˜åˆ°ä½ç»´ï¼Œä½¿å¾—ç›¸ä¼¼çš„è¾“å…¥è¾ƒä¸ºæ¥è¿‘ï¼Œä¸”ä¿ç•™è¾“å…¥çš„ä¿¡æ¯è¾ƒå¤šï¼ˆæ·˜å®çš„è¾“å…¥ä¿ç•™äº†$98.22%$çš„ä¿¡æ¯ï¼ŒGoogleçš„å®‰å“é”®ç›˜Gboardä¿ç•™äº†è¶…è¿‡$2/3$ï¼‰çš„ä¿¡æ¯ã€‚æ·˜å®æœ‰20äº¿çš„å•†å“ï¼Œæ¯”Googleçš„10000è¯å¤šå¤šäº†ï¼Œç»™æ‰€æœ‰å•†å“IDçš„è¯åµŒå…¥éœ€è¦20äº¿è¡Œï¼Œ134Gç©ºé—´ï¼ˆåµŒå…¥å‘é‡çš„ç»´åº¦ä¸º18ï¼‰ï¼Œ<strong>æ¯ä¸€ä¸ªclientä¸å¯èƒ½ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹</strong>ï¼Œæ³¨æ„åˆ°<strong>æ¯ä¸€ä¸ªclientå­éœ€è¦è¾“å…¥çš„å¾ˆå°çš„ç‰¹å¾ç©ºé—´ï¼ˆä¾‹å¦‚ä¸€ä¸ªç”¨æˆ·å¯èƒ½åªæœ‰300ä¸ªå•†å“çš„æµè§ˆè®°å½•ï¼‰</strong>ï¼Œå› æ­¤å¯ä»¥åœ¨clientä¸Šåªè¦ç€ä¸€ç‚¹ç‚¹çš„ç‰¹å¾ç©ºé—´ã€‚</li></ul><h3 id="Submodel"><a href="#Submodel" class="headerlink" title="Submodel"></a>Submodel</h3><p>æå‡º<strong>å­æ¨¡å‹æ¡†æ¶</strong>ï¼Œå®¢æˆ·ç«¯åªä»serverä¸‹è½½éœ€è¦çš„éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯å­æ¨¡å‹ï¼Œå¹¶ä¸”<strong>åªä¸Šä¼ å­æ¨¡å‹ï¼ˆ$1.99%$ï¼‰çš„æ›´æ–°</strong>ã€‚</p><p>ä¸Šä¼ å­æ¨¡å‹çš„æ›´æ–°ä¼šå¸¦æ¥æ–°çš„é—®é¢˜ï¼šå®¢æˆ·ç«¯çœŸå®éœ€è¦çš„<strong>å­æ¨¡å‹æš´éœ²äº†ç”¨æˆ·çš„éšç§æ•°æ®</strong>ï¼Œè¿èƒŒFLåˆè¡·ã€‚é¦–å…ˆï¼Œclientä¸‹è½½å’Œä¸Šä¼ å­æ¨¡å‹çš„æ—¶å€™éœ€è¦åˆ¶å®šä¸€ä¸ªç´¢å¼•é›†ï¼ˆindex setï¼‰ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼Œç´¢å¼•é›†æŒ‡ç¤ºçš„æ¨¡å‹çš„å‚æ•°éƒ¨åˆ†å¯èƒ½å°±æ˜¯è¿™ä¸ªç”¨æˆ·çš„è¾“å…¥æˆ–è€…ä¸€äº›å…¶å®ƒéšå«ç”¨æˆ·ä¿¡æ¯çš„è¯å‘é‡ã€‚å…¶æ¬¡ï¼Œæœ‰äº†è¿™ä¸ªç´¢å¼•ä¹‹ååç»­çš„æ›´æ–°ä¹Ÿä¼šè¢«æœåŠ¡å™¨çŸ¥é“ï¼Œå¯ä»¥æ ¹æ®è¿™ä¸ªæ›´æ–°å’Œæ¨¡å‹çš„å‚æ•°é‡å»ºå‡ºç”¨æˆ·çš„éšç§æ•°æ®ã€‚å…¶æ¬¡ï¼Œ<strong>æ·˜å®æ¯ä¸€ä¸ªç”¨æˆ·çš„å­æ¨¡å‹ä¹Ÿå¾ˆéš¾å’Œå…¶å®ƒç”¨æˆ·çš„å¯¹é½</strong>ã€‚</p><p>å› æ­¤è®¾è®¡ä¸€ä¸ª<strong>å®‰å…¨è”é‚¦å­æ¨¡å‹å­¦ä¹ ç­–ç•¥ï¼Œé™„å¸¦ä¸€ä¸ªéšç§é›†è”åˆä»£ç†ä½œä¸ºåŸºçŸ³</strong>ã€‚(a secure federated submodel learning scheme coupled with a private set union protocol as a cornerstone)ã€‚è¿™ä¸ªç­–ç•¥çš„å±æ€§ï¼š<strong>éšæœºå“åº”ï¼Œå®‰å…¨èšåˆï¼Œå¸ƒé²å§†è¿‡æ»¤å™¨ï¼Œå®šåˆ¶çš„å¯ä¿¡å¦è®¤</strong>ï¼ˆrandomized response, secure aggregation, and Bloom filter, and endows each client with customized plausible deniability (in terms of local differential privacy) against the position of its desired submodelï¼‰</p><h2 id="è§£å†³å­æ¨¡å‹æš´éœ²ç”¨æˆ·éšç§çš„é—®é¢˜"><a href="#è§£å†³å­æ¨¡å‹æš´éœ²ç”¨æˆ·éšç§çš„é—®é¢˜" class="headerlink" title="è§£å†³å­æ¨¡å‹æš´éœ²ç”¨æˆ·éšç§çš„é—®é¢˜"></a>è§£å†³å­æ¨¡å‹æš´éœ²ç”¨æˆ·éšç§çš„é—®é¢˜</h2><ol><li><p><strong>å®¢æˆ·ç«¯å¦‚ä½•ä¸‹è½½çŸ©é˜µçš„ä¸€äº›è¡Œè€Œä¸éœ€è¦å‘Šè¯‰æœåŠ¡å™¨æˆ‘éœ€è¦å“ªä¸€è¡Œå’Œå“ªä¸€ä¸ªè¡Œçš„ç´¢å¼•ã€‚</strong></p><ul><li>å®¢æˆ·ç«¯ç›´æ¥ä¸‹è½½æœåŠ¡å™¨å®Œæ•´çš„æ¨¡å‹ï¼Œæœ¬åœ°æŠŠè‡ªå·±éœ€è¦çš„æ¨¡å‹æ‹‰å‡ºæ¥ï¼ˆâŒï¼‰ã€‚</li><li><strong>ä¸èƒ½ä¸‹è½½å®Œæ•´çš„æ¨¡å‹</strong>ï¼Œprivate information retrievalï¼ˆPIRï¼Œåªè¯»æ¨¡å¼ï¼Œè·å–åˆ°çš„æ•°æ®çš„éšè—(concealment of the retrieved elements)ï¼‰</li></ul></li><li><p><strong>å®¢æˆ·ç«¯å¦‚ä½•ä¿®æ”¹çŸ©é˜µçš„ä¸€äº›è¡Œè€Œä¸è®©æœåŠ¡å™¨çŸ¥é“å“ªä¸€è¡Œè¢«ä¿®æ”¹äº†æˆ–è€…æ›¿æ¢äº†ã€‚</strong></p><ul><li>å®¢æˆ·ç«¯è¦æ˜¯ç›´æ¥å»ä¿®æ”¹æ¨¡å‹ä¸Šçš„è¡Œå’Œåˆ—ï¼Œé‚£ä¹ˆæœåŠ¡å™¨è‚¯å®šçŸ¥é“å®ƒæ”¹äº†ä»€ä¹ˆï¼ˆâŒï¼‰ã€‚</li><li><strong>é¦–å…ˆ</strong>è¿›è¡Œå®‰å…¨èšåˆï¼Œå³å…ˆæŠŠæ‰€æœ‰çš„ä¿®æ”¹ç›¸åŠ ç»„æˆæ¨¡å‹ï¼Œ<strong>ç„¶å</strong>ä½¿ç”¨èšåˆä¿®æ”¹ï¼Œå³æŠŠæ‰€æœ‰ä¿®æ”¹ç›¸åŠ ç»„æˆçš„æ¨¡å‹åº”ç”¨åˆ°æ•´ä¸ªæ¨¡å‹ä¸Šï¼Œè¿™æ ·å°±ä¸çŸ¥é“è°æ”¹äº†ä»€ä¹ˆã€‚å¯ç”¨çš„åŠ å¯†ç­–ç•¥ï¼šå¾ˆå¤šï¼Œä¾‹å¦‚ï¼Œthe protocol specific to the FL setting in [8] and additively homomorphic encryption [9, 43]ã€‚è¿™æ ·çš„è¯ï¼Œåªè¦æ¯ä¸€ä¸ªå®¢æˆ·ç«¯è‡³å°‘ä¿®æ”¹äº†ä¸€ä¸ªå®ƒå¯¹åº”å‘é‡çš„æ•°å€¼ï¼Œé‚£ä¹ˆå°±æ— æ³•çŸ¥é“è°æ”¹äº†ä»€ä¹ˆä¸œè¥¿ã€‚æœ‰ä¸€ä¸ªæç«¯çš„ç­–ç•¥å«secure federated learning(SFL)ï¼Œå¼ºåˆ¶æ¯ä¸€è½®æ­¤æ‰€æœ‰è¢«é€‰æ‹©çš„å®¢æˆ·ç«¯éƒ½å¿…é¡»å‚ä¸ä¿®æ”¹æ— è®ºä»–ä»¬æ˜¯ä¸æ˜¯çœŸçš„æƒ³è¦ä¿®æ”¹ï¼Œéšç§ä¿æŠ¤æœ€å¥½ï¼Œå¦ä¸€ä¸ªæç«¯çš„ç­–ç•¥æ˜¯åªè®©çœŸçš„æƒ³è¦æ”¹ä»–ä»¬å¯¹åº”è¡Œçš„å®¢æˆ·ç«¯å‚ä¸ä¿®æ”¹ï¼Œæ•ˆç‡æœ€é«˜ã€‚<strong>ç„¶è€Œ</strong>ï¼Œä¸åŒçš„å®¢æˆ·ç«¯å€¾å‘äºä¿®æ”¹<strong>é«˜åŒºåˆ†åº¦ç”šè‡³æ˜¯äº’æ–¥çš„è¡Œ(highly differentiated or even mutually exclusive rows)<strong>ï¼Œå¯¹äºå¤šä¸ªå®¢æˆ·ç«¯ä¸€èµ·ä¿®æ”¹ä¸€äº›è¡Œçš„æƒ…å†µï¼Œåªæœ‰ä¸€ä¸ªå®¢æˆ·ç«¯å‚ä¸çš„æ¦‚ç‡æ˜¯å¾ˆé«˜çš„ï¼Œè¿™ç§æ¡ä»¶ä¸‹å®‰å…¨èšåˆå°±</strong>å¤±å»äº†ä»–çš„ä½œç”¨</strong>ã€‚</li></ul></li><li><p>secure federated submodel learning (SFSL)ã€‚æœ¬æ–‡æå‡ºçš„æ¨¡å‹ï¼š</p><p> é¦–å…ˆç¡®å®šäº†æ¯ä¸€ä¸ªè½®æ¬¡è”åˆä¿®æ”¹çš„èŒƒå›´ï¼Œè¿™æ ·å¯ä»¥å¯¹å…¶åˆ†åŒ–çš„å­æ¨¡å‹ã€‚å…³é”®çš„å›°éš¾åœ¨äºï¼Œéšè—å®¢æˆ·ç«¯æƒ³è¦ä¿®æ”¹çš„å­æ¨¡å‹çš„ä½ç½®ã€‚åŸæœ¬çš„SFLä½¿ç”¨å·¨å¤§çš„ç´¢å¼•é›†ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼ŒSFSLç¡®å®šäº†éœ€è¦çš„å¯¹é½èŒƒå›´ï¼Œå³å®¢æˆ·ç«¯ä»¬çš„çœŸå®ç´¢å¼•é›†çš„é›†åˆï¼Œprivate set unionï¼ˆPSUï¼‰ã€‚</p><p> æ¯ä¸€ä¸ªè¢«é€‰æ‹©çš„å®¢æˆ·ç«¯<strong>ç”Ÿæˆéšæœºç´¢å¼•é›†</strong>å»æ›¿æ¢å’Œä¿æŠ¤è‡ªå·±çœŸå®çš„ç´¢å¼•é›†ï¼Œéšæœºç´¢å¼•é›†æ˜¯åº”ç”¨ä¸¤æ¬¡éšæœºåŒ–ç›¸åº”(random response)ç”Ÿæˆï¼Œ<strong>éšæœºåŒ–å“åº”çš„å‚æ•°</strong>è®¾ç½®<strong>ç”±å®¢æˆ·ç«¯åˆ¶å®š</strong>ï¼Œä½¿ç”¨local differential privacy(LDP)å»ä¸¥æ ¼é‡åŒ–deniabilityçš„å¼ºåº¦ï¼ŒæœåŠ¡å™¨å¯ä»¥ä»èšé›†åçš„ä¿®æ”¹ä¸­æ¨æ–­åˆå®¢æˆ·ç«¯çš„çœŸå®æ„å›¾çš„æ¦‚ç‡ä¹Ÿå¯ä»¥ç®—å‡ºæ¥ï¼Œéšæœºç´¢å¼•çš„åŸºæ•°(cardinality)å†³å®šäº†å®¢æˆ·ç«¯çš„å¼€é”€ï¼Œå®¢æˆ·ç«¯çš„çœŸå®å’Œéšæœºç´¢å¼•é›†çš„äº¤å‰æ§åˆ¶äº†å…¶æœ¬åœ°è®­ç»ƒçš„å¼€é”€ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯éƒ½èƒ½å¤Ÿå¾®è°ƒéšç§çš„ä¿æŠ¤ç¨‹åº¦å’Œæ•ˆç‡ã€‚</p></li></ol><h2 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><h2 id="å‡†å¤‡å·¥ä½œ"><a href="#å‡†å¤‡å·¥ä½œ" class="headerlink" title="å‡†å¤‡å·¥ä½œ"></a>å‡†å¤‡å·¥ä½œ</h2><h3 id="å®‰å…¨éœ€è¦"><a href="#å®‰å…¨éœ€è¦" class="headerlink" title="å®‰å…¨éœ€è¦"></a>å®‰å…¨éœ€è¦</h3><p>each client should have plausible deniability of whether a certain index is or is not in its real index set.  <strong>the strength of plausible deniability</strong>, we adopt <strong>LDP</strong>ï¼ˆLocal DPï¼Œæœ¬åœ°çš„å·®åˆ†éšç§ï¼‰ã€‚ä¸ä»…å¯ä»¥ä¿æŠ¤external attackersï¼Œä¹Ÿå¯ä»¥ä¿æŠ¤ä¸å¯ä¿¡çš„data curatorã€‚</p><ol><li><p><strong>LDPçš„å®šä¹‰</strong>ï¼š</p><p> <img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221110221433864.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221110221433864.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221110221433864"></p></li></ol><p>æ¥è‡ªå®¢æˆ·ç«¯çš„è¾“å…¥å˜äº†ï¼Œè¾“å‡ºä¸ä¼šå˜åŒ–å¤ªå¤šã€‚</p><ol start="2"><li>**å…è®¸å®¢æˆ·ç«¯è‡ªå®šä¹‰è‡ªå·±çš„éšç§ç­‰çº§ã€‚</li></ol><p><img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221110233043387.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221110233043387.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221110233043387"></p><ol start="3"><li><p><strong>éšæœºResponse</strong></p><p> å‡è®¾æˆ‘ä»¬æƒ³è¦è°ƒç ”ä¸€ä¸ªæ•æ„Ÿé—®é¢˜ï¼Œå¦‚è°ƒæŸ¥å·²å©šäººç¾¤ä¸­çš„å‡ºè½¨æ¯”ç‡ï¼Œé‚£ä¹ˆè®©æ¯ä¸€ä¸ªè¢«è°ƒç ”è€…å¦‚å®å›ç­”é—®é¢˜å¿…ç„¶å¯¼è‡´ä¸ªäººéšç§è¢«ä¾µçŠ¯ã€‚ä½†æ˜¯æˆ‘ä»¬æƒ³è¦è·å–çš„æ˜¯ç»Ÿè®¡ä¿¡æ¯ï¼Œè€Œéæ¯ä¸€ä¸ªä¸ªä½“çš„ä¿¡æ¯ï¼Œå› æ­¤å¯ä»¥æ„å»ºå¦‚ä¸‹éšæœºå›ç­”ç®—æ³•ï¼šä»¤å—è®¿è€…è‡ªå·±æŠ›ä¸€æšå‡åŒ€ç¡¬å¸ï¼Œå¦‚æœæ­£é¢æœä¸Šï¼Œé‚£ä¹ˆå¦‚å®å›ç­”é—®é¢˜ï¼Œå¦‚æœèƒŒé¢æœä¸Šï¼Œé‚£ä¹ˆå†æŠ›ä¸€æšç¡¬å¸ï¼Œæ­£é¢æœä¸Šå›ç­”æ˜¯ï¼ŒèƒŒé¢æœä¸Šå›ç­”å¦ã€‚è¿™æ ·ï¼Œå¯¹äºå—è®¿è€…è€Œè¨€ï¼Œæ— è®ºä»–å›ç­”çš„ç»“æœå¦‚ä½•éƒ½ä¸ä¼šä¾µçŠ¯éšç§ï¼Œå› ä¸ºå¯¹äºä¸€ä¸ªäººè€Œè¨€è‡³å°‘æœ‰å››åˆ†ä¹‹ä¸€çš„æ¦‚ç‡ä¼šå›ç­”â€œæœ‰å‡ºè½¨â€ï¼Œå› æ­¤ä»–çš„å›ç­”å¹¶ä¸ä¼šé€éœ²çœŸå®çš„æƒ…å†µã€‚è€Œå¯¹äºç ”ç©¶è€…è€Œè¨€ï¼Œå‡ºè½¨æ¯”ä¾‹på¯ä»¥é€šè¿‡ç®€å•çš„è®¡ç®—å¾—åˆ°ï¼š</p></li></ol><p><img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221110233329380.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221110233329380.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221110233329380"></p><p><img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221110233823708.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221110233823708.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221110233823708"></p><h2 id="DESIGN-OF-SFSL"><a href="#DESIGN-OF-SFSL" class="headerlink" title="DESIGN OF SFSL"></a>DESIGN OF SFSL</h2><h3 id="Design-Rationale"><a href="#Design-Rationale" class="headerlink" title="Design Rationale"></a>Design Rationale</h3><p><strong>key design principles</strong> through how to handle the <strong>two fundamental problems</strong> raised in Section 1.4 and how to resolve <strong>several practical issues</strong></p><h4 id="Solve-two-fundamental-problems"><a href="#Solve-two-fundamental-problems" class="headerlink" title="Solve two fundamental problems"></a>Solve two fundamental problems</h4><p><strong>two fundamental problems</strong> :</p><ul><li>How a client can <strong>download</strong> a row of a matrix, which represents the global/full model and is maintained by an untrusted cloud server, <strong>without revealing which row or the row index to the cloud server</strong>;</li><li>how a client can <strong>modify</strong> a row of the matrix, still <strong>without revealing which row was modified and the altered content</strong>. </li></ul><p>Design:</p><ul><li>During the <strong>download and upload phases</strong>, a client consistently <strong>uses a randomized index set in place of its real index set</strong></li><li>during the <strong>local training phase</strong>, the client <strong>leverages the intersection of its real and randomized index sets</strong>, the client holds <strong>plausible deniability</strong> of some index being in or not in its real index set.</li></ul><p>examine the feasibility of index set randomization in handling two problems:</p><ul><li><strong>For the first problem</strong> in the download phase, if a client intends (resp., does not intend) to download a certain row, and it actually downloads (resp., does not download), it can blame its action to randomization</li><li><strong>Regarding the second problem</strong> in the upload phase, the usage of the randomized index set still empowers a client to deny its real intention of modifying or not modifying some row, even if the cloud server observes its binary action of modifying or not modifying</li></ul><p>for a concrete row, there are two different groups of clients involved in the joint modification:</p><ul><li>(1) One group consists of <strong>those clients who intend to modify the row and contribute nonzero modifications</strong>;</li><li>(2) the other group comprises <strong>those clients who do not intend to modify the row and pretend to modify by submitting zero modifications</strong>. </li><li><strong>With the secure aggregation guarantee</strong>(???)ï¼Œit is <strong>hard to identify any individual modification</strong> and further to infer whether some client originally intends to perform a modification or not</li><li>The <strong>hardness</strong> is controlled by <strong>the sizes of two groups</strong>ï¼š<strong>the probabilities of an index in and not in the real index set</strong>.  <strong>Tunable</strong></li></ul><h4 id="two-practical-issues"><a href="#two-practical-issues" class="headerlink" title="two practical issues"></a>two practical issues</h4><ul><li>The first issue regards <strong>whether it is practical and necessary for the cloud server to ask â€œDo you have a certain index?â€ for each index in the full index set</strong>? <strong>Impractical</strong>  and <strong>Unnecessary</strong> . <ul><li>We turn to <strong>narrowing down the scope to the union of ğ‘› chosen clientsâ€™ real index sets</strong>, which is normally far smaller than the full index set (i.e., <strong>the union of the whole clientsâ€™ real index sets</strong>), it is <strong>unnecessary to cover any index outside the union of ğ‘› chosen clientsâ€™ real index sets</strong>. (è¿™æ ·ä¼šä¸ä¼šå¯¼è‡´ä¸€äº›å•†å“æ°¸è¿œæ— æ³•è¢«ç”¨æˆ·æ‰€æ„ŸçŸ¥åˆ°ï¼Ÿ)</li><li>how multiple clients can obtain the union of their real index sets under the mediation of an untrusted cloud server without revealing any clientâ€™s real index set(Prosecution Services Unit, <strong>PSU</strong>):<ul><li><strong>a novel design of PSU</strong> based on <strong>Bloom filter</strong> and <strong>secure aggregation</strong></li></ul></li></ul></li><li>The second issue regards <strong>the longitudinal privacy</strong> when a client is chosen to participate in multiple rounds of FSL<ul><li>we need to extend the initial version to <strong>allow repeated responses from the same client to those already answered indices</strong>. (key principles from randomized aggregatable privacypreserving ordinal response (RAPPOR) [19, 21]): the noisy answers generated by the inner (permanent) randomized response will <strong>be memoized and permanently replace the real answers</strong> in the outer (instantaneous) randomized response</li></ul></li></ul><h3 id="Design-Details"><a href="#Design-Details" class="headerlink" title="Design Details"></a>Design Details</h3><h4 id="Secure-Federated-Submodel-Learning-SFSL"><a href="#Secure-Federated-Submodel-Learning-SFSL" class="headerlink" title="Secure Federated Submodel Learning (SFSL)"></a>Secure Federated Submodel Learning (SFSL)</h4><p><img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112212046250.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112212046250.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221112212046250"></p><ul><li>At the initial stage, the cloud server randomly initializes the global model (Line 1). In each communication round, the cloud server selects ğ‘› clients to participate (Lines 2 and 3) and also maintains an up-to-date set of the chosen clients who are alive throughout the whole round, denoted by $\hat{C}$. A chosen client determines its real index set based on its local data, which specifies the â€œpositionâ€ of its truly required submodel (Line 10)ã€‚</li><li>Then, the cloud server launches PSU to obtain the union of the chosen clientsâ€™ real index sets while keeping each individual clientâ€™s real index set in secret (Lines 4 and 11)ã€‚</li><li>The union is delivered to live clients for them to generate randomized index sets with customized LDP guarantees against the cloud server (Line 12ï¼‰</li><li>Each live client will use its randomized index set, rather than its real index set, to download the submodel and to securely upload the submodel update (Lines 13 and 19)</li><li>Upon receiving the randomized index set from a client, the cloud server stores it for later usage and returns the corresponding submodel and training hyperparameters to the client (Line 6)</li><li>the client extracts a succinct submodel and prepares involved data as the succinct training set (Line 14).  For example, if a Taobao userâ€™s real index set is {1, 2, 4}, and his/her randomized index set is {2, 4, 6, 9}, he/she receives a submodel with the row indices {2, 4, 6, 9} from the cloud server, but just needs to train the succinct submodel with the row indices {2, 4} over his/her local data <strong>involving the goods IDs {2, 4}</strong>,  <strong>(why???)</strong></li><li>After training under the preset hyperparameters, the client obtains the update of the succinct submodel (Line 15)</li><li>it prepares the submodel update to be uploaded with the randomized index set by adding the update of the succinct submodel to the rows with the succinct indices and padding zero vectors to the other rows (Line 16)</li><li>To help the cloud server average multiple submodel updates according to the size of relevant local training data, <strong>each chosen client also needs to count the number of its training samples involving every index</strong> in the randomized index set (Line 17). the numbers of samples involving the indices outside the succinct index set are all zeros.</li></ul><h4 id="Index-Set-Randomization"><a href="#Index-Set-Randomization" class="headerlink" title="Index Set Randomization"></a>Index Set Randomization</h4><p>how a client can generate a randomized index set in each participating round</p><ul><li>basic design</li></ul><p><img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112214156196.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112214156196.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221112214156196"></p><ul><li>let client ğ‘– maintain two index sets with â€œYesâ€ and â€œNoâ€ answers in the permanent randomized response</li></ul><ul><li>Client ğ‘–â€™s Index Set Randomization</li></ul><p><img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112214520078.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112214520078.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221112214520078"></p><ul><li>ä¸»è¦è§£å†³ éšæœºç”Ÿæˆçš„indexè·Ÿéšè½®æ¬¡å˜åŒ–çš„é—®é¢˜ï¼Œå¸Œæœ›è·Ÿéšè½®æ¬¡çš„å˜åŒ–ï¼Œç”¨æˆ·è¦çš„indexä¹Ÿä¸è¦è¢«çŒœå‡ºæ¥ã€‚ç»†èŠ‚è§åŸæ–‡ã€‚</li></ul><h4 id="Private-Set-Union-PSU"><a href="#Private-Set-Union-PSU" class="headerlink" title="Private Set Union (PSU)"></a>Private Set Union (PSU)</h4><p><img src="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112214740791.png" class="lazyload placeholder" data-srcset="/2022/11/12/mobicom20-billion-scale-federated-learning-on-mobile-clients-a-submodel-design-with-tunable-privacy/image-20221112214740791.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221112214740791"></p><h2 id="THEORETICAL-ANALYSIS"><a href="#THEORETICAL-ANALYSIS" class="headerlink" title="THEORETICAL ANALYSIS"></a>THEORETICAL ANALYSIS</h2><h3 id="Security-and-Privacy-Analyses"><a href="#Security-and-Privacy-Analyses" class="headerlink" title="Security and Privacy Analyses"></a>Security and Privacy Analyses</h3><h3 id="Complexity-Analysis-and-Comparison"><a href="#Complexity-Analysis-and-Comparison" class="headerlink" title="Complexity Analysis and Comparison"></a>Complexity Analysis and Comparison</h3><h2 id="EVALUATION"><a href="#EVALUATION" class="headerlink" title="EVALUATION"></a>EVALUATION</h2><ul><li>Dataset</li><li>Model, Hyperparameters, and Metric</li><li>Prototypes and Configurations</li></ul><h3 id="Model-Accuracy-and-Convergency"><a href="#Model-Accuracy-and-Convergency" class="headerlink" title="Model Accuracy and Convergency"></a>Model Accuracy and Convergency</h3><h3 id="Communication-Overhead"><a href="#Communication-Overhead" class="headerlink" title="Communication Overhead"></a>Communication Overhead</h3><h3 id="Computation-Overhead"><a href="#Computation-Overhead" class="headerlink" title="Computation Overhead"></a>Computation Overhead</h3><h3 id="Memory-and-Disk-Loads"><a href="#Memory-and-Disk-Loads" class="headerlink" title="Memory and Disk Loads"></a>Memory and Disk Loads</h3><h3 id="Discussion-on-Billion-Scale-Issues"><a href="#Discussion-on-Billion-Scale-Issues" class="headerlink" title="Discussion on Billion-Scale Issues"></a>Discussion on Billion-Scale Issues</h3><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OSDI21-Oort Efficient Federated Learning via Guided Participant Selection</title>
      <link href="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/"/>
      <url>/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/</url>
      
        <content type="html"><![CDATA[<ul><li>Oortä¼˜åŒ–äº†ä¸­å¤®æœåŠ¡å™¨é€‰æ‹©clientçš„ç­–ç•¥ï¼Œä¸å†ç”¨randomé€‰æ‹©çš„ç­–ç•¥ï¼Œè€Œæ˜¯ç»™æ¯ä¸€ä¸ªclientéƒ½è®¾ç½®ä¸€ä¸ªåå«utilityçš„ä¸œè¥¿ï¼Œutilityæ˜¯æ ¹æ®clientçš„å†å²ä¸Šæ ¹æ®å¯¹modelçš„æ›´æ–°ç®—å‡ºæ¥çš„.</li><li>ä¼˜åŒ–äº†statistical model efficiencyå’Œsystem efficiency</li><li>åŒæ—¶åšå‡ºäº†ä¸€ä¸ªå¯ä»¥è®©å¼€å‘è€…é€‰æ‹©è‡ªå·±æƒ³è¦çš„æ•°æ®çš„ç³»ç»Ÿæ–¹ä¾¿å¼€å‘è€…å»åšæµ‹è¯•ã€‚</li></ul><p>ä½œè€…æ˜¯æ€ä¹ˆåœ¨é‚£ä¹ˆå¤šä¸Šä½¿ç”¨ImageNetæ•°æ®é›†è·‘MobileNetæ¨¡å‹çš„ï¼Ÿæ¯ä¸ªclienræ˜¯æ€ä¹ˆç¡®å®šè‡ªå·±ç”¨ä»€ä¹ˆæ ·çš„æ•°æ®çš„ï¼Ÿ</p><p><a href="https://zhuanlan.zhihu.com/p/507487766">ä¸€ç§æ–°å‹çš„è”é‚¦åˆ†å¸ƒå¼æ¶æ„ï¼ˆOortï¼‰åŠæœªæ¥ç ”ç©¶æ–¹å‘ - çŸ¥ä¹ (zhihu.com)</a></p><h1 id="OSDI21-Oort-Efficient-Federated-Learning-via-Guided-Participant-Selection"><a href="#OSDI21-Oort-Efficient-Federated-Learning-via-Guided-Participant-Selection" class="headerlink" title="OSDI21-Oort Efficient Federated Learning via Guided Participant Selection"></a>OSDI21-Oort Efficient Federated Learning via Guided Participant Selection</h1><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul><li><strong>Target</strong>: improve the performance of federated training and testing with guided participant selection<ul><li>prioritizes the use of those clients who have both data that offers the greatest utility in improving model accuracy and the capability to run training quickly</li><li>To enable FL developers to interpret their results in model testing, Oort enforces their requirements on the distribution of participant data while improving the duration of federated testing by cherry-picking clients</li></ul></li><li><strong>Result</strong>: time-to-accuracy performance by 1.2Ã—-14.1Ã— and final model accuracy by 1.3%-9.8%, while efficiently enforcing developer-specified model testing criteria at the scale of millions of clients.</li><li>æ–¹æ³•ï¼šé€šè¿‡é€‰æ‹©ä¸€ä¸ªclientçš„å­é›†ï¼ˆé€šè¿‡clientæœ€è¿‘çš„losså»é€‰æ‹©ï¼Œæƒ©ç½šä¼šå¯¼è‡´å…¨å±€èšåˆæ—¶é—´å¢åŠ çš„clientï¼‰å»ä¼˜åŒ–statistical model efficiencyå’Œsystem efficiency<ul><li>é€‰æ‹©clientçš„æ–¹æ³•ç”¨çš„æ˜¯FLå·²ç»ç»™ä½ æä¾›çš„ä¿¡æ¯ï¼Œå¹¶ä¸”é™åˆ¶å‚ä¸è€…çš„æ•°é‡ã€‚ï¼ˆå¯ä»¥é€šè¿‡è°ƒå‚å…¼é¡¾æ¨¡å‹çš„å…¬å¹³æ€§å’Œä¿æŠ¤éšç§çš„è¦æ±‚ï¼‰ã€‚</li><li>å¼€å‘è€…å¯ä»¥è‡ªå®šä¹‰æµ‹è¯•æ•°æ®é›†çš„åˆ†å¸ƒå»debug model efficiencyã€‚</li><li>PySyftä¸Šå®ç°ï¼ŒçœŸå®çš„workloadsä¸Šæµ‹è¯•ã€‚</li></ul></li></ul><h2 id="Background-and-Motivation"><a href="#Background-and-Motivation" class="headerlink" title="Background and Motivation"></a>Background and Motivation</h2><h3 id="FL"><a href="#FL" class="headerlink" title="FL"></a>FL</h3><p>è”é‚¦å­¦ä¹ é€šå¸¸æ¯ä¸€è½®å‡ ç™¾å°è®¾å¤‡ï¼ŒèŠ±å¥½å‡ å¤©è®­ç»ƒå®Œæˆã€‚ï¼ˆä¸ºä»€ä¹ˆä¸æŒ‘é€‰å‡ºäººæ•°æœ€å¤šçš„åˆ†å¸ƒå»åšè®­ç»ƒï¼Ÿç„¶ååœ¨è®¾å¤‡ä¸Šåšfinetuneï¼‰</p><p>å¼€å‘è€…æµ‹è¯•çš„æ—¶å€™æœ‰æŒ‘é€‰å‡ºä¸€éƒ¨åˆ†æ•°æ®é›†åšè®­ç»ƒçš„éœ€æ±‚ï¼š</p><ul><li>â€œ50k representative samplesâ€</li><li>â€œx samples of class yâ€</li><li>â€œa subset with less than X% data deviation from the globalâ€</li></ul><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><ul><li>æ•°æ®å¼‚æ„æ€§ï¼šå±•ç¤ºäº†ä¸€äº›æ•°æ®é›†ä¸Šä¸åŒclientsä¹‹é—´æ•°æ®è§„æ¨¡å’Œpairwise data divergenceçš„CDFå›¾ã€‚</li><li>è®¾å¤‡å¼‚æ„æ€§ï¼šç”¨mobilenetåœ¨å‡ ç™¾ä¸ªè®¾å¤‡ä¸Šè·‘ï¼Œè®¡ç®—äº†<strong>æ¨å¯¼å»¶è¿Ÿ</strong>å’Œ<strong>ç½‘ç»œçš„ååç‡</strong>ç”»äº†ä¸¤å¼ å›¾ã€‚</li><li>Enormous population and pervasive uncertainty</li><li>Privacy concerns</li></ul><h3 id="Limitations-of-Existing-FL-Solutions"><a href="#Limitations-of-Existing-FL-Solutions" class="headerlink" title="Limitations of Existing FL Solutions"></a>Limitations of Existing FL Solutions</h3><ul><li>the potential for curbing these disadvantages by cherry-picking participants before execution has largely been overlooked</li><li>Suboptimality in maximizing efficiencyï¼š ç”¨MobileNetå’ŒShuffleNetåœ¨1.6 millionçš„OpenImageæ•°æ®é›†ä¸Šåšè®­ç»ƒï¼Œåœ¨è¶…è¿‡ä¸€ä¸‡å››çš„clientsä¸Šéšæœºé€‰æ‹©100ä¸ªåšè®­ç»ƒï¼Œç”¨å›¾ç‰‡åœ¨100clientså‡åŒ€åˆ†å¸ƒè®­ç»ƒå‡ºçš„ç»“æœä¸ºupper boundï¼Œå‘ç°å³ä½¿ç”¨äº†YoGiå’ŒProxç­‰ä¼˜åŒ–æŠ€æœ¯ï¼Œå¿½ç•¥ç³»ç»Ÿçš„å¼‚æ„æ€§éƒ½ä¼šæ˜¾è‘—å¢åŠ æ¯ä¸€è½®æ­¤çš„å»¶è¿Ÿã€‚</li><li>Inability to enforce data selection criteriaï¼šå½“å‰çš„participantsé€‰æ‹©ç­–ç•¥ä¸ä»…å½±å“æ‰§è¡Œï¼Œä¹Ÿå½±å“æœ€ç»ˆè®­ç»ƒçš„biaså’Œconfidenceã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„shufflenetåšè®­ç»ƒçš„æ—¶å€™ï¼Œå’Œæ•°æ®çš„å…¨å±€åˆ†å¸ƒæ¯”èµ·æ¥ï¼Œå·®è·ä¹Ÿå¾ˆå¤§ï¼Œå°½ç®¡å½“clientsæ•°é‡å¢åŠ çš„æ—¶å€™ï¼Œè¿™ç§deviationä¼šå‡å°‘ï¼Œä½†æ˜¯<strong>å»é‡åŒ–ä¸åŒå®¢æˆ·ç«¯æ•°é‡ä¸‹çš„è¿™ç§åå·®</strong>ï¼Œä»ç„¶ååˆ†é‡è¦ã€‚åŒæ—¶ï¼Œå³ä½¿é€‰æ‹©äº†å¾ˆå¤šçš„clientsï¼Œå¼€å‘è€…ä»ç„¶ä¸èƒ½æŒ‡å®šä»–ä»¬æƒ³è¦çš„æ•°æ®é›†çš„åˆ†å¸ƒï¼Œæ¯”å¦‚å¼€å‘è€…æƒ³è¦ä¸€ä¸ªç›¸å¯¹balanced datasetsï¼Œä½†æ˜¯randomchoiceå¯èƒ½ä¼šå¯¼è‡´è¿™æ ·çš„biasï¼Œå¦‚å›¾4(b)æ‰€ç¤ºã€‚</li></ul><h2 id="Oortè®¾è®¡"><a href="#Oortè®¾è®¡" class="headerlink" title="Oortè®¾è®¡"></a>Oortè®¾è®¡</h2><p>Oort is a participant selection framework that identifies and cherry-picks valuable participants for FL training and testing</p><ul><li>ä½äºCoordinatoré‡Œé¢ï¼Œå’ŒFL execution interact.</li><li>Given developer-specified criteria, it responds with a list of participants, whereas the driver is in charge of initiating and managing execution on the Oort-selected remote participants</li></ul><p><img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111134439590.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111134439590.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111134439590"></p><p>Oort interacts with the developer and FL execution frameworks. </p><ul><li><strong>Job submission</strong>: the developer submits and specifies the participant selection criteria to the FL coordinator in the cloud. </li><li><strong>Participant selection</strong>: the coordinator enquires the clients meeting eligibility properties (e.g., battery level), and forwards their characteristics (e.g., liveness) to Oort.</li><li><strong>Given the developer requirements (and execution feedbacks in case of training 2aï¼‰.</strong></li><li><strong>Oort selects participants based on the given criteria and notifies the coordinator of this participant selection( 2b )</strong></li><li>Execution: the coordinator distributes relevant profiles (e.g., model) to these participants, and then each participant independently computes results (e.g., model weights in training) on her data</li><li>Aggregation: when participants complete the computation, the coordinator aggregates updates from participants</li></ul><h3 id="Interface"><a href="#Interface" class="headerlink" title="Interface"></a>Interface</h3><p><strong>Training selector</strong>: </p><ul><li>improve the timeto-accuracy performance of federated training</li><li>it captures the utility of clients in training, and efficiently explores and selects high-utility clients at runtime</li></ul><p><img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111135103438.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111135103438.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111135103438"></p><ul><li><strong>When the individual client data characteristics (e.g., categorical distribution) are not provided</strong>, the testing selector determines the number of participants needed to cap the data deviation of participants <strong>from the global</strong></li><li>it cherry-picks participants to <strong>serve the exact developer-specified requirements</strong> on data while minimizing the duration of testing</li></ul><h2 id="è”é‚¦æ¨¡å‹çš„è®­ç»ƒ"><a href="#è”é‚¦æ¨¡å‹çš„è®­ç»ƒ" class="headerlink" title="è”é‚¦æ¨¡å‹çš„è®­ç»ƒ"></a>è”é‚¦æ¨¡å‹çš„è®­ç»ƒ</h2><h3 id="the-trade-off-in-selecting-participants-for-FL-training"><a href="#the-trade-off-in-selecting-participants-for-FL-training" class="headerlink" title="the trade-off in selecting participants for FL training"></a>the trade-off in selecting participants for FL training</h3><p>å®¢æˆ·ç«¯æ•°æ®å’Œç³»ç»Ÿperformanceæœ‰coupledçš„æœ¬è´¨ï¼Œå› æ­¤è¦ç»¼åˆè€ƒè™‘ã€‚</p><p>åˆä½¿ç”¨äº†MobileNetåœ¨OpenImageçš„datasetå¯è§†åŒ–äº†è¿™ä¸¤ä¸ªæœ‰æ•ˆæ€§çš„åˆ†åˆ«çš„å½±å“ï¼ˆFigure 7ï¼‰.</p><ul><li>ä¼˜åŒ–System Efficiencyï¼ˆâ€œOpt-Sys. Efficiencyâ€ï¼‰èƒ½å¤Ÿå‡å°‘æ¯ä¸ªè½®æ¬¡çš„æ‰§è¡Œæ—¶é—´ï¼ˆä¾‹å¦‚é€‰æ‹©æœ€å¿«çš„clientsï¼‰ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ›´å¤šçš„è®­ç»ƒè½®æ¬¡ï¼ˆç›¸æ¯”éšæœºé€‰æ‹©è€Œè¨€ï¼‰ï¼Œå› ä¸ºclientçš„æ•°æ®ä¹‹å‰å¯èƒ½å·²ç»è¢«å…¶å®ƒå‚ä¸è€…è¿‡å»çš„è½®æ¬¡çš„æ•°æ®è¡¨ç¤ºè¿‡äº†ï¼ˆ<strong>ä¸ºä»€ä¹ˆï¼Ÿ</strong>ï¼‰ã€‚</li><li>é€‰æ‹©é«˜statistical utilityï¼ˆâ€Opt-Stat. Efficiencyâ€ï¼‰çš„clientså¯èƒ½ä¼šå¯¼è‡´æ¯ä¸€è½®çš„æ—¶é—´æ›´é•¿ï¼ˆå¦‚æœclientsæ˜¯systemçš„bottleneckçš„è¯ï¼ˆ<strong>ä¸ºä»€ä¹ˆ</strong>ï¼Ÿï¼‰ï¼‰</li><li><img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111141201330.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111141201330.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111141201330"></li></ul><h3 id="how-Oort-quantifies-the-client-utility-while-respecting-privacy"><a href="#how-Oort-quantifies-the-client-utility-while-respecting-privacy" class="headerlink" title="how Oort quantifies the client utility while respecting privacy"></a>how Oort quantifies the client utility while respecting privacy</h3><h4 id="Client-Statistical-Utilityï¼š"><a href="#Client-Statistical-Utilityï¼š" class="headerlink" title="Client Statistical Utilityï¼š"></a>Client Statistical Utilityï¼š</h4><p>be able to efficiently capture the client data utility toward improving model performance for various training tasks, and respect privacy</p><p><strong>leverage importance sampling</strong>ï¼šå°±æ˜¯é€‰æ‹©å¸¦æ¥æ¢¯åº¦æ›´æ–°æœ€å¤§çš„clientsï¼Œ</p><p><strong>impractical</strong>ï¼š</p><ul><li>å®¢æˆ·ç«¯ç®—è¿™äº›èŒƒæ•°çš„æ—¶å€™ä¼šå¸¦æ¥é¢å¤–çš„æ—¶é—´å¼€é”€ã€‚</li><li>è¿™ä¸ªæ¢¯åº¦èŒƒæ•°éšç€æ¨¡å‹çš„æ›´æ–°ä¼šä¸æ–­çš„å˜åŒ–ã€‚ï¼ˆä¸ºä»€ä¹ˆè¿™æ˜¯impracticalçš„ç†ç”±ï¼Ÿï¼‰</li></ul><p><img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111141548161.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111141548161.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111141548161"></p><p><strong>å› æ­¤</strong>ï¼šåšäº†ä¸ªè¿‘ä¼¼æ¥å‡å°‘æ—¶é—´å¼€é”€ï¼Œå°±æ˜¯ç”¨lossè€Œä¸æ˜¯ç”¨weightsï¼ˆæ„Ÿè§‰åƒæ˜¯ç¼–äº†ä¸ªæ•…äº‹ï¼Œå…ˆç¼–ä¸ªæ¯”è¾ƒéš¾çš„ï¼Œç„¶åè¯´å®ƒä¸å¥½ï¼Œç„¶åè¯´æˆ‘è¿™ä¸ªå¥½ï¼Œæä¸ªç®€å•çš„ï¼‰ã€‚Insightï¼šæ›´å¤§çš„æ¢¯åº¦å¯¹åº”ç€æ›´å¤§çš„lossï¼ˆä¸ºä»€ä¹ˆï¼Ÿï¼‰</p><p><img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111142603020.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111142603020.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111142603020"></p><p><strong>How Oort respects privacyï¼š</strong></p><ul><li>Training loss measures the prediction confidence of a model without revealing the raw data and is often collected in real FL deploymentsã€‚ï¼ˆäººå®¶éƒ½æ”¶é›†äº†è¿™ä¸ªï¼Œæˆ‘è¿™ä¸ªæ²¡æœ‰é¢å¤–çš„æ”¶é›†æ•°æ®ï¼‰ã€‚</li><li>ä¸‰ç§è€ƒè™‘éšç§çš„æ–¹å¼ï¼ˆå…¶å®å°±æ˜¯å¯¹ä¸Šé¢è¿™ä¸ªå…¬å¼çš„è§£é‡Šï¼Œæ²¡æœ‰é¢å¤–çš„å¤šåšå¤šå°‘å·¥ä½œï¼‰ï¼š<ul><li>åªä¾é lossé€‰æ‹©clientï¼Œlossæ˜¯æœ¬åœ°è®¡ç®—çš„ï¼Œä¸ä¼šæ­ç¤ºå®¢æˆ·ç«¯ä¸Šæ ·æœ¬çš„æ•°æ®åˆ†å¸ƒã€‚</li><li>å’ŒLDPä¸€æ ·ï¼Œå³ä½¿çš„losså¼•èµ·äº†éšç§é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ç”¨å·®åˆ†éšç§æŠ€æœ¯å»è§£å†³è¿™æ ·çš„é—®é¢˜ã€‚</li><li>Oortå¯ä»¥çµæ´»çš„æ›´æ¢å…¶å®ƒçš„é€‰æ‹©clientçš„å®¢æˆ·ç«¯çš„ç­–ç•¥ã€‚</li></ul></li><li>åœ¨technical reportä¸Šæœ‰è¿›ä¸€æ­¥çš„å„ç§ç­–ç•¥çš„ç†è®ºåˆ†æï¼ˆæ¯”å¦‚ä½¿ç”¨batchæ•°æ®çš„æ¢¯åº¦çš„normï¼‰ï¼Œå¹¶ä¸”ç”¨å®éªŒå±•ç¤ºäº†ä½¿ç”¨æ•°æ®æœ‰å™ªéŸ³æƒ…å†µä¸‹çš„è¾ƒå¥½çš„æ€§èƒ½ï¼ˆä¸ºäº†è¯æ˜ç¡®å®å¯ä»¥ç”¨å·®åˆ†éšç§è¿™æ ·çš„æŠ€æœ¯å»è§£å†³è¿™ä¸ªé—®é¢˜ï¼‰ã€‚</li></ul><h4 id="Trading-off-Statistical-and-System-Efficiency"><a href="#Trading-off-Statistical-and-System-Efficiency" class="headerlink" title="Trading off Statistical and System Efficiency"></a>Trading off Statistical and System Efficiency</h4><p>Simply selecting clients with high statistical utility can hamper the system efficiencyã€‚ï¼ˆå› ä¸ºstatistical utilityçš„å¢åŠ ä¼šå¯¼è‡´è®­ç»ƒæ—¶é—´çš„å¢é•¿ï¼Œè¿™å¯èƒ½ä¼šå½±å“system efficiencyï¼Œå¥½çš„system efficiencyåº”è¯¥æ˜¯è®­ç»ƒæ¯ä¸€è½®æ­¤çš„æ—¶é—´ç›¸å¯¹æ¥è¯´æ¯”è¾ƒå¿«çš„ï¼‰ï¼Œé‡‡å–äº†å›¾ä¸­æ‰€ç¤ºçš„è®¡ç®—æ–¹å¼ã€‚</p><ul><li>tæ˜¯æ¯ä¸€ä¸ªå®¢æˆ·ç«¯è®­ç»ƒçš„æ—¶é—´ã€‚</li><li>æƒ©ç½šäº†é‚£äº›ä¼šæˆä¸ºç³»ç»Ÿoverheadçš„å®¢æˆ·ç«¯ã€‚</li></ul><p><img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111150040681.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111150040681.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111150040681"></p><ul><li><p><strong>Navigating the trade-off</strong>ï¼š</p><p>  <img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111153210944.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111153210944.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111153210944"></p><p>  éšç€roundçš„è¿›è¡Œï¼Œæ¨¡å‹çš„lossä¼šé€æ¸ä¸‹é™ï¼Œåœ¨<strong>æ¨¡å‹çš„è®­ç»ƒçš„åæœŸï¼Œåº”è¯¥å‡å°å¯¹èƒ½å¤Ÿå–å¾—é«˜statistical utilityçš„clientsçš„é€‰æ‹©</strong>ï¼š</p><p>  <img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111153506139-1668152108109-1.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111153506139-1668152108109-1.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111153506139"></p></li></ul><h4 id="how-it-selects-high-utility-clients-at-scale-despite-staleness-in-client-utility-as-training-evolves"><a href="#how-it-selects-high-utility-clients-at-scale-despite-staleness-in-client-utility-as-training-evolves" class="headerlink" title="how it selects high-utility clients at scale despite staleness in client utility as training evolves"></a>how it selects high-utility clients at scale despite staleness in client utility as training evolves</h4><p>è¿˜æœ‰ä¸€äº›å…¶å®ƒçš„è€ƒè™‘ï¼š</p><ul><li><strong>Scalability</strong>: a clientâ€™s utility can only be determined after it has participated in training; <strong>how to choose from clients at scale without having to try all clients once</strong>? ï¼ˆä¸ºäº†æé«˜é€‰æ‹©å™¨çš„æ•ˆç‡ï¼‰</li><li><strong>Staleness</strong>: since not every client participates in every round, <strong>how to account for the change in a clientâ€™s utility since its last participation</strong>?ï¼ˆå®¢æˆ·ç«¯å¹¶ä¸æ˜¯æ¯ä¸€è½®éƒ½å‚åŠ çš„ï¼Œå¦‚ä½•å¤„ç†è¿™æ ·çš„é—®é¢˜ï¼‰ã€‚</li><li><strong>Robustness</strong>: how to <strong>be robust to outliers in the presence of corrupted clients</strong> (e.g., with noisy data)? ï¼ˆå¦‚ä½•å¯¹æœ‰outliersçš„æ•°æ®é²æ£’ï¼‰</li></ul><p>åšäº†ä¸€äº›å®éªŒä¸Šçš„æ¢ç´¢ï¼š</p><p><strong>Online exploration-exploitation of high-utility clients</strong></p><ul><li>å°†é€‰æ‹©å®¢æˆ·ç«¯æŠ½è±¡ä¸ºå¤šè‡‚è€è™æœºé—®é¢˜ï¼Œclientsæ¯”ä½œâ€œarmâ€ï¼Œutilityæ¯”ä½œâ€œrewardâ€ï¼Œå¤šè‡‚è€è™æœºå’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•æ¯”èµ·æ¥ï¼Œscalable and flexibleï¼ˆå³ä½¿è§£ç©ºé—´ä¾‹å¦‚å®¢æˆ·ç«¯çš„æ•°é‡è·Ÿéšæ—¶é—´å˜åŒ–å¾ˆå¤§ï¼‰ï¼ˆè¿™é‡Œå…¶å®å°±æ˜¯è§£é‡Šä¸ºä»€ä¹ˆè¦ç”¨å¤šè‡‚è€è™æœºï¼Œè¿™æ ·å¥½ï¼‰ã€‚<strong>å¤šè‡‚è€è™æœºæ—¢å¯ä»¥æ ¹æ®ä¹‹å‰çš„æ¦‚ç‡åšå‡ºé€‰æ‹©ï¼Œä¹Ÿå¯ä»¥æ¢ç´¢æœªè¢«é€‰æ‹©çš„å®¢æˆ·ç«¯ï¼š</strong><ul><li>æ¯ä¸€ä¸ªé€‰æ‹©è½®æ¬¡å¼€å§‹çš„æ—¶å€™,Oortéƒ½æ¥å—ä¸Šä¸€ä¸ªè®­ç»ƒè½®æ¬¡çš„åé¦ˆï¼Œä¼šæ›´æ–°æ¯ä¸€ä¸ªå®¢æˆ·ç«¯çš„statistical utilityå’Œsystem performanceã€‚</li><li>å¯¹äºå·²ç»exploredçš„å®¢æˆ·ç«¯æ¥è¯´ï¼ŒOortè®¡ç®—å®¢æˆ·ç«¯çš„utilityå¹¶ä¾æ®high-utilityå‡å°é€‰æ‹©çš„èŒƒå›´ã€‚</li><li>ç„¶åé‡‡æ ·$\epsilon \in [0,1]$æ¯”ç‡çš„participantsï¼ˆé‚£äº›ä¹‹å‰æ²¡æœ‰è¢«selectedçš„ï¼‰ã€‚ </li></ul></li></ul><p><strong>Exploitation under staleness in client utility</strong></p><p>å—åˆ°å¤šè‡‚è€è™æœºç½®ä¿¡åŒºé—´çš„å½±å“ï¼Œæäº†ä¸ªincentive termï¼Œæ…¢æ…¢çš„å¢åŠ clientçš„utilityï¼ˆå¦‚æœä¸€ä¸ªclientå·²ç»è¢«å¿½ç•¥äº†å¾ˆé•¿çš„æ—¶é—´ï¼‰ï¼Œè¿™æ ·çš„è¯å½“ä¸€ä¸ªclientè¢«å¿½ç•¥å¾ˆä¹…äº†ï¼Œä»–çš„è¢«é€‰çš„æ¦‚ç‡ä¼šæ…¢æ…¢çš„å¢åŠ ã€‚</p><p>ä¸ç›´æ¥çš„é€‰æ‹©top-kçš„utilityï¼Œæäº†ä¸ªconfidence interval c on the cut-off utilityï¼ˆé»˜è®¤$95%$ï¼‰ï¼Œå°±æ˜¯é€‰æ‹©utilityå¤§äº$c%$çš„top $(1-\epsilon) \times K$çš„ï¼Œè¿™æ ·çš„è¯å¯ä»¥å»é™¤clients utilityä¸ç¡®å®šæ€§çš„å¹²æ‰°ã€‚</p><p><strong>Robust exploitation under outliers</strong></p><p>åªæ ¹æ®utilityé€‰be vulnerable to outliers in unfavorable settings</p><ul><li><p>å› æ­¤Oortä¼šåœ¨ä¸€äº›å®¢æˆ·ç«¯å·²ç»è¢«ç”¨è¿‡ä¸€äº›è½®æ¬¡ä¹‹åæŠŠå…¶ç§»é™¤ã€‚</p></li><li><p>é™åˆ¶client utilityçš„ä¸Šé™å€¼ã€‚</p></li></ul><p><strong>Accommodation to diverse selection criteria</strong></p><p>å¯ä»¥è‡ªå®šä¹‰utilityçš„è®¡ç®—æ–¹æ³•ã€‚</p><h2 id="è”é‚¦å­¦ä¹ çš„æ¨¡å‹æµ‹è¯•ï¼ˆæ²¡å¤§çœ‹æ‡‚ï¼‰"><a href="#è”é‚¦å­¦ä¹ çš„æ¨¡å‹æµ‹è¯•ï¼ˆæ²¡å¤§çœ‹æ‡‚ï¼‰" class="headerlink" title="è”é‚¦å­¦ä¹ çš„æ¨¡å‹æµ‹è¯•ï¼ˆæ²¡å¤§çœ‹æ‡‚ï¼‰"></a>è”é‚¦å­¦ä¹ çš„æ¨¡å‹æµ‹è¯•ï¼ˆæ²¡å¤§çœ‹æ‡‚ï¼‰</h2><h3 id="å¼€å‘è€…æœªè‡ªå®šä¹‰æ•°æ®çš„åˆ†å¸ƒæ—¶é»˜è®¤ç»™å‡ºçš„æ•°æ®å…·æœ‰ä»£è¡¨æ€§"><a href="#å¼€å‘è€…æœªè‡ªå®šä¹‰æ•°æ®çš„åˆ†å¸ƒæ—¶é»˜è®¤ç»™å‡ºçš„æ•°æ®å…·æœ‰ä»£è¡¨æ€§" class="headerlink" title="å¼€å‘è€…æœªè‡ªå®šä¹‰æ•°æ®çš„åˆ†å¸ƒæ—¶é»˜è®¤ç»™å‡ºçš„æ•°æ®å…·æœ‰ä»£è¡¨æ€§"></a>å¼€å‘è€…æœªè‡ªå®šä¹‰æ•°æ®çš„åˆ†å¸ƒæ—¶é»˜è®¤ç»™å‡ºçš„æ•°æ®å…·æœ‰ä»£è¡¨æ€§</h3><p>enable guided participant selection by determining the number of participants needed to guarantee this deviation target.</p><p><strong>å¥½åƒåªæ˜¯å¯¹æ•°æ®çš„æ ·æœ¬æ•°è¿›è¡Œäº†ä¸€ä¸ªæŠ½å–æ•°æ®çš„åˆ†å¸ƒçš„é™åˆ¶ï¼Œä½¿æŠ½å–çš„æ•°æ®æ›´åŠ å…·æœ‰ä»£è¡¨æ€§ã€‚</strong></p><h4 id="Preserving-Data-Representativeness"><a href="#Preserving-Data-Representativeness" class="headerlink" title="Preserving Data Representativeness"></a>Preserving Data Representativeness</h4><p>ä½¿ç”¨L1 distanceå»åº¦é‡æ‰€æœ‰å‚ä¸è€…ä¸å…¨å±€æ•°æ®åˆ†å¸ƒçš„åå·®ï¼Œå¯¹äºç±»åˆ«X, L1-distanceè®¡ç®—å¦‚ä¸‹ï¼š</p><p>$\left| \overline{X} - E[\overline{X}] \right|$ï¼Œè¡¨ç¤ºæ‰€æœ‰å‚ä¸è€…çš„æ ·æœ¬å‡å€¼å’Œæ‰€æœ‰clientsä¹‹é—´æ ·æœ¬å‡å€¼çš„å…³ç³»ã€‚</p><p>ç”±äºä¸€ä¸ªå®¢æˆ·ç«¯çš„æ ·æœ¬çš„æ•°é‡ä¸ä¼šå½±å“å…¶å®ƒå®¢æˆ·ç«¯çš„æ ·æœ¬çš„æ•°é‡ï¼Œæ‰€ä»¥æ¯ä¸€ä¸ªå®¢æˆ·ç«¯çš„æ ·æœ¬çš„ä¸ªæ•°å¯ä»¥çœ‹ä½œæ€»ä½“Xçš„é‡‡æ ·ï¼Œç»™å®šç½®ä¿¡åº¦å’Œç½®ä¿¡åº¦ï¼Œæˆ‘ä»¬çš„<strong>ç›®æ ‡</strong>å°±æ˜¯æ±‚å‡ºéœ€è¦çš„å‚ä¸è€…çš„æ•°é‡ï¼Œè¿™æ ·çš„è¯è¦é€‰å‡ºæ¥çš„ä»£è¡¨æ€§çš„æ ·æœ¬çš„åˆ†å¸ƒè¢«boundedï¼Œä¹Ÿå°±æ˜¯ï¼š</p><p>$Pr[\overline{X}-E[\overline{X}]]&gt;\delta$ï¼Œè¿™å°±å˜æˆäº†ä¸€ä¸ªéšæœºæ•°é‡‡æ ·çš„é—®é¢˜ï¼Œè¿ç”¨äº†<strong>Hoeffding bound</strong>å»åº¦é‡ä¸åŒå‚ä¸è€…é‡‡æ ·å¾—åˆ°çš„æ•°æ®åç§»ã€‚ï¼ˆæŠ€æœ¯æŠ¥å‘Šä¸­æœ‰ç†è®ºçš„é¢è¯æ˜å’Œç»“æœï¼‰ã€‚</p><h4 id="Estimating-the-number-of-participants-to-cap-deviation"><a href="#Estimating-the-number-of-participants-to-cap-deviation" class="headerlink" title="Estimating the number of participants to cap deviation"></a>Estimating the number of participants to cap deviation</h4><p>å³ä½¿ä¸ªä½“çš„æ•°æ®ç‰¹å¾ä¸å¯å¾—ï¼Œå¼€å‘è€…ä¹Ÿå¯ä»¥å»æŒ‡å®štolerance $\epsilon$ï¼ŒOortä¼šè¾“å‡ºæ»¡è¶³è¿™æ ·çš„toleranceéœ€è¦çš„å‚ä¸è€…æ•°ç›®ã€‚</p><p>æ¨¡å‹è¦æ±‚å¼€å‘è€…è¾“å…¥æ¯ä¸€ä¸ªclientçš„æœ€å¤§å’Œæœ€å°æ ·æœ¬æ•°ï¼Œå®¢æˆ·ç«¯çš„æ€»æ•°ï¼ˆä¸ºä»€ä¹ˆè¦å®¢æˆ·ç«¯çš„æ€»æ•°ï¼Ÿï¼‰ã€‚å¼€å‘è€…ä¹Ÿå¯ä»¥è®¾ä¸€ä¸ªæ¨¡ç³Šçš„é™åˆ¶ï¼ˆä¾‹å¦‚ä¾æ®è®¾å¤‡æ¨¡å‹çš„å®¹é‡æ¥è®¾ç½®ï¼‰</p><p>å¼€å‘è€…å¯ä»¥å°†è‡ªå·±çš„æ¨¡å‹å…ˆéƒ¨ç½²åˆ°å·²ç»é€‰æ‹©çš„å‚ä¸è€…ä¸Šé¢ï¼Œå†æ”¶é›†äº†è¿™äº›å‚ä¸è€…çš„ç»“æœä¹‹åï¼Œå†å»ç¡®è®¤ä¸€ä¸‹è®¡ç®—æ•°æ®çš„ä»£è¡¨æ€§ã€‚</p><h3 id="å¼€å‘è€…è‡ªå®šä¹‰è‡ªå·±çš„æ•°æ®åˆ†å¸ƒ"><a href="#å¼€å‘è€…è‡ªå®šä¹‰è‡ªå·±çš„æ•°æ®åˆ†å¸ƒ" class="headerlink" title="å¼€å‘è€…è‡ªå®šä¹‰è‡ªå·±çš„æ•°æ®åˆ†å¸ƒ"></a>å¼€å‘è€…è‡ªå®šä¹‰è‡ªå·±çš„æ•°æ®åˆ†å¸ƒ</h3><p>å¯¹äºæ¯ä¸€ä¸ªclientï¼Œæ ¹æ®æ¯ä¸€ä¸ªclientå¯ä»¥æä¾›çš„å¼€å‘äººå‘˜éœ€è¦çš„æ ·æœ¬çš„æ•°ç›®ï¼Œæ¯ä¸€ä¸ªclientçš„ç®—åŠ›ï¼Œæ¯ä¸€ä¸ªclientä¸Šä¼ è¾“çš„æ•°æ®å¤§å°ï¼Œæ¯ä¸€ä¸ªclientçš„å¸¦å®½ï¼Œå¯ä»¥è®¡ç®—å‡ºæ‰€æœ‰participantä¸Šæœ€å¤šçš„èŠ±è´¹æ—¶é—´ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼˜åŒ–è¿™ä¸ªæœ€å¤§æ—¶é—´ã€‚</p><p><img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111222910741.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111222910741.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111222910741"></p><p>è¿™æ ·çš„mixed-integer linear programming (MILP) modelè§£å‡ºæ¥çš„ç»“æœå¾ˆå¥½ï¼Œä½†æ˜¯<strong>æ—¶é—´å¤æ‚åº¦è¾ƒé«˜</strong>ï¼Œæ—¶é—´å¤æ‚åº¦å¾ˆé«˜ï¼ˆæ²¡è¯´å¤šå°‘ï¼‰ã€‚</p><ul><li><p>è´ªå¿ƒçš„å¯å‘å¼ç®—æ³•è§£å†³æ—¶é—´å¤æ‚åº¦é«˜çš„é—®é¢˜ï¼š</p><p>  <img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111223455902.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111223455902.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111223455902"></p></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>Oortï¼Œ2617è¡Œä»£ç ï¼Œå’ŒFL engineä¾‹å¦‚ï¼ˆPySyftå’ŒTensorFlowé›†æˆï¼‰ã€‚</p><ul><li>ä½æ¶ˆè€—</li><li>å®šæœŸå¤‡ä»½ã€‚</li><li>æ•…éšœæ¢å¤ã€‚</li><li>Gurobi solverå»è§£å†³MILPé—®é¢˜ã€‚</li></ul><h2 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h2><p>68ä¸ªè‹±ä¼Ÿè¾¾ Tesla P100çš„GPUsä¸Šæ¯ä¸€è½®æ¨¡æ‹Ÿäº†1300ä¸ªå‚ä¸è€…ï¼Œåœ¨ã€‚è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µæ¨¡æ‹ŸçœŸå®çš„å¼‚æ„clientç³»ç»Ÿçš„æ€§èƒ½å’Œæ•°æ®ï¼Œä½¿ç”¨äº†ä¸€ä¸ªå¼€æºçš„FL benchmarkï¼š</p><ul><li>ä¸åŒçš„è®¾å¤‡</li><li>ä¸åŒçš„æ¨¡å‹</li><li>ä¸åŒçš„ç½‘ç»œååé‡å’Œè¿æ¥æ€§</li></ul><p>æ‰‹åŠ¨çš„è®©ä¸åŒclientä¸Šçš„æ•°æ®äº§ç”Ÿæ•°æ®å¼‚æ„æ€§ã€‚</p><p>coordinatorå’Œclientsä¹‹é—´çš„äº¤æµæ¶æ„æ˜¯å‚æ•°æœåŠ¡å™¨æ¶æ„ã€‚ï¼ˆPySyftå’ŒçœŸå®çš„FLéƒ¨ç½²éƒ½æ˜¯è¿™ä¹ˆå¹²çš„ï¼‰ã€‚</p><p>ä¸ºäº†å‡å°staggersçš„å½±å“ï¼Œä½¿ç”¨äº†çœŸå®FLéƒ¨ç½²ä¸­ä½¿ç”¨çš„æœºåˆ¶ã€‚</p><p><strong>æ•°æ®é›†å’Œæ¨¡å‹</strong>ï¼š</p><p><img src="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111224714887.png" class="lazyload placeholder" data-srcset="/2022/11/12/osdi21-oort-efficient-federated-learning-via-guided-participant-selection/image-20221111224714887.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221111224714887"></p><p><strong>æ¨¡å‹è®¾ç½®çš„ä¸€äº›å‚æ•°</strong></p><h3 id="FL-Training-Evaluation"><a href="#FL-Training-Evaluation" class="headerlink" title="FL Training Evaluation"></a>FL Training Evaluation</h3><h4 id="End-to-End-Performance"><a href="#End-to-End-Performance" class="headerlink" title="End-to-End Performance"></a>End-to-End Performance</h4><ul><li>Oort improves time-to-accuracy performance</li><li>Oort improves final model accuracy</li></ul><h4 id="Performance-Breakdown"><a href="#Performance-Breakdown" class="headerlink" title="Performance Breakdown"></a>Performance Breakdown</h4><ul><li>Breakdown of time-to-accuracy efficiency</li><li>Oort achieves close to upper-bound statistical performance</li></ul><h4 id="Sensitivity-Analysis"><a href="#Sensitivity-Analysis" class="headerlink" title="Sensitivity Analysis"></a>Sensitivity Analysis</h4><ul><li>Impact of number of participants K</li><li>Impact of penalty factor Î± on stragglers</li><li>Impact of outliers</li><li>Impact of noisy utility</li><li>Oort can respect developer-preferred fairness</li></ul>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICML22-FedScale-Benchmarking Model and System Performance of Federated Learning at Scale&#39;</title>
      <link href="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/"/>
      <url>/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/</url>
      
        <content type="html"><![CDATA[<h1 id="ICML22-FedScale-Benchmarking-Model-and-System-Performance-of-Federated-Learning-at-Scale"><a href="#ICML22-FedScale-Benchmarking-Model-and-System-Performance-of-Federated-Learning-at-Scale" class="headerlink" title="ICML22-FedScale-Benchmarking Model and System Performance of Federated Learning at Scale"></a>ICML22-FedScale-Benchmarking Model and System Performance of Federated Learning at Scale</h1><ul><li>ä¸å’Œå¤§å®¶å·ç®—æ³•ï¼Œå»å·ç³»ç»Ÿå»ã€‚</li><li>åšäº†ä¸€ä¸ªå¯ä»¥æ¨¡æ‹ŸFLä»»åŠ¡çš„ç³»ç»Ÿã€‚</li><li>ä½¿ç”¨çœŸå®çš„æ•°æ®é›†æ¥åšæµ‹è¯„ã€‚</li></ul><h2 id="a-federated-learning-FL-benchmarking-suite"><a href="#a-federated-learning-FL-benchmarking-suite" class="headerlink" title="a federated learning (FL) benchmarking suite"></a>a federated learning (FL) benchmarking suite</h2><ol><li><strong>realistic datasets</strong>ï¼ˆ20ï¼‰ï¼ˆranging from image classification and object detection to language modeling and speech recognitionï¼‰ã€‚Each dataset comes with a unified evaluation protocol <strong>using real-world data splits and evaluation metrics</strong>ã€‚</li><li><strong>a scalable and extensible runtime</strong>ã€‚API to implement FL algorithmsï¼Œdeploy them at scale across diverse hardware and software backends, and evaluate them at scaleã€‚<ol><li>mobile backendï¼šenable on-device FL evaluation</li><li>a cluster backendï¼š benchmark various practical FL metrics</li></ol></li><li><strong>systematic experiments</strong>ï¼ˆsome implicationsï¼‰ï¼šhighlight the pressing need for co-optimizing system and statistical efficiency, especially in tackling system stragglers, accuracy bias, and device energy trade-offs</li></ol><h2 id="Existing-FL-benchmarks-can-be-misleading"><a href="#Existing-FL-benchmarks-can-be-misleading" class="headerlink" title="Existing FL benchmarks can be misleading"></a>Existing FL benchmarks can be misleading</h2><ol><li>A lot of problems</li></ol><p><img src="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123.png" class="lazyload placeholder" data-srcset="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><ol start="2"><li>Figure 2: <ol><li>statistical performance becomes worse when encountering realistic client behavior.</li><li>FL training with hundreds of participants each round performs better than that with tens of participants</li></ol></li></ol><h2 id="Realistic-FL-Workloads"><a href="#Realistic-FL-Workloads" class="headerlink" title="Realistic FL Workloads"></a>Realistic FL Workloads</h2><p><img src="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667052732044-2.png" class="lazyload placeholder" data-srcset="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667052732044-2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="Realistic-data-and-partitions"><a href="#Realistic-data-and-partitions" class="headerlink" title="Realistic data and partitions"></a>Realistic data and partitions</h3><p><strong>how to generate data?</strong> Following the practical FL deployments (Yang et al., 2018), we assign the clients of each dataset into the training, validation and testing groups. <strong>A high statistical deviation</strong> (e.g., wide distribution of the density) across clients <strong>not only in the quantity of samples</strong> (Figure 3(a)) <strong>but also in the data distribution</strong> (Figure 3(b)).</p><p><strong>hundreds to millions of clients</strong></p><h3 id="Client-device-system-speed-is-heterogeneous"><a href="#Client-device-system-speed-is-heterogeneous" class="headerlink" title="Client device system speed is heterogeneous"></a>Client device system speed is heterogeneous</h3><ol><li>Client device system speed is heterogeneous<ol><li>system trace of different clients: <strong>AI Benchmark (Ignatov et al.)</strong> and <strong>MobiPerf Measurements (mob)</strong> on mobiles.</li><li><strong>AI Benchmark</strong> provides the <strong>training and inference speed of diverse models</strong> (e.g., MobileNet) across a wide range of device models (e.g., Huawei P40 and Samsung Galaxy S20)</li><li><strong>MobiPerf</strong> has collected the available <strong>cloud-to-edge network throughput</strong> of over <strong>100k world-wide mobile clients</strong></li></ol></li><li>Client device availability is dynamic: <strong>a large-scale user behavior dataset</strong>: 136k users (Yang et al., 2021). It includes 180 million trace items of client devices (e.g., battery charge or screen lock) over a week.</li></ol><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p><strong>customize their dataset:</strong> standardized APIs, a Python package, for the user to easily leverage these datasets</p><h3 id="Mobile-Backend"><a href="#Mobile-Backend" class="headerlink" title="Mobile Backend"></a>Mobile Backend</h3><p>enable ondevice FL evaluation</p><p>FedScale mobile backend (Singapuram et al., 2022) is built atop the Termux app (ter),</p><h3 id="Cluster-Backend"><a href="#Cluster-Backend" class="headerlink" title="Cluster Backend"></a>Cluster Backend</h3><h4 id="deployment-mode"><a href="#deployment-mode" class="headerlink" title="deployment mode:"></a>deployment mode:</h4><p><strong>acts as the cloud aggregator</strong> and orchestrates FL executions across real devices</p><h4 id="simulation-mode"><a href="#simulation-mode" class="headerlink" title="simulation mode"></a>simulation mode</h4><p>providing various <strong>practical FL metrics</strong> by emulating realistic FL behaviors, such as <strong>computation/communication cost, latency and wall clock time</strong>. the first platform that enables FL benchmarking with practical FL runtime on GPUs/CPUs</p><ol><li><p><strong>Aggregator Simulator</strong>: </p><p>Once receiving new events, the event monitor activates the <strong>handler</strong> (e.g., aggregation handler to perform model aggregation), or the <strong>gRPC</strong> communicator to send/receive messages. The communicator <strong>records the size (cost)</strong> of every network traffic, and its <strong>runtime latency in FL wall-clock time ( traffic_size / client_bandwidth )</strong></p></li><li><p><strong>Client Simulator</strong>: </p><p>FedScale data loader loads the federated dataset of that client and feeds this data to the compute engine to run real training/testing. <strong>The computation latency</strong>: #_processed_sample Ã— latency_per_sample. <strong>The communication latency</strong> ( traffic_size client_bandwidth ). The device monitor will terminate the simulation of a client if the current FL runtime exceeds his available slot.</p></li><li><p><strong>Resource Manager</strong>: </p><p>when the number of participants/round exceeds the resource capacity (e.g., simulating thousands of clients on a few GPUs), the resource manager queues the overcommitted tasks of clients and schedules new client simulation from this queue once resource becomes available. This queuing will not affect the simulated FL runtime. </p></li></ol><p><strong>automated FL simulation</strong>:</p><ol><li>Task submission</li><li>FL simulation</li><li>Metrics output</li></ol><p><strong>FedScale Runtime is scalable and efficient</strong></p><h2 id="Other-flexible-APIs"><a href="#Other-flexible-APIs" class="headerlink" title="Other flexible APIs"></a>Other flexible APIs</h2><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>10 NVIDIA Tesla P100 GPUs in our evaluations, collects updates from the first N completed participants out of 1.3N participants to mitigate system stragglers in each round.</p><ol><li><p><strong>the performance of existing benchmarks</strong> and <strong>FedScale</strong> are <strong>quite close</strong> in the same settings if we <strong>turn off the optional system traces</strong> in FedScale. (underlying training and FL protocols in evaluations are the same)</p></li><li><p>Benchmarking FL statistical efficiency. (<strong>IID data vs. Non-IID data</strong>)</p><p><img src="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667055455629-4.png" class="lazyload placeholder" data-srcset="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667055455629-4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p></li><li><p>Benchmarking FL system efficiency</p><p>(1). FedScale Runtime enables fast-forward evaluations of the practical FL wall-clock time with fewer evaluation hours. Taking different number of local steps K in local SGD as an example (McMahan et al., 2017), Figure 12(a) and Table 12(b) illustrate that FedScale can evaluate this impact of K on practical FL runtime in a few hours.</p><p>(2). FedScale Runtime can dictate the FL execution cost by using realistic system traces.</p><p><img src="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667055708884-6.png" class="lazyload placeholder" data-srcset="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667055708884-6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p></li><li><p>Benchmarking FL privacy and security</p><p><img src="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667055979760-8.png" class="lazyload placeholder" data-srcset="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667055979760-8.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>privacy: DP-SGD (Geyer et al., 2017;Kairouz et al., 2021a): </p></li></ol><p>â€‹        FL security: backdoor attacks (Sun et al., 2019; Wang et al., 2020), </p><h2 id="Implications"><a href="#Implications" class="headerlink" title="Implications"></a>Implications</h2><ol><li>Heterogeneity-aware co-optimizations of communication and computation</li><li>Co-optimizations of statistical and system efficiency</li><li>FL design-decisions considering mobile environment</li></ol><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><p><img src="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667056105725-11.png" class="lazyload placeholder" data-srcset="/2022/10/28/icml22-fedscale-benchmarking-model-and-system-performance-of-federated-learning-at-scale/123-1667056105725-11.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OSDI22-Automatic Reliability Testing for Cluster Management Controllers</title>
      <link href="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/"/>
      <url>/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/</url>
      
        <content type="html"><![CDATA[<h1 id="OSDI22-Automatic-Reliability-Testing-for-Cluster-Management-Controllers"><a href="#OSDI22-Automatic-Reliability-Testing-for-Cluster-Management-Controllers" class="headerlink" title="OSDI22-Automatic Reliability Testing for Cluster Management Controllers"></a>OSDI22-Automatic Reliability Testing for Cluster Management Controllers</h1><p><strong>ä½œè€…æ ¹æ®insightç»™çš„ä¸€äº›å¯èƒ½çš„é”™è¯¯ç±»å‹ï¼Œå¯¹k8sçš„è°ƒåº¦å™¨è°ƒç”¨APIçš„æ¥å£è¿›è¡Œäº†ä¸€ä¸ªå°è£…ï¼Œé€šè¿‡å¯èƒ½çš„é”™è¯¯ç±»å‹ç”Ÿæˆäº†ä¸€äº›æµ‹è¯•è®¡åˆ’ï¼Œæ‰¾åˆ°äº†ä¸€äº›bugã€‚</strong></p><p>Borg, Omega and Kubernetes ä¾èµ–state-reconciliation principleå»å˜å¾—é«˜å¼¹æ€§å’Œå¯æ‹“å±•æ€§ï¼Œæ‰€æœ‰çš„<strong>é›†ç¾¤ç®¡ç†</strong>çš„<strong>é€»è¾‘</strong>å³<strong>æ§åˆ¶å™¨</strong>è¢«åµŒå…¥åœ¨æ¾æ•£è€¦åˆçš„å¾®æœåŠ¡ä¸­ï¼ˆè¿™äº›<strong>é›†ç¾¤ç®¡ç†çš„é€»è¾‘å°±æ˜¯å¾®æœåŠ¡</strong>ï¼‰ã€‚æ¯ä¸€ä¸ªæ§åˆ¶å™¨ç‹¬ç«‹çš„è§‚æµ‹å½“å‰çš„é›†ç¾¤çš„çŠ¶æ€ï¼Œé‡‡å–ç‰¹å®šçš„æªæ–½ä½¿å¾—å½“å‰çš„é›†ç¾¤æ»¡è¶³ç‰¹å®šçš„çŠ¶æ€ã€‚å¤æ‚çš„åˆ†å¸ƒå¼çš„æœ¬è´¨æ˜¯å»ºç«‹ç¨³å®šæ€§å¼ºçš„æ­£ç¡®çš„æ§åˆ¶å™¨å¾ˆéš¾ï¼Œæ§åˆ¶å™¨é¢ä¸´ç€<strong>æ— æ•°çš„å¯é æ€§é—®é¢˜</strong>ï¼Œå¯¼è‡´<strong>æ•°æ®æŸå¤±ï¼Œå®‰å…¨å’Œèµ„æºæ³„æ¼</strong>ã€‚</p><p><strong>Sieve</strong>ï¼Œæµ‹è¯•é›†ç¾¤æ§åˆ¶å™¨çš„å·¥å…·ï¼Œé€šè¿‡ä¸æ–­çš„å¹²æ‰°æ§åˆ¶å™¨çœ‹åˆ°çš„å½“å‰é›†ç¾¤çš„çŠ¶æ€ï¼Œç„¶åå»æ¯”è¾ƒé›†ç¾¤å—åˆ°å¹²æ‰°å’Œæ²¡å—åˆ°å¹²æ‰°æ—¶å€™çš„çŠ¶æ€å»æ£€æµ‹å®‰å…¨å’Œlivenessçš„é—®é¢˜ã€‚Sieveçš„è®¾è®¡åŸºäºåŸºæœ¬<strong>çŠ¶æ€æ¢å¤ç³»ç»Ÿçš„åŸºæœ¬æœºä¼š</strong>ï¼Œè¿™äº›ç³»ç»ŸåŸºäº<strong>state-centric interfaces between controllers and cluster state</strong>ï¼ŒSieveæ‰¾åˆ°äº†46ä¸ªä¸¥é‡çš„å®‰å…¨å’Œlivenessçš„bugï¼Œ35ä¸ªç¡®è®¤ï¼Œ22ä¸ªä¿®å¤ï¼Œfalse-positiveæ¯”ä¾‹æ˜¯$3.5%$</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>æ§åˆ¶å™¨éµå¾ª<strong>state-reconciliation principle</strong>ï¼Œreconciles the current state of the cluster to match a desired state. é›†ç¾¤çš„çŠ¶æ€æ”¾åœ¨é€»è¾‘ä¸Šä¸­å¿ƒåŒ–çš„ï¼Œé«˜å¯ç”¨çš„æ•°æ®ä¸­å¿ƒï¼ŒK8Sé‡Œé¢çš„<strong>podsã€nodesã€volumnså’Œåº”ç”¨å®ä¾‹éƒ½è¢«è¡¨ç¤ºä¸ºé›†ç¾¤ä¸­çŠ¶æ€çš„å¯¹è±¡</strong>ã€‚</p><p><img src="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/123.png" class="lazyload placeholder" data-srcset="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/123.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>Kubernetes controllersç®¡ç†Cassandraçš„æ—¶å€™ï¼Œåˆ æ‰podå’Œfinalizingä¸­é—´å¦‚æœå—åˆ°å¹²æ‰°ï¼Œvolæ²¡åˆ æ‰ï¼Œå°±ä¼šå¯¼è‡´storageçš„æ³„æ¼ã€‚</p><p><strong>ä¼˜ç‚¹</strong>ï¼šé«˜å¯ç”¨ï¼Œ1) ä¸éœ€è¦æ­£å¼çš„å¯¹æ§åˆ¶å™¨å’Œé›†ç¾¤ç®¡ç†çš„é…ç½®è¯´æ˜æ–‡ä»¶ 2) ä¸éœ€è¦å‡è®¾ä»£ç å¯èƒ½åœ¨å“ªé‡Œå‡ºç°bug 3) é«˜åº¦ä¸“ä¸šåŒ–çš„æµ‹è¯•è¾“å…¥ã€‚<strong>åªéœ€è¦åˆ›å»ºæ§åˆ¶å™¨é•œåƒå’ŒåŸºæœ¬æµ‹è¯•å·¥ä½œæµçš„è¡¨ç¤ºï¼Œè‡ªåŠ¨åŒ–æµ‹è¯•</strong>ï¼Œå³åªéœ€è¦æä¾›manifestè¯´æ˜å¦‚ä½•åœ¨æµ‹è¯•ä¸‹åˆ›å»ºå’Œéƒ¨ç½²æ§åˆ¶å™¨ï¼Œä»¥åŠä¸€ç³»åˆ—çš„æµ‹è¯•å·¥ä½œæµï¼ˆæˆç†Ÿçš„æ§åˆ¶å™¨ä¸€èˆ¬éƒ½æœ‰ï¼‰ã€‚</p><p><strong>insight</strong>ï¼šè®¤ä¸ºstate-centric interfaceséå¸¸é€‚åˆå»è§‚å¯Ÿå’Œå¹²æ‰°æ§åˆ¶å™¨çš„viewã€‚<strong>æ§åˆ¶å™¨çš„åŠ¨ä½œæ˜¯å½“å‰å®ƒçœ‹åˆ°çš„é›†ç¾¤çŠ¶æ€çš„ä¸¥æ ¼å‡½æ•°</strong>ã€‚</p><p>1ï¼‰ è‡ªåŠ¨çš„ä¿®æ”¹äº†æ§åˆ¶å™¨å£°ç§°æ”¯æŒçš„è§‚æµ‹åˆ°çš„é›†ç¾¤çŠ¶æ€ã€‚</p><p>2ï¼‰è‡ªåŠ¨çš„ä½¿ç”¨ç”Ÿæˆçš„æµ‹è¯•è®¡åˆ’ï¼Œæ ‡è®°å®‰å…¨å’Œlivenessçš„é—®é¢˜ã€‚ï¼ˆå¯ä»¥è¿™ä¹ˆåšæ˜¯ç”±äºstate-centric interfacesçš„ä¸€äº›ç‰¹ç‚¹ï¼Œè¿™æ ·çš„ç³»ç»Ÿå¾€å¾€<strong>æœ‰ç®€å•çš„é«˜åº¦å†…èšçš„</strong>state centricçš„æ¥å£ï¼Œè¿™ç§æ¥å£åŸºæœ¬ä¸Šåªåšè¯»å†™å’Œæ¥æ”¶çŠ¶æ€æ”¹å˜çš„æ“ä½œï¼Œå¹¶ä¸”æ‰€æœ‰çš„å¯¹è±¡å…±äº«ç­–ç•¥ï¼ˆä¾‹å¦‚k8sé‡Œé¢å°±ç”¨åŒæ ·çš„åŸŸå»è¡¨ç¤ºmetadataï¼‰ï¼‰ã€‚</p><p><strong>æµ‹è¯•æ–¹æ³•ï¼š</strong>å¹²æ‰°æ§åˆ¶å™¨çš„viewï¼Œæ’å…¥1ï¼‰ä¸­é—´çŠ¶æ€ 2ï¼‰å·²ç»å¤±æ•ˆçš„çŠ¶æ€ 3ï¼‰æœªè§‚æµ‹çš„çŠ¶æ€ï¼Œæ ¹æ®æ§åˆ¶å™¨çš„çŠ¶æ€å’Œè¡Œä¸ºå»ç”Ÿæˆ<strong>æµ‹è¯•è®¡åˆ’</strong>ï¼Œé¿å…å†—ä½™å’Œæ— æ•ˆçš„æµ‹è¯•è®¡åˆ’ã€‚</p><h2 id="Bg"><a href="#Bg" class="headerlink" title="Bg"></a>Bg</h2><p><strong>k8sçš„æ¶æ„</strong>ï¼š</p><p><img src="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/123-1667563760827-2.png" class="lazyload placeholder" data-srcset="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/123-1667563760827-2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>k8sçš„æ ¸å¿ƒç”±APIæœåŠ¡å™¨çš„é›†æˆå’Œé«˜å¯ç”¨ã€é«˜ä¸€è‡´æ€§çš„æ•°æ®å­˜å‚¨ï¼ˆetcdï¼‰ç»„æˆï¼Œetcdé‡Œé¢å­˜å‚¨äº†<strong>å¯¹è±¡çŠ¶æ€ï¼ˆpodsã€volumnsã€nodesã€groups of applicationsï¼‰</strong>ï¼Œæ‰€æœ‰k8sä¸­çš„å…¶ä»–ç»„ä»¶éƒ½å’Œk8sè¿›è¡Œäº¤äº’ã€‚æ§åˆ¶å™¨è°ƒç”¨API Serverä¸­çš„APIå»è·å¾—Objectçš„çŠ¶æ€ã€‚</p><p>å¥½å¤„ï¼šå¯æ‹“å±•æ€§å¼ºï¼Œå¢åŠ æ–°çš„åº”ç”¨çš„æ—¶å€™åªéœ€è¦å¢åŠ æ–°çš„æ§åˆ¶å™¨å’Œæ–°çš„State Objectã€‚</p><p><strong>k8sçš„æ§åˆ¶å™¨çš„å·¥ä½œæ–¹å¼</strong>ï¼š</p><p><img src="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/123-1667564276682-4.png" class="lazyload placeholder" data-srcset="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/123-1667564276682-4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>éƒ¨ç½²ZooKeeperé›†ç¾¤çš„æ—¶å€™ï¼Œç”¨æˆ·<strong>é¦–å…ˆåˆ›å»º</strong>ä¸€ä¸ªZooKeeperçš„å¯¹è±¡ï¼Œè¿™ä¸ª<strong>å¯¹è±¡</strong>æŒ‡å®šäº†ç”¨æˆ·æƒ³è¦çš„é›†ç¾¤çš„çŠ¶æ€ï¼Œä¾‹å¦‚å¤šå°‘ä¸ªï¼Œç‰ˆæœ¬ï¼Œå­˜å‚¨çš„å¤§å°ï¼Œç„¶å<strong>ZooKeeper Controller</strong>æ¥å—åˆ°ZooKeeperå¯¹è±¡è¢«åˆ›å»ºçš„æ¶ˆæ¯ï¼Œå°±æƒ³æ–¹è®¾æ³•è¾¾åˆ°ç”¨æˆ·åˆ¶å®šçš„çŠ¶æ€ï¼Œé¦–å…ˆåˆ›å»ºä¸€ä¸ªStatefulSetçš„å¯¹è±¡ï¼ˆæœ‰çŠ¶æ€åº”ç”¨çš„æŠ½è±¡ï¼‰ï¼Œç„¶åä¸€ä¸ªa StatefulSet controlleréšåè¢«å‘ŠçŸ¥ä¸€ä¸ªStatefulSetçš„å¯¹è±¡è¢«åˆ›å»ºäº†ï¼Œç„¶åè½®æµçš„åˆ›å»ºpodå’Œvolumnï¼Œä¹‹åschedulerï¼Œstorage controllerï¼Œworkeréƒ½è¢«åˆ›å»ºå»å¸¦æ¥å®é™…çš„containerå’Œvolumnã€‚å¦‚æœè¿™ä¸ªæ—¶å€™ç”¨æˆ·ç¼–è¾‘äº†ZooKeeperå¯¹è±¡æ—¶å€™ï¼Œæ¯ä¸€ä¸ªæ§åˆ¶å™¨å°±éƒ½ä¼šå¾®è°ƒè¾¾åˆ°å¯¹åº”çš„çŠ¶æ€ã€‚</p><p><strong>è¿™ä¸ªæµ‹è¯•å¾ˆé‡è¦ï¼Œæ§åˆ¶å™¨çš„å¯é æ€§å¾ˆéš¾ä¿è¯ï¼Œå½“å‰çš„æµ‹è¯•æ–¹æ³•ä¸è¡Œ</strong>ã€‚</p><h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p><strong>Sieve tests controllers with the following workflowï¼š</strong></p><ul><li>Collecting reference tracesï¼šå­¦ä¹ æ§åˆ¶å™¨åœ¨æ²¡æœ‰é”™è¯¯æ—¶å€™çš„è¡¨ç°ï¼ˆæµ‹è¯•å·¥ä½œæµä¸‹ï¼‰ï¼Œè®°å½•è¿™ä¸ªæ—¶å€™çš„çŠ¶æ€è½¬ç§»ï¼Œä»è€Œå¯¹æ§åˆ¶å™¨å’Œé›†ç¾¤çŠ¶æ€äº¤äº’çš„æ¥å£è¿›è¡Œæ£€æµ‹ã€‚</li><li>ç”Ÿæˆæµ‹è¯•è®¡åˆ’ï¼šæ ¹æ®å‚è€ƒè½¨è¿¹ç”Ÿæˆæµ‹è¯•è®¡åˆ’ï¼ˆæè¿°äº†å…·ä½“çš„å¹²æ‰°ï¼‰ï¼Œæè¿°äº†è¦æ³¨å…¥ä»€ä¹ˆé”™è¯¯ï¼Œä»€ä¹ˆæ—¶å€™æ³¨å…¥é”™è¯¯ã€‚</li><li>é¿å…æ— æ•ˆçš„æµ‹è¯•è®¡åˆ’ï¼šåˆ é™¤å¤šä½™æˆ–è€…æ— ç”¨çš„æµ‹è¯•è®¡åˆ’ï¼ˆä¾‹å¦‚æ˜æ˜¾ä¸ä¼šå¯¼è‡´é”™è¯¯çš„æµ‹è¯•è®¡åˆ’ï¼‰ã€‚</li><li>æ‰§è¡Œæµ‹è¯•è®¡åˆ’ï¼šä½¿ç”¨test coordinatorï¼Œæ‰§è¡Œæ¯ä¸€ä¸ªæµ‹è¯•è®¡åˆ’ï¼Œ<strong>test coordinator</strong>ç›‘æ§é›†ç¾¤æµ‹è¯•æ—¶å€™çš„çŠ¶æ€è½¬å˜ï¼Œæ³¨å…¥é”™è¯¯ã€‚</li><li>æ£€æŸ¥æµ‹è¯•ç»“æœï¼šgeneric, effective, differential oracles to automatically check test results</li></ul><h4 id="å¦‚ä½•Perturbingæ§åˆ¶å™¨çš„state-view"><a href="#å¦‚ä½•Perturbingæ§åˆ¶å™¨çš„state-view" class="headerlink" title="å¦‚ä½•Perturbingæ§åˆ¶å™¨çš„state view"></a>å¦‚ä½•Perturbingæ§åˆ¶å™¨çš„state view</h4><p>æ³¨å…¥ç‰¹å®šçš„é”™è¯¯ï¼ˆcrashã€delayã€connection changeï¼‰ã€‚</p><p><strong>decouple policy from mechanism</strong>ï¼šæœ‰åˆ©äºæ‹“å±•å½“å‰çš„ç­–ç•¥ï¼Œé€šè¿‡ç¼–æ’æ½œåœ¨çš„å¹²æ‰°æœºåˆ¶å¢åŠ æ–°çš„ç­–ç•¥ã€‚<strong>ç­–ç•¥</strong>ï¼ša view Sieve exposes to the controller at a particular condition. <strong>æœºåˆ¶</strong>ï¼šè¯´æ˜äº†å¦‚ä½•æ³¨å…¥é”™è¯¯å»create a viewã€‚</p><h4 id="è¿™ä¸‰ç§å¹²æ‰°åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"><a href="#è¿™ä¸‰ç§å¹²æ‰°åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ" class="headerlink" title="è¿™ä¸‰ç§å¹²æ‰°åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"></a>è¿™ä¸‰ç§å¹²æ‰°åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ</h4><p><strong>Intermediate states</strong>ï¼šä¸­é—´çŠ¶æ€æŒ‡çš„æ˜¯æ§åˆ¶å™¨è¿˜æ²¡å®Œæˆæ‰€æœ‰çŠ¶æ€æ›´æ–°ä¹‹å‰ä¸­é—´çš„ä¸€äº›çŠ¶æ€ï¼Œåœ¨controllerå¤±è´¥åï¼Œk8så°±ä¼šå¼€ä¸€ä¸ªæ–°çš„å®ä¾‹ï¼Œæ¢å¤ä¹‹å‰çš„ä¸­é—´çŠ¶æ€ã€‚</p><p><img src="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/Snipaste_2022-11-04_21-35-34.png" class="lazyload placeholder" data-srcset="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/Snipaste_2022-11-04_21-35-34.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>ä¾‹å¦‚RabbitMQæ§åˆ¶å™¨ï¼Œæ‰¾åˆ°äº†ä¸€ä¸ªbugï¼š</p><p>workloadï¼šå°è¯•å°†å­˜å‚¨ä»10GB-&gt;15GB,1ï¼‰é¦–å…ˆæ›´æ–°VolCurä¸º15GBï¼Œç„¶åæ›´æ–°VolReqä¸º15GBï¼ˆè§¦å‘k8sï¼‰resizeå¤§å°ã€‚åœ¨æ›´æ–°VolCurå’ŒVolReqçš„æ—¶å€™SieveæŒ‚äº†ï¼Œä½†æ˜¯å´ä¸èƒ½æ­£ç¡®æ¢å¤ï¼Œå·²ç»è¢«700è¡Œgoä»£ç ä¿®å¤äº†ã€‚</p><p><strong>Stale states</strong>ï¼š</p><p>å›¾2å¯ä»¥çœ‹å‡ºæ¥ï¼Œæ§åˆ¶å™¨ä¸ç›´æ¥å’Œconsist data storeså»äº¤äº’ï¼Œè€Œæ˜¯å’ŒAPI serverå»äº¤äº’ï¼ŒAPI serverçš„è¯é‡Œé¢çš„çŠ¶æ€å¯èƒ½ä¼šå—åˆ°delayed notificationsçš„å½±å“ã€‚</p><p>å‡å¦‚æœ‰å¤šä¸ªAPI Serveréƒ½å¯ç”¨çš„æ—¶å€™ï¼Œå¯èƒ½æœ‰ä¸€äº›API Serverçš„çŠ¶æ€æ¯”è¾ƒæ–°ï¼Œæœ‰çš„çŠ¶æ€ä¸æ˜¯å¾ˆæ–°ï¼Œè¦æ˜¯æ§åˆ¶å™¨åˆšå¼€å§‹è¿çš„æ˜¯ä¸€ä¸ªæ¯”è¾ƒæ–°çš„API Serverï¼Œç„¶ååˆè¿æ¥äº†ä¸€ä¸ªæ¯”è¾ƒæ—§çš„Serverï¼ˆç”±äºè´Ÿè½½å‡è¡¡ç­‰åŸå› ï¼‰ï¼Œæ§åˆ¶å™¨å¯èƒ½ä¼šåšé‡æ–°é…ç½®ï¼Œä½†æ˜¯æ§åˆ¶å™¨ä¸åº”è¯¥è¿™ä¹ˆåšï¼Œåº”è¯¥æ­£ç¡®è¯†åˆ«è¿™äº›é”™è¯¯ã€‚</p><p>Perconaâ€™s MongoDBï¼Œæ‰¾åˆ°ä¸€ä¸ªæ–°bugï¼š</p><p><img src="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/Snipaste_2022-11-04_21-45-19.png" class="lazyload placeholder" data-srcset="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/Snipaste_2022-11-04_21-45-19.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>å…³é—­MongoDBé›†ç¾¤çš„æ—¶å€™ï¼Œcontrollerç­‰ç€çœ‹è§MongoDBçš„state objectçš„åˆ é™¤çš„æ—¶é—´æˆ³ï¼Œæ§åˆ¶å™¨çœ‹åˆ°è¿™ä¸ªå˜åŒ–çš„æ—¶å€™ï¼Œå°±ä¼šåˆ é™¤æ‰€æœ‰çš„podså’Œvolumesã€‚</p><p>Sieveè®©è¿™ä¸ªæ§åˆ¶å™¨å°±é”™è¯¯çš„åˆ é™¤äº†ä¸€ä¸ªliveçš„MongoDBçš„é›†ç¾¤ã€‚æœ‰ä¸€ä¸ªå·¥ä½œæµé¦–å…ˆå…³é—­äº†MongoDBçš„é›†ç¾¤ç„¶åé‡æ–°åˆ›å»ºäº†ä¸€ä¸ªåŒæ ·çš„é›†ç¾¤ï¼Œç­‰åˆ°æ–°çš„é›†ç¾¤åˆ›å»ºå®Œæˆåï¼ŒSieveå°±å¼•å…¥äº†ä¸€ä¸ªè¿™æ ·çš„é”™è¯¯ï¼Œä½¿å¾—controlleræŠŠè¿™ä¸ªé›†ç¾¤ç»™åˆ äº†ã€‚</p><p><strong>Unobserved states</strong>ï¼šç°åœ¨çš„controllerè®¾è®¡æˆlevel-triggered systems(opposed to being edge-triggered)ï¼Œå’Œåˆ«çš„è®¾è®¡ä¸åŒçš„æ˜¯ï¼Œåˆ«çš„è®¾è®¡è§‚æµ‹æ‰€æœ‰çš„é›†ç¾¤çŠ¶æ€çš„æ”¹å˜ï¼Œåšè¿™æ‰€æœ‰çš„çŠ¶æ€çš„æ”¹å˜ï¼Œä½†æ˜¯level-triggered systemså°±åªæ ¹æ®å½“å‰çš„çŠ¶æ€å»æ§åˆ¶è½¬å˜é›†ç¾¤çš„çŠ¶æ€.</p><p>Instaclustrâ€™s Cassandra controllerï¼Œæ‰¾åˆ°äº†ä¸€ä¸ªbugå¯¼è‡´èµ„æºæ³„éœ²å’ŒæœåŠ¡å¤±è´¥ï¼š</p><p><img src="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/Snipaste_2022-11-04_21-56-52.png" class="lazyload placeholder" data-srcset="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/Snipaste_2022-11-04_21-56-52.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p>åœ¨scale-downçš„æ—¶å€™ï¼Œcontrolleråº”è¯¥å¾—çŸ¥podsè¢«åˆ é™¤åç§»é™¤äº†æ‰€æœ‰çš„volumnsï¼Œè€Œpodsçš„ç”Ÿå‘½å‘¨æœŸæ˜¯ç”±StatefulSet Controllerç®¡ç†çš„ï¼ŒSieveå°±æš‚åœäº†äº†å‘é€ç»™Cassandraæ§åˆ¶å™¨çš„æ¶ˆæ¯ï¼Œå› æ­¤Cassandraä¸çŸ¥é“podsè¢«åˆ é™¤äº†ï¼Œè¿™å¯¼è‡´Cassandraæ§åˆ¶å™¨æ²¡æœ‰åˆ é™¤å¯¹åº”çš„volumnsã€‚</p><h4 id="å¦‚ä½•æ”¶é›†Reference-Traces"><a href="#å¦‚ä½•æ”¶é›†Reference-Traces" class="headerlink" title="å¦‚ä½•æ”¶é›†Reference Traces"></a>å¦‚ä½•æ”¶é›†Reference Traces</h4><p>é€šè¿‡å°è£…API Serverçš„å‡½æ•°ï¼Œå°è£…äº†10ä¸ªå‡½æ•°ï¼Œåœ¨è¿™é‡Œé¢æ³¨å…¥å¹²æ‰°ã€‚</p><p>ç”±æ­¤è·‘ä¸€ä¸‹æ‰€æœ‰å¼€å‘è€…æä¾›çš„workloadï¼Œé€šè¿‡å­¦ä¹ æ¯ä¸€ä¸ªcontrolleræ”¶åˆ°çš„é›†ç¾¤çŠ¶æ€æ”¹å˜çš„æé†’ï¼Œå’Œæ§åˆ¶å™¨å¯¹é›†ç¾¤çŠ¶æ€çš„ä»»ä½•è¯»å’Œå†™çš„æ“ä½œï¼Œæˆ–è€…æ˜¯å¯¹API Serverçš„clientï¼ˆä¹Ÿå°±æ˜¯controlleræœºå™¨è‡ªå·±ï¼‰ç»´æŠ¤çš„local cacheå»æ”¶é›†ä¸¤æ–¹é¢çš„reference tracesï¼š</p><ul><li><strong>Controller trace</strong>ï¼šä¸€ç³»åˆ—ä½¿ç”¨clientçš„APIå°±å¯ä»¥è§‚æµ‹çš„æ—¶é—´ï¼Œä¾‹å¦‚çŠ¶æ€æ”¹å˜çš„notificationï¼Œreconciliation cycleçš„entryå’Œexitsï¼Œclient-APIçš„invocationï¼ˆè°ƒç”¨ï¼‰</li><li><strong>Cluster state trace</strong>ï¼šåˆå§‹åŒ–çš„é›†ç¾¤çŠ¶æ€å’ŒçŠ¶æ€æ”¹å˜çš„åºåˆ—ã€‚</li></ul><h4 id="å¦‚ä½•ä½¿ç”¨Reference-Tracesç”Ÿæˆæµ‹è¯•è®¡åˆ’ï¼š"><a href="#å¦‚ä½•ä½¿ç”¨Reference-Tracesç”Ÿæˆæµ‹è¯•è®¡åˆ’ï¼š" class="headerlink" title="å¦‚ä½•ä½¿ç”¨Reference Tracesç”Ÿæˆæµ‹è¯•è®¡åˆ’ï¼š"></a>å¦‚ä½•ä½¿ç”¨Reference Tracesç”Ÿæˆæµ‹è¯•è®¡åˆ’ï¼š</h4><p>æ¯ä¸€ä¸ªæµ‹è¯•è®¡åˆ’åšä¸€ç§å¹²æ‰°ã€‚</p><p><strong>æµ‹è¯•è®¡åˆ’</strong>ï¼šæ¯ä¸€ä¸ªæµ‹è¯•è®¡åˆ’ç”±self-containedçš„æ–‡ä»¶æ„æˆï¼Œè¿™ä¸ªæ–‡ä»¶æè¿°äº†<strong>æµ‹è¯•å·¥ä½œæµã€ä¸€äº›åˆ—çš„æ³¨å…¥çš„é”™è¯¯ã€æ³¨å…¥é”™è¯¯è¿™ä¸€è¡Œä¸ºçš„è§¦å‘æ¡ä»¶</strong>ã€‚</p><p>ç›®å‰æ”¯æŒï¼š</p><ul><li>crash/restart a controller</li><li>disconnect/reconnect a controller to an API server</li><li>block/unblock a controller from processing events</li><li>block/unblock an API server from processing events</li></ul><p>æ ¹æ®æµ‹è¯•è®¡åˆ’å°±å¯ä»¥å¤ç°bugã€‚</p><p><img src="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/image-20221104221955496.png" class="lazyload placeholder" data-srcset="/2022/10/28/osdi22-automatic-reliability-testing-for-cluster-management-controllers/image-20221104221955496.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20221104221955496"></p><h4 id="æµ‹è¯•è®¡åˆ’ç”Ÿæˆ"><a href="#æµ‹è¯•è®¡åˆ’ç”Ÿæˆ" class="headerlink" title="æµ‹è¯•è®¡åˆ’ç”Ÿæˆ"></a>æµ‹è¯•è®¡åˆ’ç”Ÿæˆ</h4><p>åˆ¶å®šäº†ä¸€ç³»åˆ—è§„åˆ™ï¼Œåœ¨è¿™äº›è§„åˆ™é‡Œç”Ÿæˆè®¡åˆ’ï¼Œ<strong>è¿™ä¸ªè¯´æ³•å¾ˆæœ‰æ„æ€ï¼Œä¸è¯´å¦‚ä½•åˆ¶å®šæµ‹è¯•çŠ¶æ€ï¼Œå› ä¸ºå¤§å®¶çœ‹åˆ°ä½ æ˜¯æ€ä¹ˆåˆ¶å®šçš„å°±æ„Ÿè§‰å°±è¿™ï¼Œä½†æ˜¯é¿å…æ— ç”¨çš„æµ‹è¯•è®¡åˆ’è¿™ä¸ªè¯´æ³•è®©æˆ‘æ„Ÿè§‰å¾ˆç‰›</strong>ã€‚</p><ul><li><strong>Intermediate-state rule</strong>ï¼šé›†ç¾¤ä»ä¸€ä¸ªçŠ¶æ€æ›´æ–°åˆ°å¦ä¸€ä¸ªçŠ¶æ€è¦è‹¥å¹²æ­¥éª¤ï¼Œå¯¹äºè‹¥å¹²ä¸ªæ­¥éª¤$U_{1}, U_{2}â€¦$ï¼Œåšä¸€ä¸ªæ­¥éª¤å°±è®©Controllerå´©æºƒä¸€éï¼Œç”Ÿæˆä¸€ä¸ªæµ‹è¯•è®¡åˆ’ã€‚</li><li><strong>Stale-state rule</strong>ï¼šä¸åœçš„è®©controller travel back in timeï¼Œç„¶åç»™ä»–çœ‹ä»¥å‰çš„çŠ¶æ€ï¼Œæ–¹æ³•å°±æ˜¯ä¸æ–­çš„åšä¸€äº›å¯èƒ½ä¼šå¸¦æ¥å†²çªæ•ˆæœçš„æ“ä½œï¼Œå»è§‚å¯Ÿcontrolleræ˜¯å¦æ­£å¸¸ã€‚å…·ä½“çš„åšæ³•æ˜¯ï¼Œé¦–å…ˆè§‚æµ‹åˆ°æ›´æ–°Uçš„Nä¸ªç»“æœï¼Œç„¶åæœç´¢ä¹‹åå¯èƒ½çš„Nâ€™ä¸ªç»“æœï¼Œå°½é‡ç”¨ä¾‹å¦‚å…ˆåˆ äº†ä¸€ä¸ªobjectï¼Œå†åˆ›å»ºä¸€ä¸ªobjectè¿™æ ·å¯èƒ½ä¼šå¯¼è‡´å†²çªçš„æ“ä½œã€‚</li><li><strong>Unobserved-state rule</strong>ï¼šè·³è¿‡å¯èƒ½çš„æ­£å¸¸çŠ¶æ€çš„æ”¹å˜ï¼Œå¯¹äºçŠ¶æ€å¯¹(N, Nâ€™)ï¼Œè¿™æ ·ç”Ÿæˆæµ‹è¯•è®¡åˆ’1ï¼‰å…ˆä¸è®©æ§åˆ¶å™¨çœ‹åˆ°çŠ¶æ€N 2ï¼‰å½“Nâ€™åˆ°æ¥çš„æ—¶å€™è®©æ§åˆ¶å™¨çœ‹åˆ°Nã€‚</li></ul><h4 id="å¦‚ä½•é¿å…æ— ç”¨çš„æµ‹è¯•è®¡åˆ’"><a href="#å¦‚ä½•é¿å…æ— ç”¨çš„æµ‹è¯•è®¡åˆ’" class="headerlink" title="å¦‚ä½•é¿å…æ— ç”¨çš„æµ‹è¯•è®¡åˆ’"></a>å¦‚ä½•é¿å…æ— ç”¨çš„æµ‹è¯•è®¡åˆ’</h4><p>ä¾æ®<strong>æµ‹è¯•è®¡åˆ’ç”Ÿæˆäº§ç”Ÿäº†å¤§é‡çš„æµ‹è¯•è®¡åˆ’</strong>ï¼Œå¤ªå¤šäº†ï¼Œå…‰stale-stateå°±åœ¨MongoDB Controllerä¸Šç”Ÿæˆäº†140000+æµ‹è¯•è®¡åˆ’ã€‚</p><p><strong>a guiding principle</strong>ï¼š</p><ul><li>prune a test plan if the test plan does not introduce an intermediate-, a stale- or an unobservedstate that can affect the controllerâ€™s outputs.</li><li>the introduced state is identical to states introduced by other test plans.</li></ul><p><strong>3.4.1 Pruning by Causality</strong></p><p>æ ¹æ®<strong>å› æœå…³ç³»</strong>ï¼ˆæ›´æ–°Uæ˜¯åŸºäºçŠ¶æ€Nåšå‡ºæ¥çš„ï¼Œé‚£ä¹ˆNå’ŒUæ˜¯å› æœç›¸å…³çš„ï¼‰ç­›é€‰ã€‚å› æœå…³ç³»å¾ˆéš¾åšã€ç°åœ¨çš„k8sä¸å¥½åšå› æœåˆ†æã€‚</p><p>æ‰€ä»¥åˆ¶å®šäº†<strong>ä¸¤ä¸ªè§„åˆ™ï¼ˆä¸ªäººè®¤ä¸ºï¼šåŸºäºå±€éƒ¨æ€§åŸç†ï¼‰</strong>ï¼Œå¯èƒ½ä¼šå¯¼è‡´<strong>false positive</strong>çš„é”™è¯¯ï¼š</p><ul><li>Read-before-update ruleï¼šthe object pertaining to N is read by the controller before it issues Uã€‚æ›´æ–°Uæ˜¯åœ¨è¯»å–Nä¹‹åä½œå‡ºçš„ã€‚<strong>ä¸ªäººç†è§£ï¼š</strong>åˆšè¯»è¿‡çŠ¶æ€å°±å»æ›´æ–°ï¼Œé‚£ä¹ˆæ›´æ–°Uå¤§æ¦‚ç‡å’ŒNç›¸å…³ã€‚</li><li>Earliest-reconciliation ruleï¼šN and U happen in the same or adjacent reconciliation cycles.  Nå’ŒUå‘ç”Ÿåœ¨ç›¸åŒæˆ–è€…ç›¸è¿‘çš„reconciliationå‘¨æœŸä¸­ï¼Œå¯ä»¥ç†è§£ã€‚</li></ul><p><strong>ä¾‹å¦‚åªä¿ç•™è‡³å°‘ä¸€ä¸ªUå’ŒNå› æœç›¸å…³çš„æµ‹è¯•è®¡åˆ’</strong>ã€‚</p><p><strong>3.4.2 Pruning Unsuccessful Updates</strong></p><p><strong>å¿½ç•¥ä»»ä½•ä¸æ”¹å˜å½“å‰é›†ç¾¤çŠ¶æ€çš„æ›´æ–°</strong>ã€‚</p><p>ç†ç”±å°±æ˜¯ï¼šå¦‚æœä¸€ä¸ªæ›´æ–°æ²¡æœ‰æ”¹å˜å½“å‰çš„é›†ç¾¤çš„çŠ¶æ€ï¼Œå°±ä¸å¤§å¯èƒ½å¯¼è‡´æ–°çš„çŠ¶æ€å‡ºç°ã€‚</p><h4 id="Test-Plan-Execution"><a href="#Test-Plan-Execution" class="headerlink" title="Test Plan Execution"></a>Test Plan Execution</h4><p>ç”±Sieveæµ‹è¯•åè°ƒå™¨æ‰§è¡Œã€‚</p><h4 id="Differential-Test-Oracles"><a href="#Differential-Test-Oracles" class="headerlink" title="Differential Test Oracles"></a>Differential Test Oracles</h4><p><strong>3.6.1 Checking End States</strong></p><p><strong>3.6.2 Checking State-Update Summaries</strong></p>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About Serverless </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SysML19-TOWARDS FEDERATED LEARNING AT SCALE: SYSTEM DESIGN</title>
      <link href="/2022/10/28/sysml19-towards-federated-learning-at-scale-system-design/"/>
      <url>/2022/10/28/sysml19-towards-federated-learning-at-scale-system-design/</url>
      
        <content type="html"><![CDATA[<h1 id="SysML19-TOWARDS-FEDERATED-LEARNING-AT-SCALE-SYSTEM-DESIGN"><a href="#SysML19-TOWARDS-FEDERATED-LEARNING-AT-SCALE-SYSTEM-DESIGN" class="headerlink" title="SysML19-TOWARDS FEDERATED LEARNING AT SCALE: SYSTEM DESIGN"></a>SysML19-TOWARDS FEDERATED LEARNING AT SCALE: SYSTEM DESIGN</h1><p>Androidâ€™s AIDL IPC mechanismï¼Ÿ</p><h2 id="æ¦‚è¿°"><a href="#æ¦‚è¿°" class="headerlink" title="æ¦‚è¿°"></a>æ¦‚è¿°</h2><p>æŠŠä»£ç æ”¾æ•°æ®é‚£ï¼Œå¤„ç†çš„é—®é¢˜ï¼šprivacy, ownership, the locality of data</p><h2 id="éƒ¨åˆ†ç›¸å…³å·¥ä½œ"><a href="#éƒ¨åˆ†ç›¸å…³å·¥ä½œ" class="headerlink" title="éƒ¨åˆ†ç›¸å…³å·¥ä½œ"></a>éƒ¨åˆ†ç›¸å…³å·¥ä½œ</h2><ol><li><p>general description of FL: McMahan &amp; Ramage (2017)</p></li><li><p>theory of FL: KoneË‡ cn  Ì y et al. (2016a), McMahan et al. (2017; 2018)</p></li><li><p>Federated Learning infrastructure:</p><ol><li>whether to focus on asynchronous or synchronous training algorithms: asynchronous training: Dean et al. (2012)</li><li>a consistent trend towards synchronous large batch training: (data center)Goyal et al., 2017; Smith et al., 2018</li><li>(Federated Averaging algorithm)McMahan et al. (2017)</li><li>enhancing privacy guarantees: (differential privacy)McMahan et al., 2018, (Secure Aggregation)Bonawitz et al., 2017</li></ol></li><li><p>Our system:</p><ol><li>Federated Averaging</li><li>Secure Aggregation: ensures that on a global level individual updates from phones are uninspectable</li><li>phone keyboard, tens of millions of real-world devices</li><li>issues: <ol><li>device availability that correlates with the local data distribution in complex ways</li><li>unreliable device connectivity and interrupted execution</li><li>orchestration of lock-step execution across devices with varying availability</li><li>limited device storage and compute resources</li></ol></li></ol></li></ol><h2 id="PROTOCOL"><a href="#PROTOCOL" class="headerlink" title="PROTOCOL:"></a>PROTOCOL:</h2><ol><li><p>è®­ç»ƒè¿‡ç¨‹ï¼šdeviceè®­ç»ƒå®Œæ¯•æ—¶ï¼ŒæœåŠ¡å™¨å‘deviceå‘é€æœ¬è½®çš„å…¨å±€å‚æ•°å’Œå…¶ä»–å¿…è¦çš„çŠ¶æ€ä¾‹å¦‚FL Checkpointï¼Œæ¯ä¸ªå‚ä¸è€…åœ¨æœ¬åœ°æ ¹æ®å…¨å±€çš„çŠ¶æ€å’Œæœ¬åœ°çš„æ•°æ®é›†è¿›è¡Œæ¢¯åº¦çš„è®¡ç®—ï¼Œå°†æ›´æ–°çš„æ•°æ®å‘é€ç»™æœåŠ¡å™¨ï¼ŒæœåŠ¡å™¨å†å°†è¿™äº›æ›´æ–°å…¨éƒ¨èšåˆã€‚</p></li><li><p>é€‰æ‹©ï¼šè®¾å¤‡çš„é€‰æ‹©ï¼Œæ»¡è¶³ä¸€å®šçš„æ¡ä»¶ï¼ˆä¾‹å¦‚å……ç”µå’Œä½¿ç”¨wifiï¼‰ï¼Œä¸€è½®é€‰æ‹©å°å‡ ç™¾çš„æ•°æ®é‡ï¼Œæ»¡è¶³æ¡ä»¶åå°±å’ŒæœåŠ¡å™¨å»ºç«‹èµ·ä¸€ä¸ªåŒå‘çš„è¿æ¥ï¼Œç”¨äºæ£€æŸ¥è®¾å¤‡çš„livenesså¹¶è¿›è¡Œmulti-stepçš„äº¤æµã€‚ï¼ˆé€‰æ‹©çš„æ•°é‡å¤§æ¦‚å°å‡ ç™¾ï¼‰ã€‚æœªè¿æ¥çš„è®¾å¤‡æœåŠ¡å™¨å‘Šè¯‰ä»–ç¨åè¿æ¥ã€‚è¾¾åˆ°è¶³å¤Ÿæ•°é‡çš„è®¾å¤‡æœ¬è½®æ‰ç®—æ˜¯æˆåŠŸã€‚</p></li><li><p>Pace steering: ç»™deviceå‘é€é‡è¿çš„æ—¶é—´çª—å£ï¼ŒFL Populationså°‘çš„æ—¶å€™ï¼ŒæœåŠ¡å™¨ä½¿ç”¨æ— çŠ¶æ€çš„æ¦‚ç‡ç®—æ³•æ‹’ç»ä¸€äº›è®¾å¤‡ä»¥ç¡®ä¿checkinåŒæ—¶åˆ°è¾¾ã€‚å¤šçš„æ—¶å€™ï¼Œä½¿ç”¨éšæœºçš„è®¾å¤‡check inæ—¶é—´ï¼Œé¿å…äº†thundering herdé—®é¢˜ï¼Œå‘Šè¯‰è®¾å¤‡æŒ‰éœ€è¿æ¥ã€‚</p></li><li><p>æœåŠ¡å™¨</p><ol><li>å¤„ç†10ä¸ª-ä¸Šäº¿ä¸ªè®¾å¤‡çš„æƒ…å†µï¼Œå¤„ç†10ä¸ª-å‡ ä¸‡ä¸ªå‚ä¸è€…çš„æƒ…å†µï¼Œæ²¡ä¸€è½®çš„æ›´æ–°å¯èƒ½æ˜¯åƒKåˆ°å‡ åMã€‚</li><li>å¤šä¸ªCoordinatorsï¼šenable global synchronization and advancing rounds in lockstepï¼Œæ¯ä¸ªCoordinatorè´Ÿè´£ä¸€ä¸ªFL Populationï¼ŒCoordinatorç”¨åœ°å€å’ŒFL Populationçš„åå­—æ³¨å†Œä¸€ä¸ªå…±äº«é”æœåŠ¡ï¼Œä¸€ä¸ªFL Populationåªèƒ½æœ‰ä¸€ä¸ªownerã€‚æ¥æ”¶å¹¶å¤„ç†æ¯ä¸ªSelectoræœ‰å¤šå°‘è®¾å¤‡å‚ä¸çš„ä¿¡æ¯ï¼Œç”ŸæˆAggregatorå»ç®¡ç†ä¸åŒFLä»»åŠ¡çš„è½®æ¬¡ã€‚</li><li>Selectorï¼šå’Œdeviceäº¤æµï¼Œä¸æ–­æ¥å—Coordinatorçš„ä¿¡æ¯ï¼ˆFL Populationéœ€è¦å¤šå°‘è®¾å¤‡ï¼Œå¦‚ä½•å†³å®šè¦ä¸è¦ä¸€ä¸ªè®¾å¤‡ï¼‰ï¼Œå½“Aggregatorç”Ÿæˆåï¼Œå°†subsetçš„deviceså‘é€ç»™Aggregatorã€‚</li><li>Master Aggregatorï¼šç®¡ç†æ¯ä¸€ä¸ªFLä»»åŠ¡çš„è½®æ¬¡ï¼Œ</li><li>æµæ°´çº¿ä¼˜åŒ–ï¼Œè¿™æ ·çš„æ¶æ„ä½¿å¾—ä¸Šä¸€è½®çš„æŠ¥å‘Šå’Œä¸‹ä¸€è½®çš„é€‰æ‹©å¯ä»¥ä¸€èµ·åšã€‚</li></ol></li><li><p>è®¾å¤‡:</p><ol><li>æœåŠ¡å™¨åˆ†åä¸€ä¸ªa TensorFlow graph and instructions for how to execute itã€‚</li><li>åœ¨example storeé‡Œé¢é€‰æ‹©æ•°æ®çš„æ ‡å‡†ï¼Œå¦‚ä½•æ‰“åŒ…dataï¼Œè®¾å¤‡ä¸Šè·‘å¤šå°‘ä¸ªepochï¼Œè®¡ç®—å›¾ä¸­èŠ‚ç‚¹çš„æ ‡ç­¾ä¸ªæ•°ã€‚ç»´æŠ¤ä¸€ä¸ªexample storeï¼Œå®ç°æä¾›æ•°æ®çš„APIï¼Œä¸æ–­çš„ç§»é™¤æ—§çš„æ•°æ®ï¼Œå¹¶æŒ‰ç…§å¹³å°çš„æ¨èå¯¹æ•°æ®è¿›è¡ŒåŠ å¯†ã€‚</li><li>è®¾å¤‡ä¸­çš„æ¶æ„ï¼ˆå›¾2ï¼‰ï¼Œå¤šç§Ÿæˆ·ï¼Œå¯ä»¥è®­ç»ƒå¤šä¸ªFL Populationã€‚</li><li>ä¿æŠ¤FLä¸å—åˆ°ä¸å¯ä¿¡è®¾å¤‡çš„æ”»å‡»ï¼šä¸èƒ½ç”¨ç”¨æˆ·èº«ä»½æˆæƒï¼Œæ‰€ä»¥ç”¨äº†Androidâ€™s remote attestation mechanismï¼Œå¹¶ä¸”è¿™æ ·çš„æ–¹å¼ä¹Ÿæä¾›äº†data poisoningçš„ä¿æŠ¤(Bagdasaryan et al., 2018)ï¼Œ</li></ol></li><li><p>é€šè¿‡åˆ†ææ•°æ®å»ç›‘æµ‹Deviceçš„å¥åº·ï¼ˆå°†æ•°æ®ä¸Šä¼ åˆ°äº‘ï¼‰ï¼š</p><p>Deviceä¸Šä¼ ï¼šè®­ç»ƒè¢«æ¿€æ´»æ—¶è®¾å¤‡çš„çŠ¶æ€ï¼Œè·‘äº†å¤šé¢‘ç¹å’Œå¤šä¹…ï¼Œç”¨äº†å¤šå°‘å†…å­˜ï¼Œæœ‰ä»€ä¹ˆé”™è¯¯ï¼ŒOSç­‰ç­‰ï¼Œä¸åŒ…å«ç”¨æˆ·ä¿¡æ¯ï¼ˆPIIï¼‰ï¼Œè®°å½•è®¾å¤‡çš„çŠ¶æ€åºåˆ—ã€‚æœåŠ¡å™¨ç«¯è®°å½•ï¼šæ¯ä¸€ä¸ªè½®æ¬¡å¤šå°‘è®¾å¤‡è¢«æ¥å—å’Œæ‹’ç»äº†ç­‰ç­‰ã€‚</p></li></ol><h2 id="SECURE-AGGREGATIONï¼ˆreporting-phraseï¼‰"><a href="#SECURE-AGGREGATIONï¼ˆreporting-phraseï¼‰" class="headerlink" title="SECURE AGGREGATIONï¼ˆreporting phraseï¼‰"></a>SECURE AGGREGATIONï¼ˆreporting phraseï¼‰</h2><p>â€‹    æ¯ä¸ªè®¾å¤‡å‘é€çš„æ¢¯åº¦åŠ å¯†ï¼ŒæœåŠ¡å™¨ç«¯åªæœ‰åœ¨æ”¶é›†åˆ°è¶³å¤Ÿå¤šçš„æ•°æ®ä¹‹åæ‰è¿›è¡Œæ€»å’Œçš„è®¡ç®—ã€‚æ—¶é—´å¤æ‚åº¦éšç€ç”¨æˆ·æ˜¯O($ç”¨æˆ·æ•°^2$)ï¼Œé¦–å…ˆæ‰€æœ‰çš„Aggregatoråšå®‰å…¨èšåˆï¼ˆå›ºå®šçš„æ•°ç›®kï¼‰ï¼Œç„¶åMaster Aggregatorå†åšä¸€æ¬¡éå®‰å…¨èšåˆã€‚</p><h2 id="å·¥å…·å’Œå·¥ä½œæµï¼š"><a href="#å·¥å…·å’Œå·¥ä½œæµï¼š" class="headerlink" title="å·¥å…·å’Œå·¥ä½œæµï¼š"></a>å·¥å…·å’Œå·¥ä½œæµï¼š</h2><ol><li><p>ä¸èƒ½ç›´æ¥æ¨æµ‹æ¯ä¸€ä¸ªçš„è®­ç»ƒæ ·æœ¬? åšä¸€ä¸ªå·¥å…·å»æŸ¥çœ‹æµ‹è¯•å’Œæ¨¡æ‹Ÿçš„æ•°æ®</p></li><li><p>æ–°ä¿®æ”¹çš„æ¨¡å‹è¦ç¼–æˆFL Planå†æ”¾åˆ°æœåŠ¡å™¨ä¸Šè·‘ã€‚</p></li><li><p>æ¨¡å‹èµ„æºçš„è¯è´¹å’Œè¿è¡Œæ—¶å€™çš„å…¼å®¹æ€§è´¨å¿…é¡»è‡ªåŠ¨çš„ç”±åŸºç¡€æ¡†æ¶éªŒè¯ã€‚</p></li><li><p>å¦‚ä½•å»ºç«‹æ¨¡å‹ï¼Ÿ</p><ol><li>å®šä¹‰åœ¨æ‰‹æœºä¸Šè·‘ç€çš„FLä»»åŠ¡ï¼ˆè®­ç»ƒå’Œè¯„æµ‹ä»»åŠ¡ï¼‰ï¼Œå°±æ˜¯å®ç°å¯¹åº”çš„å‡½æ•°æ¥å£ï¼Œå®ç°è¾“å…¥å‘é‡åˆ°lossæˆ–è€…accuracyçš„æ˜ å°„ã€‚éƒ¨ç½²æ—¶å€™å°±ä½¿ç”¨è®¾å¤‡ä¸Š æ ·æœ¬å•†åº— æä¾›çš„æ•°æ®ã€‚</li><li>é™¤äº†å®ç°æ¯ä¸ªè®¾å¤‡çš„FLä»»åŠ¡ä¹‹å¤–ï¼Œè¿˜è¦ç»™å®šä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼šä¸€è½®å¤šå°‘ä¸ªè®¾å¤‡æ•ˆæœæœ€å¥½ï¼Œæ¨¡å‹çš„è¶…å‚ï¼ˆå­¦ä¹ ç‡ï¼‰ç­‰ç­‰ã€‚åŒæ—¶ï¼Œå¯ä»¥å®šä¹‰å¤šç»„FLtaskï¼Œè¿™åœ¨æ¯”å¦‚æ¢ç´¢ä»€ä¹ˆæ ·çš„å­¦ä¹ ç‡æ¯”è¾ƒå¥½ååˆ†æœ‰ç”¨ã€‚</li><li>ä½¿ç”¨æ¨¡å‹å·¥å…·å¯ä»¥æ¨¡æ‹Ÿ FLæœåŠ¡å™¨å’Œä¸€äº›åˆ—è®¾å¤‡ï¼Œä½¿ç”¨ä¸€äº›æ¨¡æ‹Ÿçš„æ•°æ®é›†ã€‚ è®­ç»ƒå‡ºæ¥çš„ç»“æœä¹Ÿå¯ç”¨ä½œé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚</li></ol></li><li><p>FL Planç”Ÿæˆï¼š</p><ol><li>FL Planè‡ªåŠ¨ç”±å‰é¢çš„æ¨¡å‹å’Œé…ç½®ç”Ÿæˆã€‚è¿™ä¸ªè®¡åˆ’ï¼Œå…¶å®å¯ä»¥ç”±ä¸€ä¸ªpythonç¨‹åºè¡¨ç¤ºï¼Œpythonç¨‹åºä¼šç¼–æ’ä¸€ä¸ªTFè®¡ç®—å›¾ã€‚</li><li>ç‰ˆæœ¬ã€æµ‹è¯•å’Œéƒ¨ç½²ç®¡ç†ï¼š<ol><li>æ¨¡å‹çš„æ›´æ–°ï¼š<ol><li>å¿…é¡»æ¥è‡ªå¯å®¡è®¡çš„ï¼ŒåŒè¡Œå®¡è®®è¿‡çš„ä»£ç </li><li>å¿…é¡»åœ¨æ¨¡æ‹Ÿæµ‹è¯•é›†ä¸Šå·²ç»ç»è¿‡æµ‹è¯•</li><li>ä½¿ç”¨çš„èµ„æºé™å®šåœ¨ä¸€å®šçš„èŒƒå›´å†…</li><li>å£°ç§°æ”¯æŒçš„tfä¸Šå…¨éƒ¨ç»è¿‡æµ‹è¯•ã€‚ä¸åŒç»ˆç«¯ä¸ŠTFç‰ˆæœ¬ä»£ç çš„å˜æ¢ï¼Œå’Œæ•°æ®ä¸­å¿ƒåŒ–çš„è®­ç»ƒä¸åŒçš„æ˜¯ï¼Œæ•°æ®ä¸­å¿ƒåŒ–çš„è®­ç»ƒå¯ä»¥ä¸€ç›´rebuildå›¾ï¼Œä½†æ˜¯ç»ˆç«¯ä¸Šçš„è®¾å¤‡çš„TF runtimeç‰ˆæœ¬å¯èƒ½å¾ˆè€ã€‚å› æ­¤ç”±FL infrastructureæ¥å¯¹FL Planè¿›è¡Œç­‰ä»·çš„å˜æ¢ä»¥é€‚åº”ç‰¹å®šçš„ç‰ˆæœ¬ï¼Œä¸åŒçš„ç‰ˆæœ¬ç»è¿‡åŒæ ·çš„release testä»¥ç¡®ä¿ä¸åŒçš„ç‰ˆæœ¬çš„å˜æ¢æ˜¯ç­‰ä»·çš„ã€‚</li></ol></li><li>æ•°æ®çš„å†™å…¥ï¼šæ¯ä¸€è½®ç»“æŸä¹‹åï¼Œæ¨¡å‹çš„èšç±»çš„å‚æ•°å°±ä¼šè¢«å†™åˆ°serverçš„è¢«æŒ‡å®šçš„ä½ç½®ã€‚æ¯ä¸ªè®¾å¤‡çš„æ¯ä¸€æ¬¡è®­ç»ƒç”±ä»»åŠ¡åç§°ã€è®ºæ¬¡åç§°å’Œå…¶ä»–çš„ä¸€äº›æ•°æ®æ ‡è¯†ï¼Œå¯¹åº”çš„ä¸€äº›æŒ‡æ ‡å¯ä»¥ç”±è¿™ä¸ªç³»ç»Ÿå»åˆ†æã€‚</li></ol></li></ol><h2 id="åº”ç”¨"><a href="#åº”ç”¨" class="headerlink" title="åº”ç”¨"></a>åº”ç”¨</h2></li><li><p>On-device item ranking</p></li><li><p>Content suggestions for on-device keyboards</p></li><li><p>Next word prediction</p></li></ol><h2 id="OPERATIONAL-PROFILEï¼ˆç»éªŒï¼‰"><a href="#OPERATIONAL-PROFILEï¼ˆç»éªŒï¼‰" class="headerlink" title="OPERATIONAL PROFILEï¼ˆç»éªŒï¼‰"></a>OPERATIONAL PROFILEï¼ˆç»éªŒï¼‰</h2><p>â€‹    </p>]]></content>
      
      
      <categories>
          
          <category> About Thesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Thesis </tag>
            
            <tag> About FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>æ¦‚ç‡è®ºéƒ¨åˆ†å†…å®¹å¤ä¹ </title>
      <link href="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/"/>
      <url>/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="æ¦‚ç‡è®ºéƒ¨åˆ†å†…å®¹å¤ä¹ "><a href="#æ¦‚ç‡è®ºéƒ¨åˆ†å†…å®¹å¤ä¹ " class="headerlink" title="æ¦‚ç‡è®ºéƒ¨åˆ†å†…å®¹å¤ä¹ "></a>æ¦‚ç‡è®ºéƒ¨åˆ†å†…å®¹å¤ä¹ </h1><h2 id="å¡æ–¹åˆ†å¸ƒ"><a href="#å¡æ–¹åˆ†å¸ƒ" class="headerlink" title="å¡æ–¹åˆ†å¸ƒ"></a>å¡æ–¹åˆ†å¸ƒ</h2><p>è®¾$X_{1}, X_{2}, â€¦, X_{n}$æ˜¯æ¥è‡ªæ€»ä½“$N(0, 1)$çš„æ ·æœ¬ï¼Œåˆ™ç§°ç»Ÿè®¡é‡</p><p>$$\chi^2=X_{1}^{2}+X_{2}^{2}+â€¦+X_{n}^{2}$$</p><p>æœä»è‡ªç”±åº¦ä¸ºnçš„$\chi^{2}$åˆ†å¸ƒã€‚è®°ä¸º$\chi^{2}\sim \chi^{2}(n)$.</p><ol><li><p><strong>æ»¡è¶³å¯åŠ æ€§ï¼šå³ä»æ€»ä½“ä¸­åˆ†ä¸¤æ¬¡æŠ½å–$n1$å’Œ$n2$ä¸ªæ ·æœ¬çš„å¹³æ–¹å’Œï¼Œå’Œä¸€æ¬¡æŠ½å–$n1+n2$ä¸ªæ ·æœ¬çš„å¹³æ–¹å’Œæ˜¯ç›¸ç­‰çš„ã€‚</strong></p><p>$$\chi_{1}^{2}+ \chi_{2}^{2} \sim \chi^{2}(n_{1}+n_{2})$$</p></li></ol><h2 id="tåˆ†å¸ƒ"><a href="#tåˆ†å¸ƒ" class="headerlink" title="tåˆ†å¸ƒ"></a>tåˆ†å¸ƒ</h2><p>è‹¥$X \sim N(0, 1)$, $Y\sim\chi^{2}(n)$ï¼Œä¸”X, Yç›¸äº’ç‹¬ç«‹ï¼Œåˆ™ç§°éšæœºå˜é‡:</p><p>$t=\frac{X}{\sqrt{Y/n}}$æœä»è‡ªç”±åº¦ä¸ºnçš„tåˆ†å¸ƒã€‚è®°ä¸º$t \sim t(n)$ã€‚</p><p><strong>tåˆ†å¸ƒæ€ä¹ˆæ¥çš„ï¼Ÿ</strong></p><p><strong>æºè‡ªå¯¹$\frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0, 1)$çš„ç ”ç©¶ã€‚</strong></p><p>ä¹Ÿå°±æ˜¯æ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼Œä»æ€»ä½“Xä¸­æŠ½å‡ºnä¸ªæ ·æœ¬ï¼Œæ ·æœ¬çš„å‡å€¼ä¸º$\overline{X}$ï¼Œæ€»ä½“çš„å‡å€¼ä¸º$\mu$ï¼Œæ–¹å·®ä¸º$\sigma$ï¼Œæ ·æœ¬çš„æ–¹å·®ä¸º$\sigma_{\overline{X}}$ï¼š</p><p>$\overline{X}\sim N(\mu, \sigma_{\overline{X}}^{2})$å³$\frac{\overline{X}-\mu}{\sigma_{\overline{X}}}\sim N(0, 1)$å³$t=\frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0, 1)$ï¼Œä¸ºè‡ªç”±åº¦é—®$n-1$çš„tåˆ†å¸ƒã€‚</p><h2 id="ä¸­å¿ƒæé™å®šç†"><a href="#ä¸­å¿ƒæé™å®šç†" class="headerlink" title="ä¸­å¿ƒæé™å®šç†"></a>ä¸­å¿ƒæé™å®šç†</h2><p>å’Œçš„åˆ†å¸ƒæ”¶æ•›äºæ­£æ€åˆ†å¸ƒçš„å®šç†å¦‚ï¼š</p><p><strong>æ ·æœ¬å‡å€¼$\overline{X}\sim N(\mu,\frac{\sigma^{2}}{n})$ã€‚</strong></p><h2 id="å¤§æ•°å®šå¾‹"><a href="#å¤§æ•°å®šå¾‹" class="headerlink" title="å¤§æ•°å®šå¾‹"></a>å¤§æ•°å®šå¾‹</h2><p><strong>æ ·æœ¬å‡å€¼ä¾æ¦‚ç‡æ”¶æ•›äºæœŸæœ›å€¼</strong></p><h2 id="å‡è®¾æ£€éªŒ"><a href="#å‡è®¾æ£€éªŒ" class="headerlink" title="å‡è®¾æ£€éªŒ"></a>å‡è®¾æ£€éªŒ</h2><ul><li>åŸå‡è®¾$H_0$ï¼šå®éªŒä¹‹å‰å·²æœ‰çš„å‡è®¾ï¼ŒABä¸¤æ¬¡æµ‹è¯•çš„å·®è·ä¸º0.</li><li>å¤‡æ‹©å‡è®¾$H_{1}$ï¼šå¯¹ç«‹äºåŸå‡è®¾ã€‚</li><li><strong>å…ˆå¯¹æ€»ä½“çš„ç‰¹å¾ä½œå‡ºæŸç§å‡è®¾ï¼Œç„¶åé€šè¿‡æŠ½æ ·ç ”ç©¶çš„ç»Ÿè®¡æ¨ç†ï¼Œå¯¹æ­¤å‡è®¾åº”è¯¥è¢«æ‹’ç»è¿˜æ˜¯æ¥å—ä½œå‡ºåˆ¤æ–­ã€‚</strong></li></ul><h3 id="ä¸¤ç±»é”™è¯¯"><a href="#ä¸¤ç±»é”™è¯¯" class="headerlink" title="ä¸¤ç±»é”™è¯¯"></a>ä¸¤ç±»é”™è¯¯</h3><ul><li><p>åŸå‡è®¾ä¸ºçœŸï¼Œå³$\mu=\mu_{0}$ï¼Œæ— æ˜¾è‘—æ€§å·®å¼‚ã€‚æˆ‘ä»¬å´ä¸æ¥å—ç»“æœï¼Œå«<strong>å¼ƒçœŸé”™è¯¯ï¼Œç¬¬ä¸€ç±»é”™è¯¯</strong>ï¼ŒçŠ¯é”™æ¦‚ç‡ä¸º$\alpha$ã€‚<strong>å³å®é™…ä¸ºçœŸï¼Œä½†æ˜¯æ ·æœ¬å´æŠ½å–åˆ°äº†è®©æˆ‘ä»¬åˆ¤æ–­ç»“æœä¸ºå‡çš„æƒ…å†µã€‚</strong></p></li><li><p><strong>æ˜¾è‘—æ€§æ°´å¹³ï¼ˆçŠ¯é”™çš„æ¦‚ç‡ï¼‰</strong>è¶Šå¤§ï¼Œé‚£ä¹ˆåŸå‡è®¾è¢«æ‹’ç»çš„å¯èƒ½æ€§å°±è¶Šå¤§ï¼ŒçŠ¯ç¬¬ä¸€ç±»é”™è¯¯çš„å¯èƒ½æ€§ä¹Ÿå°±è¶Šå¤§ã€‚<strong>på€¼</strong>å³ä¸ºåœ¨è§‚æµ‹æ•°æ®ä¸‹æ‹’ç»åŸå‡è®¾çš„æœ€å°<strong>æ˜¾è‘—æ€§æ°´å¹³</strong>ã€‚</p></li><li><p>åŸå‡è®¾ä¸ºå‡ï¼Œå³$\mu\neq\mu_{0}$ï¼Œæœ‰æ˜¾è‘—æ€§å·®å¼‚ã€‚æˆ‘ä»¬å´æ¥å—ç»“æœï¼Œå«<strong>å–ä¼ªé”™è¯¯ï¼Œç¬¬äºŒç±»é”™è¯¯</strong>ï¼ŒçŠ¯é”™æ¦‚ç‡ä¸º$\beta$ã€‚<strong>å³å®é™…ä¸ºå‡ï¼Œä½†æ˜¯æ ·æœ¬å´æŠ½å–åˆ°äº†è®©æˆ‘ä»¬åˆ¤æ–­ç»“æœä¸ºçœŸçš„æƒ…å†µã€‚</strong></p></li></ul><p><img src="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123.png" class="lazyload placeholder" data-srcset="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="å¦‚ä½•å‡å°‘ä¸¤ç±»é”™è¯¯"><a href="#å¦‚ä½•å‡å°‘ä¸¤ç±»é”™è¯¯" class="headerlink" title="å¦‚ä½•å‡å°‘ä¸¤ç±»é”™è¯¯"></a>å¦‚ä½•å‡å°‘ä¸¤ç±»é”™è¯¯</h3><p><img src="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667132397600-2.png" class="lazyload placeholder" data-srcset="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667132397600-2.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="æ˜¾è‘—æ€§æ°´å¹³"><a href="#æ˜¾è‘—æ€§æ°´å¹³" class="headerlink" title="æ˜¾è‘—æ€§æ°´å¹³"></a>æ˜¾è‘—æ€§æ°´å¹³</h3><p><img src="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667132490168-4.png" class="lazyload placeholder" data-srcset="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667132490168-4.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="på€¼è®¡ç®—"><a href="#på€¼è®¡ç®—" class="headerlink" title="på€¼è®¡ç®—"></a>på€¼è®¡ç®—</h3><p><img src="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667133450982-6.png" class="lazyload placeholder" data-srcset="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667133450982-6.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><h3 id="ç½®ä¿¡æ°´å¹³å’ŒåŒºé—´"><a href="#ç½®ä¿¡æ°´å¹³å’ŒåŒºé—´" class="headerlink" title="ç½®ä¿¡æ°´å¹³å’ŒåŒºé—´"></a>ç½®ä¿¡æ°´å¹³å’ŒåŒºé—´</h3><p><img src="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667133632548-8.png" class="lazyload placeholder" data-srcset="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667133632548-8.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p><img src="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667133747607-10.png" class="lazyload placeholder" data-srcset="/2022/10/28/gai-lu-lun-yu-shu-li-tong-ji/123-1667133747607-10.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg"></p><p><strong>ç†è§£ï¼šç½®ä¿¡åŒºé—´å¯ä»¥ç†è§£æˆæ€»ä½“å‡å€¼ç­‰é‡ç”¨æ ·æœ¬è¡¨ç¤ºæ—¶å€™ï¼Œå¾€å¾€åœ¨ä»€ä¹ˆæ ·çš„èŒƒå›´å†…ï¼Œåœ¨è¿™ä¸ªèŒƒå›´å†…çš„æ¦‚ç‡å°±æ˜¯ç½®ä¿¡æ°´å¹³ã€‚</strong></p><ul><li>ä¸ºä»€ä¹ˆè¦ç½®ä¿¡åŒºé—´ï¼Œå› ä¸º<strong>è¯¯å·®ä¸å¯é¿å…</strong>ï¼Œ<strong>ç½®ä¿¡åŒºé—´å³ä¸ºç»Ÿè®¡é‡çš„è¯¯å·®èŒƒå›´</strong>ã€‚æ¯”å¦‚ç”¨$[a,b]$è¡¨ç¤ºæ ·æœ¬ä¼°è®¡æ€»ä½“å‡å€¼çš„è¯¯å·®èŒƒå›´ï¼Œé‚£ä¹ˆè¿™ä¸€ç»“æœå…·æœ‰çš„<strong>å¯ä¿¡ç¨‹åº¦</strong>ï¼Œå°±æ˜¯ç½®ä¿¡åº¦ã€‚</li></ul><h3 id="å…¶ä»–"><a href="#å…¶ä»–" class="headerlink" title="å…¶ä»–"></a>å…¶ä»–</h3><p>tæ£€éªŒå’Œzæ£€éªŒéƒ½å¯ä»¥ç”¨æ¥åˆ¤æ–­æ ·æœ¬ï¼Œå‡ æ¬¡æ ·æœ¬ä¹‹é—´çš„<strong>å‡å€¼</strong>æ˜¯å¦æ˜¾è‘—ï¼Œå¡æ–¹æ£€éªŒç”¨äºåˆ¤æ–­æ ·æœ¬åå·®æ˜¯å¦åˆç†ï¼Œå³ç”¨æ¥åˆ¤æ–­æ£€éªŒæŠ½æ ·æ˜¯å¦åˆç†ã€‚</p><h2 id="å‚è€ƒ"><a href="#å‚è€ƒ" class="headerlink" title="å‚è€ƒ"></a>å‚è€ƒ</h2><ul><li>2021.01.18: &lt;&lt;æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡&gt;&gt;</li><li>2021.08.22: <a href="https://zhuanlan.zhihu.com/p/346602966">https://zhuanlan.zhihu.com/p/346602966</a>, <a href="https://www.zhihu.com/question/24801731">https://www.zhihu.com/question/24801731</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> About Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Reading </tag>
            
            <tag> About Math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Build a blog like this</title>
      <link href="/2021/10/14/build-a-blog-like-this/"/>
      <url>/2021/10/14/build-a-blog-like-this/</url>
      
        <content type="html"><![CDATA[<h1 id="How-to-build-a-blog-like-this"><a href="#How-to-build-a-blog-like-this" class="headerlink" title="How to build a blog like this?"></a>How to build a blog like this?</h1><ol><li>How to build a blog like this? What you need is this <a href="https://fuhanshi.github.io/2018/10/03/Hexo-Github%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E7%82%AB%E9%85%B7%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">blog</a> .(There may exists some typos, but itâ€™s not a big problem.)</li><li>Then how to replace a theme? What you need is this <a href="https://www.jianshu.com/p/ef7a29e3ee8e">blog</a>.</li><li>Then where to find many many wonderful themes? What you need is this <a href="https://hexo.io/themes/">website</a>.</li><li>About how to config this website? <a href="https://yuang01.github.io/">clicke here</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> About Me </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Blogs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>About Me</title>
      <link href="/2021/08/07/about-me/"/>
      <url>/2021/08/07/about-me/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> About Me </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About Me </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
